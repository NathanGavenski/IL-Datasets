{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Imitation Learning Datasets Hi, welcome to the Imitation Learning (IL) Datasets. Something that always bothered me was how difficult it is to find good weights for an expert, trying to create a dataset for different state-of-the-art methods, and having to run all methods due to no common datasets. For these reasons, I've created IL-Datasets, in an effort to make it more accessible for researchers to create datasets using experts from the Hugging Face. IL-Datasets provides teacher weights for different environments, a multi-threading solution for creating datasets faster, datasets for a set of environments, and a benchmark for common imitation learning methods. This project is under development. If you are interested in helping, feel free to contact me . Main Features Dataset creation with StableBaselines from HuggingFace weights. Dataset creation with user custom policies. Readily available datasets for common benchmark environments. Benchmark results for all implemented methods in the published datasets.","title":"Home"},{"location":"#imitation-learning-datasets","text":"Hi, welcome to the Imitation Learning (IL) Datasets. Something that always bothered me was how difficult it is to find good weights for an expert, trying to create a dataset for different state-of-the-art methods, and having to run all methods due to no common datasets. For these reasons, I've created IL-Datasets, in an effort to make it more accessible for researchers to create datasets using experts from the Hugging Face. IL-Datasets provides teacher weights for different environments, a multi-threading solution for creating datasets faster, datasets for a set of environments, and a benchmark for common imitation learning methods. This project is under development. If you are interested in helping, feel free to contact me .","title":"Imitation Learning Datasets"},{"location":"#main-features","text":"Dataset creation with StableBaselines from HuggingFace weights. Dataset creation with user custom policies. Readily available datasets for common benchmark environments. Benchmark results for all implemented methods in the published datasets.","title":"Main Features"},{"location":"about/license/","text":"MIT License Copyright (c) 2022 Nathan Schneider Gavenski Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"about/release-notes/","text":"Release Notes v0.4.0 Benchmarking Now IL-Datasets has its own benchmarking! We are adding new methods and environments to the repository. For a full list of the methods and environments planned for release, please check the repository readme.md file. Support for benchmark requirements We split the imitation_datasets and benchmark modules requirements. pip install il-datasets will only install requirements regarding the imitation_datasets module. For using benchmark please use: pip install \"il-datasets[benchmark]\" Full Changelog : https://github.com/NathanGavenski/IL-Datasets/compare/0.3.0...0.4.0 v0.3.0 This version adds another point from the TODO list, Datasets ! Now, if you use the baseline_enjoy and baseline_collate functions, you can use the BaselineDataset . The datasets will load the generated numpy file and organize all entries to be (s_t, a_t, s_{t+1}), provide the average reward for all episodes and also allow for fewer episodes with the parameter n_episodes . Alongside the dataset, I've implemented a HuggingFace solution as well as utility functions that allow users to upload their datasets to the HuggingFace website. There is already an example at: https://huggingface.co/datasets/NathanGavenski/CartPole-v1 In the future, these datasets will be used for benchmarking, but for now, it allows for storing outside drivers (such as Google's and Microsoft's) This version also comes with some QoL improvements, such as pylint, and unit tests, so the code is more readable and also more stable. Finally, with this release, I've implemented some metrics: performance , average episodic reward and accuracy . Future release sneak peek It is my plan that the future release will introduce benchmarking to IL-Datasets. With benchmarking, we will host a set of different datasets for common environments in the IL literature. This should help all researchers (including myself) to stop running different methods for each experiment. Full Changelog : https://github.com/NathanGavenski/IL-Datasets/compare/0.2.0...0.3.0 V0.2.0 New Features Added support for Gymnasium and Gym version 0.26.0. Created template functions for enjoy and collate for a simple and one following the original dictionary from StableBaselines . v0.1.0 FIx: Sometimes, when the policy did not reach the goal and the enjoy function returned False , the Controller would not execute the enjoy function again First release First release for IL-Datasets. The missing features (such as documentation) list is in the README.md. If you have any issues with this release be sure to open an issue or contact me \ud83d\ude04","title":"Release Notes"},{"location":"about/release-notes/#release-notes","text":"","title":"Release Notes"},{"location":"about/release-notes/#v040","text":"","title":"v0.4.0"},{"location":"about/release-notes/#benchmarking","text":"Now IL-Datasets has its own benchmarking! We are adding new methods and environments to the repository. For a full list of the methods and environments planned for release, please check the repository readme.md file.","title":"Benchmarking"},{"location":"about/release-notes/#support-for-benchmark-requirements","text":"We split the imitation_datasets and benchmark modules requirements. pip install il-datasets will only install requirements regarding the imitation_datasets module. For using benchmark please use: pip install \"il-datasets[benchmark]\" Full Changelog : https://github.com/NathanGavenski/IL-Datasets/compare/0.3.0...0.4.0","title":"Support for benchmark requirements"},{"location":"about/release-notes/#v030","text":"This version adds another point from the TODO list, Datasets ! Now, if you use the baseline_enjoy and baseline_collate functions, you can use the BaselineDataset . The datasets will load the generated numpy file and organize all entries to be (s_t, a_t, s_{t+1}), provide the average reward for all episodes and also allow for fewer episodes with the parameter n_episodes . Alongside the dataset, I've implemented a HuggingFace solution as well as utility functions that allow users to upload their datasets to the HuggingFace website. There is already an example at: https://huggingface.co/datasets/NathanGavenski/CartPole-v1 In the future, these datasets will be used for benchmarking, but for now, it allows for storing outside drivers (such as Google's and Microsoft's) This version also comes with some QoL improvements, such as pylint, and unit tests, so the code is more readable and also more stable. Finally, with this release, I've implemented some metrics: performance , average episodic reward and accuracy .","title":"v0.3.0"},{"location":"about/release-notes/#future-release-sneak-peek","text":"It is my plan that the future release will introduce benchmarking to IL-Datasets. With benchmarking, we will host a set of different datasets for common environments in the IL literature. This should help all researchers (including myself) to stop running different methods for each experiment. Full Changelog : https://github.com/NathanGavenski/IL-Datasets/compare/0.2.0...0.3.0","title":"Future release sneak peek"},{"location":"about/release-notes/#v020","text":"","title":"V0.2.0"},{"location":"about/release-notes/#new-features","text":"Added support for Gymnasium and Gym version 0.26.0. Created template functions for enjoy and collate for a simple and one following the original dictionary from StableBaselines .","title":"New Features"},{"location":"about/release-notes/#v010","text":"","title":"v0.1.0"},{"location":"about/release-notes/#fix","text":"Sometimes, when the policy did not reach the goal and the enjoy function returned False , the Controller would not execute the enjoy function again","title":"FIx:"},{"location":"about/release-notes/#first-release","text":"First release for IL-Datasets. The missing features (such as documentation) list is in the README.md. If you have any issues with this release be sure to open an issue or contact me \ud83d\ude04","title":"First release"},{"location":"algorithms/abco/","text":"Augmented Behavioural Cloning from Observation Bases: BCO Augmented Behavioural Cloning from Observation method based on (Monteiro et. al., 2020) Source code in src/benchmark/methods/abco.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 class ABCO ( BCO ): \"\"\"Augmented Behavioural Cloning from Observation method based on (Monteiro et. al., 2020)\"\"\" __version__ = \"1.0.0\" __author__ = \"Monteiro et. al.\" __method_name__ = \"Augmented Behavioural Cloning from Observation\" def __init__ ( self , environment : Env , enjoy_criteria : int = 100 , verbose : bool = False ) -> None : super () . __init__ ( environment , enjoy_criteria , verbose ) self . save_path = f \"./tmp/abco/ { self . environment_name } /\" def train ( self , n_epochs : int , train_dataset : Dict [ str , DataLoader ], eval_dataset : Dict [ str , DataLoader ] = None , folder : str = None ) -> Self : if folder is None : folder = f \"../benchmark_results/abco/ { self . environment_name } \" super () . train ( n_epochs , train_dataset , eval_dataset , folder ) def _append_samples ( self , train_dataset : DataLoader ) -> DataLoader : \"\"\"Append samples to DataLoader. Args: train_dataset (DataLoader): current train dataset. Returns: train_dataset (DataLoader): new train dataset. \"\"\" metrics , i_pos = self . _enjoy ( return_ipos = True ) i_pos_ratio = metrics . get ( 'success_rate' , 0 ) idm_ratio = 1 - i_pos_ratio if i_pos_ratio == 0 : return train_dataset i_pos_size = i_pos [ \"states\" ] . shape [ 0 ] idm_size = train_dataset [ 'idm_dataset' ] . dataset . states . shape [ 0 ] i_pos_k = max ( 0 , int ( i_pos_size * i_pos_ratio )) idm_k = max ( 0 , int ( idm_size * idm_ratio )) i_pos_idx = torch . multinomial ( torch . tensor ( range ( i_pos_size )) . float (), i_pos_k ) try : idm_idx = torch . multinomial ( torch . tensor ( range ( idm_size )) . float (), idm_k ) except RuntimeError : idm_idx = [] train_dataset [ 'idm_dataset' ] . dataset . states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . states [ idm_idx ], torch . from_numpy ( i_pos [ 'states' ])[ i_pos_idx ]), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . next_states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . next_states [ idm_idx ], torch . from_numpy ( i_pos [ 'next_states' ])[ i_pos_idx ]), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . actions = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . actions [ idm_idx ], torch . from_numpy ( i_pos [ 'actions' ] . reshape (( - 1 , 1 )))[ i_pos_idx ]), dim = 0 ) return train_dataset def _enjoy ( self , render : bool = False , teacher_reward : Number = None , random_reward : Number = None , return_ipos : bool = False , ) -> Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]]: \"\"\"Function for evaluation of the policy in the environment Args: render (bool): Whether it should render. Defaults to False. teacher_reward (Number): reward for teacher policy. random_reward (Number): reward for a random policy. return_ipos (bool): whether it should return data to append to I_pos. Returns: Metrics: aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. success_rate (float): percentage that the agent reached the goal. I_pos: states (List[Number]): states before action. actions (List[Number]): action given states. next_states (List[Number]): next state given states and actions. \"\"\" environment = GymWrapper ( self . environment ) average_reward = [] i_pos = defaultdict ( list ) success_rate = [] for _ in range ( 100 ): done = False obs = environment . reset () accumulated_reward = 0 goal = False while not done : if render : environment . render () action = self . predict ( obs ) i_pos [ 'states' ] . append ( obs ) i_pos [ 'actions' ] . append ( action ) gym_return = environment . step ( action ) obs , reward , done , * _ = gym_return accumulated_reward += reward goal |= reached_goal ( self . environment_name , gym_return , accumulated_reward ) i_pos [ 'next_states' ] . append ( obs ) average_reward . append ( accumulated_reward ) success_rate . append ( goal ) metrics = average_episodic_reward ( average_reward ) if teacher_reward is not None and random_reward is not None : metrics . update ( performance ( average_reward , teacher_reward , random_reward )) metrics [ 'success_rate' ] = np . mean ( success_rate ) i_pos = { key : np . array ( value ) for key , value in i_pos . items ()} if return_ipos : return metrics , i_pos return metrics _append_samples ( train_dataset ) Append samples to DataLoader. Parameters: train_dataset ( DataLoader ) \u2013 current train dataset. Returns: train_dataset ( DataLoader ) \u2013 new train dataset. src/benchmark/methods/abco.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def _append_samples ( self , train_dataset : DataLoader ) -> DataLoader : \"\"\"Append samples to DataLoader. Args: train_dataset (DataLoader): current train dataset. Returns: train_dataset (DataLoader): new train dataset. \"\"\" metrics , i_pos = self . _enjoy ( return_ipos = True ) i_pos_ratio = metrics . get ( 'success_rate' , 0 ) idm_ratio = 1 - i_pos_ratio if i_pos_ratio == 0 : return train_dataset i_pos_size = i_pos [ \"states\" ] . shape [ 0 ] idm_size = train_dataset [ 'idm_dataset' ] . dataset . states . shape [ 0 ] i_pos_k = max ( 0 , int ( i_pos_size * i_pos_ratio )) idm_k = max ( 0 , int ( idm_size * idm_ratio )) i_pos_idx = torch . multinomial ( torch . tensor ( range ( i_pos_size )) . float (), i_pos_k ) try : idm_idx = torch . multinomial ( torch . tensor ( range ( idm_size )) . float (), idm_k ) except RuntimeError : idm_idx = [] train_dataset [ 'idm_dataset' ] . dataset . states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . states [ idm_idx ], torch . from_numpy ( i_pos [ 'states' ])[ i_pos_idx ]), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . next_states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . next_states [ idm_idx ], torch . from_numpy ( i_pos [ 'next_states' ])[ i_pos_idx ]), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . actions = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . actions [ idm_idx ], torch . from_numpy ( i_pos [ 'actions' ] . reshape (( - 1 , 1 )))[ i_pos_idx ]), dim = 0 ) return train_dataset _enjoy ( render = False , teacher_reward = None , random_reward = None , return_ipos = False ) Function for evaluation of the policy in the environment Parameters: render ( bool , default: False ) \u2013 Whether it should render. Defaults to False. teacher_reward ( Number , default: None ) \u2013 reward for teacher policy. random_reward ( Number , default: None ) \u2013 reward for a random policy. return_ipos ( bool , default: False ) \u2013 whether it should return data to append to I_pos. Returns: Metrics ( Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]] ) \u2013 aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. success_rate (float): percentage that the agent reached the goal. I_pos ( Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]] ) \u2013 states (List[Number]): states before action. actions (List[Number]): action given states. next_states (List[Number]): next state given states and actions. src/benchmark/methods/abco.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def _enjoy ( self , render : bool = False , teacher_reward : Number = None , random_reward : Number = None , return_ipos : bool = False , ) -> Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]]: \"\"\"Function for evaluation of the policy in the environment Args: render (bool): Whether it should render. Defaults to False. teacher_reward (Number): reward for teacher policy. random_reward (Number): reward for a random policy. return_ipos (bool): whether it should return data to append to I_pos. Returns: Metrics: aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. success_rate (float): percentage that the agent reached the goal. I_pos: states (List[Number]): states before action. actions (List[Number]): action given states. next_states (List[Number]): next state given states and actions. \"\"\" environment = GymWrapper ( self . environment ) average_reward = [] i_pos = defaultdict ( list ) success_rate = [] for _ in range ( 100 ): done = False obs = environment . reset () accumulated_reward = 0 goal = False while not done : if render : environment . render () action = self . predict ( obs ) i_pos [ 'states' ] . append ( obs ) i_pos [ 'actions' ] . append ( action ) gym_return = environment . step ( action ) obs , reward , done , * _ = gym_return accumulated_reward += reward goal |= reached_goal ( self . environment_name , gym_return , accumulated_reward ) i_pos [ 'next_states' ] . append ( obs ) average_reward . append ( accumulated_reward ) success_rate . append ( goal ) metrics = average_episodic_reward ( average_reward ) if teacher_reward is not None and random_reward is not None : metrics . update ( performance ( average_reward , teacher_reward , random_reward )) metrics [ 'success_rate' ] = np . mean ( success_rate ) i_pos = { key : np . array ( value ) for key , value in i_pos . items ()} if return_ipos : return metrics , i_pos return metrics","title":"Augmented Behavioural Cloning from Observation"},{"location":"algorithms/abco/#augmented-behavioural-cloning-from-observation","text":"Bases: BCO Augmented Behavioural Cloning from Observation method based on (Monteiro et. al., 2020) Source code in src/benchmark/methods/abco.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 class ABCO ( BCO ): \"\"\"Augmented Behavioural Cloning from Observation method based on (Monteiro et. al., 2020)\"\"\" __version__ = \"1.0.0\" __author__ = \"Monteiro et. al.\" __method_name__ = \"Augmented Behavioural Cloning from Observation\" def __init__ ( self , environment : Env , enjoy_criteria : int = 100 , verbose : bool = False ) -> None : super () . __init__ ( environment , enjoy_criteria , verbose ) self . save_path = f \"./tmp/abco/ { self . environment_name } /\" def train ( self , n_epochs : int , train_dataset : Dict [ str , DataLoader ], eval_dataset : Dict [ str , DataLoader ] = None , folder : str = None ) -> Self : if folder is None : folder = f \"../benchmark_results/abco/ { self . environment_name } \" super () . train ( n_epochs , train_dataset , eval_dataset , folder ) def _append_samples ( self , train_dataset : DataLoader ) -> DataLoader : \"\"\"Append samples to DataLoader. Args: train_dataset (DataLoader): current train dataset. Returns: train_dataset (DataLoader): new train dataset. \"\"\" metrics , i_pos = self . _enjoy ( return_ipos = True ) i_pos_ratio = metrics . get ( 'success_rate' , 0 ) idm_ratio = 1 - i_pos_ratio if i_pos_ratio == 0 : return train_dataset i_pos_size = i_pos [ \"states\" ] . shape [ 0 ] idm_size = train_dataset [ 'idm_dataset' ] . dataset . states . shape [ 0 ] i_pos_k = max ( 0 , int ( i_pos_size * i_pos_ratio )) idm_k = max ( 0 , int ( idm_size * idm_ratio )) i_pos_idx = torch . multinomial ( torch . tensor ( range ( i_pos_size )) . float (), i_pos_k ) try : idm_idx = torch . multinomial ( torch . tensor ( range ( idm_size )) . float (), idm_k ) except RuntimeError : idm_idx = [] train_dataset [ 'idm_dataset' ] . dataset . states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . states [ idm_idx ], torch . from_numpy ( i_pos [ 'states' ])[ i_pos_idx ]), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . next_states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . next_states [ idm_idx ], torch . from_numpy ( i_pos [ 'next_states' ])[ i_pos_idx ]), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . actions = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . actions [ idm_idx ], torch . from_numpy ( i_pos [ 'actions' ] . reshape (( - 1 , 1 )))[ i_pos_idx ]), dim = 0 ) return train_dataset def _enjoy ( self , render : bool = False , teacher_reward : Number = None , random_reward : Number = None , return_ipos : bool = False , ) -> Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]]: \"\"\"Function for evaluation of the policy in the environment Args: render (bool): Whether it should render. Defaults to False. teacher_reward (Number): reward for teacher policy. random_reward (Number): reward for a random policy. return_ipos (bool): whether it should return data to append to I_pos. Returns: Metrics: aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. success_rate (float): percentage that the agent reached the goal. I_pos: states (List[Number]): states before action. actions (List[Number]): action given states. next_states (List[Number]): next state given states and actions. \"\"\" environment = GymWrapper ( self . environment ) average_reward = [] i_pos = defaultdict ( list ) success_rate = [] for _ in range ( 100 ): done = False obs = environment . reset () accumulated_reward = 0 goal = False while not done : if render : environment . render () action = self . predict ( obs ) i_pos [ 'states' ] . append ( obs ) i_pos [ 'actions' ] . append ( action ) gym_return = environment . step ( action ) obs , reward , done , * _ = gym_return accumulated_reward += reward goal |= reached_goal ( self . environment_name , gym_return , accumulated_reward ) i_pos [ 'next_states' ] . append ( obs ) average_reward . append ( accumulated_reward ) success_rate . append ( goal ) metrics = average_episodic_reward ( average_reward ) if teacher_reward is not None and random_reward is not None : metrics . update ( performance ( average_reward , teacher_reward , random_reward )) metrics [ 'success_rate' ] = np . mean ( success_rate ) i_pos = { key : np . array ( value ) for key , value in i_pos . items ()} if return_ipos : return metrics , i_pos return metrics","title":"Augmented Behavioural Cloning from Observation"},{"location":"algorithms/abco/#benchmark.methods.abco.ABCO._append_samples","text":"Append samples to DataLoader. Parameters: train_dataset ( DataLoader ) \u2013 current train dataset. Returns: train_dataset ( DataLoader ) \u2013 new train dataset. src/benchmark/methods/abco.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def _append_samples ( self , train_dataset : DataLoader ) -> DataLoader : \"\"\"Append samples to DataLoader. Args: train_dataset (DataLoader): current train dataset. Returns: train_dataset (DataLoader): new train dataset. \"\"\" metrics , i_pos = self . _enjoy ( return_ipos = True ) i_pos_ratio = metrics . get ( 'success_rate' , 0 ) idm_ratio = 1 - i_pos_ratio if i_pos_ratio == 0 : return train_dataset i_pos_size = i_pos [ \"states\" ] . shape [ 0 ] idm_size = train_dataset [ 'idm_dataset' ] . dataset . states . shape [ 0 ] i_pos_k = max ( 0 , int ( i_pos_size * i_pos_ratio )) idm_k = max ( 0 , int ( idm_size * idm_ratio )) i_pos_idx = torch . multinomial ( torch . tensor ( range ( i_pos_size )) . float (), i_pos_k ) try : idm_idx = torch . multinomial ( torch . tensor ( range ( idm_size )) . float (), idm_k ) except RuntimeError : idm_idx = [] train_dataset [ 'idm_dataset' ] . dataset . states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . states [ idm_idx ], torch . from_numpy ( i_pos [ 'states' ])[ i_pos_idx ]), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . next_states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . next_states [ idm_idx ], torch . from_numpy ( i_pos [ 'next_states' ])[ i_pos_idx ]), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . actions = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . actions [ idm_idx ], torch . from_numpy ( i_pos [ 'actions' ] . reshape (( - 1 , 1 )))[ i_pos_idx ]), dim = 0 ) return train_dataset","title":"_append_samples()"},{"location":"algorithms/abco/#benchmark.methods.abco.ABCO._enjoy","text":"Function for evaluation of the policy in the environment Parameters: render ( bool , default: False ) \u2013 Whether it should render. Defaults to False. teacher_reward ( Number , default: None ) \u2013 reward for teacher policy. random_reward ( Number , default: None ) \u2013 reward for a random policy. return_ipos ( bool , default: False ) \u2013 whether it should return data to append to I_pos. Returns: Metrics ( Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]] ) \u2013 aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. success_rate (float): percentage that the agent reached the goal. I_pos ( Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]] ) \u2013 states (List[Number]): states before action. actions (List[Number]): action given states. next_states (List[Number]): next state given states and actions. src/benchmark/methods/abco.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def _enjoy ( self , render : bool = False , teacher_reward : Number = None , random_reward : Number = None , return_ipos : bool = False , ) -> Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]]: \"\"\"Function for evaluation of the policy in the environment Args: render (bool): Whether it should render. Defaults to False. teacher_reward (Number): reward for teacher policy. random_reward (Number): reward for a random policy. return_ipos (bool): whether it should return data to append to I_pos. Returns: Metrics: aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. success_rate (float): percentage that the agent reached the goal. I_pos: states (List[Number]): states before action. actions (List[Number]): action given states. next_states (List[Number]): next state given states and actions. \"\"\" environment = GymWrapper ( self . environment ) average_reward = [] i_pos = defaultdict ( list ) success_rate = [] for _ in range ( 100 ): done = False obs = environment . reset () accumulated_reward = 0 goal = False while not done : if render : environment . render () action = self . predict ( obs ) i_pos [ 'states' ] . append ( obs ) i_pos [ 'actions' ] . append ( action ) gym_return = environment . step ( action ) obs , reward , done , * _ = gym_return accumulated_reward += reward goal |= reached_goal ( self . environment_name , gym_return , accumulated_reward ) i_pos [ 'next_states' ] . append ( obs ) average_reward . append ( accumulated_reward ) success_rate . append ( goal ) metrics = average_episodic_reward ( average_reward ) if teacher_reward is not None and random_reward is not None : metrics . update ( performance ( average_reward , teacher_reward , random_reward )) metrics [ 'success_rate' ] = np . mean ( success_rate ) i_pos = { key : np . array ( value ) for key , value in i_pos . items ()} if return_ipos : return metrics , i_pos return metrics","title":"_enjoy()"},{"location":"algorithms/bc/","text":"Behavioural Cloning Bases: Method Behavioural Clonning method based on (Pomerleau, 1988) Source code in src/benchmark/methods/bc.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 class BC ( Method ): \"\"\"Behavioural Clonning method based on (Pomerleau, 1988)\"\"\" __version__ = \"1.0.0\" __author__ = \"Pomerleau\" __method_name__ = \"Behavioural Cloning\" def __init__ ( self , environment : Env , enjoy_criteria : int = 100 , verbose : bool = False ) -> None : \"\"\"Initialize BC method.\"\"\" self . enjoy_criteria = enjoy_criteria self . verbose = verbose self . environment_name = environment . spec . name self . save_path = f \"./tmp/bc/ { self . environment_name } /\" self . hyperparameters = import_hyperparameters ( CONFIG_FILE , self . environment_name , ) super () . __init__ ( environment , self . hyperparameters ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method for the method. Args: x (torch.Tensor): input. Returns: x (torch.Tensor): logits output. \"\"\" return self . policy ( x ) def save ( self , path : str = None ) -> None : \"\"\"Save all model weights. Args: path (str): where to save the models. Defaults to None. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): os . makedirs ( path ) torch . save ( self . policy . state_dict (), f \" { path } /best_model.ckpt\" ) def load ( self , path : str = None ) -> Self : \"\"\"Load all model weights. Args: path (str): where to look for the model's weights. Defaults to None. Raises: ValueError: if the path does not exist. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): raise ValueError ( \"Path does not exists.\" ) self . policy . load_state_dict ( torch . load ( f \" { path } best_model.ckpt\" , map_location = torch . device ( self . device ) ) ) return self def train ( self , n_epochs : int , train_dataset : DataLoader , eval_dataset : DataLoader = None , ) -> Self : \"\"\"Train process. Args: n_epochs (int): amount of epoch to run. train_dataset (DataLoader): data to train. eval_dataset (DataLoader): data to eval. Defaults to None. Returns: method (Self): trained method. \"\"\" folder = f \"./benchmark_results/bc/ { self . environment_name } \" if not os . path . exists ( folder ): os . makedirs ( f \" { folder } /\" ) board = Tensorboard ( path = folder ) self . policy . to ( self . device ) best_model = - np . inf pbar = range ( n_epochs ) if self . verbose : pbar = tqdm ( pbar , desc = self . __method_name__ ) for epoch in pbar : train_metrics = self . _train ( train_dataset ) board . add_scalars ( \"Train\" , epoch = \"train\" , ** train_metrics ) if eval_dataset is not None : eval_metrics = self . _eval ( eval_dataset ) board . add_scalars ( \"Eval\" , epoch = \"eval\" , ** eval_metrics ) board . step ([ \"train\" , \"eval\" ]) else : board . step ( \"train\" ) if epoch % self . enjoy_criteria == 0 : metrics = self . _enjoy () board . add_scalars ( \"Enjoy\" , epoch = \"enjoy\" , ** metrics ) board . step ( \"enjoy\" ) if best_model < metrics [ \"aer\" ]: self . save () return self def _train ( self , dataset : DataLoader ) -> Metrics : \"\"\"Train loop. Args: dataset (DataLoader): train data. \"\"\" accumulated_loss = [] accumulated_accuracy = [] if not self . policy . training : self . policy . train () for batch in dataset : state , action , _ = batch state = state . to ( self . device ) action = action . to ( self . device ) self . optimizer_fn . zero_grad () predictions = self . forward ( state ) loss = self . loss_fn ( predictions , action . squeeze () . long ()) loss . backward () self . optimizer_fn . step () accumulated_loss . append ( loss . item ()) accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) return { \"loss\" : np . mean ( accumulated_loss ), \"accuracy\" : np . mean ( accumulated_accuracy )} def _eval ( self , dataset : DataLoader ) -> Metrics : \"\"\"Evaluation loop. Args: dataset (DataLoader): data to eval. \"\"\" accumulated_accuracy = [] if self . policy . training : self . policy . eval () for batch in dataset : state , action , _ = batch state = state . to ( self . device ) with torch . no_grad (): predictions = self . policy ( state ) accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) return { \"accuracy\" : np . mean ( accumulated_accuracy )} __init__ ( environment , enjoy_criteria = 100 , verbose = False ) Initialize BC method. src/benchmark/methods/bc.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , environment : Env , enjoy_criteria : int = 100 , verbose : bool = False ) -> None : \"\"\"Initialize BC method.\"\"\" self . enjoy_criteria = enjoy_criteria self . verbose = verbose self . environment_name = environment . spec . name self . save_path = f \"./tmp/bc/ { self . environment_name } /\" self . hyperparameters = import_hyperparameters ( CONFIG_FILE , self . environment_name , ) super () . __init__ ( environment , self . hyperparameters ) _eval ( dataset ) Evaluation loop. Parameters: dataset ( DataLoader ) \u2013 data to eval. src/benchmark/methods/bc.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def _eval ( self , dataset : DataLoader ) -> Metrics : \"\"\"Evaluation loop. Args: dataset (DataLoader): data to eval. \"\"\" accumulated_accuracy = [] if self . policy . training : self . policy . eval () for batch in dataset : state , action , _ = batch state = state . to ( self . device ) with torch . no_grad (): predictions = self . policy ( state ) accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) return { \"accuracy\" : np . mean ( accumulated_accuracy )} _train ( dataset ) Train loop. Parameters: dataset ( DataLoader ) \u2013 train data. src/benchmark/methods/bc.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def _train ( self , dataset : DataLoader ) -> Metrics : \"\"\"Train loop. Args: dataset (DataLoader): train data. \"\"\" accumulated_loss = [] accumulated_accuracy = [] if not self . policy . training : self . policy . train () for batch in dataset : state , action , _ = batch state = state . to ( self . device ) action = action . to ( self . device ) self . optimizer_fn . zero_grad () predictions = self . forward ( state ) loss = self . loss_fn ( predictions , action . squeeze () . long ()) loss . backward () self . optimizer_fn . step () accumulated_loss . append ( loss . item ()) accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) return { \"loss\" : np . mean ( accumulated_loss ), \"accuracy\" : np . mean ( accumulated_accuracy )} forward ( x ) Forward method for the method. Parameters: x ( Tensor ) \u2013 input. Returns: x ( Tensor ) \u2013 logits output. src/benchmark/methods/bc.py 49 50 51 52 53 54 55 56 57 58 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method for the method. Args: x (torch.Tensor): input. Returns: x (torch.Tensor): logits output. \"\"\" return self . policy ( x ) load ( path = None ) Load all model weights. Parameters: path ( str , default: None ) \u2013 where to look for the model's weights. Defaults to None. Raises: ValueError \u2013 if the path does not exist. src/benchmark/methods/bc.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def load ( self , path : str = None ) -> Self : \"\"\"Load all model weights. Args: path (str): where to look for the model's weights. Defaults to None. Raises: ValueError: if the path does not exist. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): raise ValueError ( \"Path does not exists.\" ) self . policy . load_state_dict ( torch . load ( f \" { path } best_model.ckpt\" , map_location = torch . device ( self . device ) ) ) return self save ( path = None ) Save all model weights. Parameters: path ( str , default: None ) \u2013 where to save the models. Defaults to None. src/benchmark/methods/bc.py 60 61 62 63 64 65 66 67 68 69 70 def save ( self , path : str = None ) -> None : \"\"\"Save all model weights. Args: path (str): where to save the models. Defaults to None. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): os . makedirs ( path ) torch . save ( self . policy . state_dict (), f \" { path } /best_model.ckpt\" ) train ( n_epochs , train_dataset , eval_dataset = None ) Train process. Parameters: n_epochs ( int ) \u2013 amount of epoch to run. train_dataset ( DataLoader ) \u2013 data to train. eval_dataset ( DataLoader , default: None ) \u2013 data to eval. Defaults to None. Returns: method ( Self ) \u2013 trained method. src/benchmark/methods/bc.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def train ( self , n_epochs : int , train_dataset : DataLoader , eval_dataset : DataLoader = None , ) -> Self : \"\"\"Train process. Args: n_epochs (int): amount of epoch to run. train_dataset (DataLoader): data to train. eval_dataset (DataLoader): data to eval. Defaults to None. Returns: method (Self): trained method. \"\"\" folder = f \"./benchmark_results/bc/ { self . environment_name } \" if not os . path . exists ( folder ): os . makedirs ( f \" { folder } /\" ) board = Tensorboard ( path = folder ) self . policy . to ( self . device ) best_model = - np . inf pbar = range ( n_epochs ) if self . verbose : pbar = tqdm ( pbar , desc = self . __method_name__ ) for epoch in pbar : train_metrics = self . _train ( train_dataset ) board . add_scalars ( \"Train\" , epoch = \"train\" , ** train_metrics ) if eval_dataset is not None : eval_metrics = self . _eval ( eval_dataset ) board . add_scalars ( \"Eval\" , epoch = \"eval\" , ** eval_metrics ) board . step ([ \"train\" , \"eval\" ]) else : board . step ( \"train\" ) if epoch % self . enjoy_criteria == 0 : metrics = self . _enjoy () board . add_scalars ( \"Enjoy\" , epoch = \"enjoy\" , ** metrics ) board . step ( \"enjoy\" ) if best_model < metrics [ \"aer\" ]: self . save () return self","title":"Behavioural Cloning"},{"location":"algorithms/bc/#behavioural-cloning","text":"Bases: Method Behavioural Clonning method based on (Pomerleau, 1988) Source code in src/benchmark/methods/bc.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 class BC ( Method ): \"\"\"Behavioural Clonning method based on (Pomerleau, 1988)\"\"\" __version__ = \"1.0.0\" __author__ = \"Pomerleau\" __method_name__ = \"Behavioural Cloning\" def __init__ ( self , environment : Env , enjoy_criteria : int = 100 , verbose : bool = False ) -> None : \"\"\"Initialize BC method.\"\"\" self . enjoy_criteria = enjoy_criteria self . verbose = verbose self . environment_name = environment . spec . name self . save_path = f \"./tmp/bc/ { self . environment_name } /\" self . hyperparameters = import_hyperparameters ( CONFIG_FILE , self . environment_name , ) super () . __init__ ( environment , self . hyperparameters ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method for the method. Args: x (torch.Tensor): input. Returns: x (torch.Tensor): logits output. \"\"\" return self . policy ( x ) def save ( self , path : str = None ) -> None : \"\"\"Save all model weights. Args: path (str): where to save the models. Defaults to None. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): os . makedirs ( path ) torch . save ( self . policy . state_dict (), f \" { path } /best_model.ckpt\" ) def load ( self , path : str = None ) -> Self : \"\"\"Load all model weights. Args: path (str): where to look for the model's weights. Defaults to None. Raises: ValueError: if the path does not exist. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): raise ValueError ( \"Path does not exists.\" ) self . policy . load_state_dict ( torch . load ( f \" { path } best_model.ckpt\" , map_location = torch . device ( self . device ) ) ) return self def train ( self , n_epochs : int , train_dataset : DataLoader , eval_dataset : DataLoader = None , ) -> Self : \"\"\"Train process. Args: n_epochs (int): amount of epoch to run. train_dataset (DataLoader): data to train. eval_dataset (DataLoader): data to eval. Defaults to None. Returns: method (Self): trained method. \"\"\" folder = f \"./benchmark_results/bc/ { self . environment_name } \" if not os . path . exists ( folder ): os . makedirs ( f \" { folder } /\" ) board = Tensorboard ( path = folder ) self . policy . to ( self . device ) best_model = - np . inf pbar = range ( n_epochs ) if self . verbose : pbar = tqdm ( pbar , desc = self . __method_name__ ) for epoch in pbar : train_metrics = self . _train ( train_dataset ) board . add_scalars ( \"Train\" , epoch = \"train\" , ** train_metrics ) if eval_dataset is not None : eval_metrics = self . _eval ( eval_dataset ) board . add_scalars ( \"Eval\" , epoch = \"eval\" , ** eval_metrics ) board . step ([ \"train\" , \"eval\" ]) else : board . step ( \"train\" ) if epoch % self . enjoy_criteria == 0 : metrics = self . _enjoy () board . add_scalars ( \"Enjoy\" , epoch = \"enjoy\" , ** metrics ) board . step ( \"enjoy\" ) if best_model < metrics [ \"aer\" ]: self . save () return self def _train ( self , dataset : DataLoader ) -> Metrics : \"\"\"Train loop. Args: dataset (DataLoader): train data. \"\"\" accumulated_loss = [] accumulated_accuracy = [] if not self . policy . training : self . policy . train () for batch in dataset : state , action , _ = batch state = state . to ( self . device ) action = action . to ( self . device ) self . optimizer_fn . zero_grad () predictions = self . forward ( state ) loss = self . loss_fn ( predictions , action . squeeze () . long ()) loss . backward () self . optimizer_fn . step () accumulated_loss . append ( loss . item ()) accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) return { \"loss\" : np . mean ( accumulated_loss ), \"accuracy\" : np . mean ( accumulated_accuracy )} def _eval ( self , dataset : DataLoader ) -> Metrics : \"\"\"Evaluation loop. Args: dataset (DataLoader): data to eval. \"\"\" accumulated_accuracy = [] if self . policy . training : self . policy . eval () for batch in dataset : state , action , _ = batch state = state . to ( self . device ) with torch . no_grad (): predictions = self . policy ( state ) accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) return { \"accuracy\" : np . mean ( accumulated_accuracy )}","title":"Behavioural Cloning"},{"location":"algorithms/bc/#benchmark.methods.bc.BC.__init__","text":"Initialize BC method. src/benchmark/methods/bc.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , environment : Env , enjoy_criteria : int = 100 , verbose : bool = False ) -> None : \"\"\"Initialize BC method.\"\"\" self . enjoy_criteria = enjoy_criteria self . verbose = verbose self . environment_name = environment . spec . name self . save_path = f \"./tmp/bc/ { self . environment_name } /\" self . hyperparameters = import_hyperparameters ( CONFIG_FILE , self . environment_name , ) super () . __init__ ( environment , self . hyperparameters )","title":"__init__()"},{"location":"algorithms/bc/#benchmark.methods.bc.BC._eval","text":"Evaluation loop. Parameters: dataset ( DataLoader ) \u2013 data to eval. src/benchmark/methods/bc.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def _eval ( self , dataset : DataLoader ) -> Metrics : \"\"\"Evaluation loop. Args: dataset (DataLoader): data to eval. \"\"\" accumulated_accuracy = [] if self . policy . training : self . policy . eval () for batch in dataset : state , action , _ = batch state = state . to ( self . device ) with torch . no_grad (): predictions = self . policy ( state ) accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) return { \"accuracy\" : np . mean ( accumulated_accuracy )}","title":"_eval()"},{"location":"algorithms/bc/#benchmark.methods.bc.BC._train","text":"Train loop. Parameters: dataset ( DataLoader ) \u2013 train data. src/benchmark/methods/bc.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def _train ( self , dataset : DataLoader ) -> Metrics : \"\"\"Train loop. Args: dataset (DataLoader): train data. \"\"\" accumulated_loss = [] accumulated_accuracy = [] if not self . policy . training : self . policy . train () for batch in dataset : state , action , _ = batch state = state . to ( self . device ) action = action . to ( self . device ) self . optimizer_fn . zero_grad () predictions = self . forward ( state ) loss = self . loss_fn ( predictions , action . squeeze () . long ()) loss . backward () self . optimizer_fn . step () accumulated_loss . append ( loss . item ()) accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) return { \"loss\" : np . mean ( accumulated_loss ), \"accuracy\" : np . mean ( accumulated_accuracy )}","title":"_train()"},{"location":"algorithms/bc/#benchmark.methods.bc.BC.forward","text":"Forward method for the method. Parameters: x ( Tensor ) \u2013 input. Returns: x ( Tensor ) \u2013 logits output. src/benchmark/methods/bc.py 49 50 51 52 53 54 55 56 57 58 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method for the method. Args: x (torch.Tensor): input. Returns: x (torch.Tensor): logits output. \"\"\" return self . policy ( x )","title":"forward()"},{"location":"algorithms/bc/#benchmark.methods.bc.BC.load","text":"Load all model weights. Parameters: path ( str , default: None ) \u2013 where to look for the model's weights. Defaults to None. Raises: ValueError \u2013 if the path does not exist. src/benchmark/methods/bc.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def load ( self , path : str = None ) -> Self : \"\"\"Load all model weights. Args: path (str): where to look for the model's weights. Defaults to None. Raises: ValueError: if the path does not exist. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): raise ValueError ( \"Path does not exists.\" ) self . policy . load_state_dict ( torch . load ( f \" { path } best_model.ckpt\" , map_location = torch . device ( self . device ) ) ) return self","title":"load()"},{"location":"algorithms/bc/#benchmark.methods.bc.BC.save","text":"Save all model weights. Parameters: path ( str , default: None ) \u2013 where to save the models. Defaults to None. src/benchmark/methods/bc.py 60 61 62 63 64 65 66 67 68 69 70 def save ( self , path : str = None ) -> None : \"\"\"Save all model weights. Args: path (str): where to save the models. Defaults to None. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): os . makedirs ( path ) torch . save ( self . policy . state_dict (), f \" { path } /best_model.ckpt\" )","title":"save()"},{"location":"algorithms/bc/#benchmark.methods.bc.BC.train","text":"Train process. Parameters: n_epochs ( int ) \u2013 amount of epoch to run. train_dataset ( DataLoader ) \u2013 data to train. eval_dataset ( DataLoader , default: None ) \u2013 data to eval. Defaults to None. Returns: method ( Self ) \u2013 trained method. src/benchmark/methods/bc.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def train ( self , n_epochs : int , train_dataset : DataLoader , eval_dataset : DataLoader = None , ) -> Self : \"\"\"Train process. Args: n_epochs (int): amount of epoch to run. train_dataset (DataLoader): data to train. eval_dataset (DataLoader): data to eval. Defaults to None. Returns: method (Self): trained method. \"\"\" folder = f \"./benchmark_results/bc/ { self . environment_name } \" if not os . path . exists ( folder ): os . makedirs ( f \" { folder } /\" ) board = Tensorboard ( path = folder ) self . policy . to ( self . device ) best_model = - np . inf pbar = range ( n_epochs ) if self . verbose : pbar = tqdm ( pbar , desc = self . __method_name__ ) for epoch in pbar : train_metrics = self . _train ( train_dataset ) board . add_scalars ( \"Train\" , epoch = \"train\" , ** train_metrics ) if eval_dataset is not None : eval_metrics = self . _eval ( eval_dataset ) board . add_scalars ( \"Eval\" , epoch = \"eval\" , ** eval_metrics ) board . step ([ \"train\" , \"eval\" ]) else : board . step ( \"train\" ) if epoch % self . enjoy_criteria == 0 : metrics = self . _enjoy () board . add_scalars ( \"Enjoy\" , epoch = \"enjoy\" , ** metrics ) board . step ( \"enjoy\" ) if best_model < metrics [ \"aer\" ]: self . save () return self","title":"train()"},{"location":"algorithms/bco/","text":"Behavioural Cloning from Observation Bases: Method Behavioural Cloning from Observation method based on (Torabi et. al., 2018) Source code in src/benchmark/methods/bco.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 class BCO ( Method ): \"\"\"Behavioural Cloning from Observation method based on (Torabi et. al., 2018)\"\"\" __version__ = \"1.0.0\" __author__ = \"Torabi et. al.\" __method_name__ = \"Behavioural Cloning from Observation\" def __init__ ( self , environment : Env , enjoy_criteria : int = 100 , verbose : bool = False ) -> None : \"\"\"Initialize BCO method.\"\"\" self . enjoy_criteria = enjoy_criteria self . verbose = verbose self . environment_name = environment . spec . name self . save_path = f \"./tmp/bco/ { self . environment_name } /\" self . hyperparameters = import_hyperparameters ( CONFIG_FILE , environment . spec . id , ) super () . __init__ ( environment , self . hyperparameters ) idm = self . hyperparameters . get ( 'idm' , 'MlpPolicy' ) if idm == 'MlpPolicy' : self . idm = MLP ( self . observation_size * 2 , self . action_size ) elif idm == 'MlpWithAttention' : self . idm = MlpWithAttention ( self . observation_size * 2 , self . action_size ) self . idm_optimizer = optim . Adam ( self . idm . parameters (), lr = self . hyperparameters [ 'idm_lr' ]) self . idm_loss = nn . CrossEntropyLoss () if self . discrete else nn . MSELoss () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method for the method. Args: x (torch.Tensor): input. Returns: x (torch.Tensor): logits output. \"\"\" return self . policy ( x ) def save ( self , path : str = None ) -> None : \"\"\"Save all models weights. Args: path (str): where to save the models. Defaults to None. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): os . makedirs ( path ) torch . save ( self . policy . state_dict (), f \" { path } /best_model.ckpt\" ) torch . save ( self . idm . state_dict (), f \" { path } /idm.ckpt\" ) def load ( self , path : str = None ) -> Self : \"\"\"Load all model weights. Args: path (str): where to look for the model's weights. Defaults to None. Raises: ValueError: if the path does not exist. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): raise ValueError ( \"Path does not exists.\" ) self . policy . load_state_dict ( torch . load ( f \" { path } best_model.ckpt\" , map_location = torch . device ( self . device ) ) ) self . idm . load_state_dict ( torch . load ( f \" { path } /idm.ckpt\" , map_location = torch . device ( self . device ) ) ) return self def train ( self , n_epochs : int , train_dataset : Dict [ str , DataLoader ], eval_dataset : Dict [ str , DataLoader ] = None , folder : str = None ) -> Self : \"\"\"Train process. Args: n_epochs (int): amount of epoch to run. train_dataset (DataLoader): data to train. eval_dataset (DataLoader): data to eval. Defaults to None. Returns: method (Self): trained method. \"\"\" if folder is None : folder = f \"../benchmark_results/bco/ { self . environment_name } \" if not os . path . exists ( folder ): os . makedirs ( f \" { folder } /\" ) board = Tensorboard ( path = folder ) self . policy . to ( self . device ) self . idm . to ( self . device ) best_model = - np . inf if not isinstance ( train_dataset , dict ): train_dataset = { \"expert_dataset\" : train_dataset } if \"idm_dataset\" not in train_dataset . keys (): print ( \"No random dataset found\" ) random_path = f \"./dataset/random_ { self . environment . spec . id } \" if not os . path . exists ( random_path ): print ( \"Creating random dataset from scratch\" ) train_dataset [ \"idm_dataset\" ] = get_random_dataset ( environment_name = self . environment . spec . id , episodes = self . hyperparameters [ \"random_episodes\" ] ) else : print ( \"Loading local random dataset\" ) train_dataset [ \"idm_dataset\" ] = BaselineDataset ( f \" { random_path } /teacher.npz\" ) train_dataset [ \"idm_dataset\" ] = DataLoader ( train_dataset [ \"idm_dataset\" ], batch_size = train_dataset [ \"expert_dataset\" ] . batch_size , shuffle = True ) pbar = range ( n_epochs ) if self . verbose : pbar = tqdm ( pbar ) for epoch in pbar : train_metrics = self . _train ( ** train_dataset ) board . add_scalars ( \"Train\" , epoch = \"train\" , ** train_metrics ) if eval_dataset is not None : eval_metrics = self . _eval ( eval_dataset ) board . add_scalars ( \"Eval\" , epoch = \"eval\" , ** eval_metrics ) board . step ([ \"train\" , \"eval\" ]) else : board . step ( \"train\" ) if epoch % self . enjoy_criteria == 0 : train_dataset = self . _append_samples ( train_dataset ) if epoch % self . enjoy_criteria == 0 or epoch + 1 == n_epochs : metrics = self . _enjoy () board . add_scalars ( \"Enjoy\" , epoch = \"enjoy\" , ** metrics ) board . step ( \"enjoy\" ) if best_model < metrics [ \"aer\" ]: self . save () return self def _append_samples ( self , train_dataset : DataLoader ) -> DataLoader : \"\"\"Append samples to DataLoader. Args: train_dataset (DataLoader): current train dataset. Returns: train_dataset (DataLoader): new train dataset. \"\"\" _ , i_pos = self . _enjoy ( return_ipos = True ) train_dataset [ 'idm_dataset' ] . dataset . states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . states , torch . from_numpy ( i_pos [ 'states' ])), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . next_states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . next_states , torch . from_numpy ( i_pos [ 'next_states' ])), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . actions = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . actions , torch . from_numpy ( i_pos [ 'actions' ] . reshape (( - 1 , 1 )))), dim = 0 ) return train_dataset def _train ( self , idm_dataset : DataLoader , expert_dataset : DataLoader ) -> Metrics : \"\"\"Train loop. Args: dataset (DataLoader): train data. \"\"\" if not self . idm . training : self . idm . train () if not self . policy . training : self . policy . train () idm_accumulated_loss = [] idm_accumulated_accuracy = [] accumulated_loss = [] accumulated_accuracy = [] for batch in idm_dataset : state , action , next_state = batch state = state . to ( self . device ) action = action . to ( self . device ) next_state = next_state . to ( self . device ) self . idm_optimizer . zero_grad () predictions = self . idm ( torch . cat (( state , next_state ), dim = 1 )) loss = self . idm_loss ( predictions , action . squeeze () . long ()) loss . backward () idm_accumulated_loss . append ( loss . item ()) self . idm_optimizer . step () accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () idm_accumulated_accuracy . append ( accuracy ) self . idm . eval () for batch in expert_dataset : state , _ , next_state = batch state = state . to ( self . device ) next_state = next_state . to ( self . device ) with torch . no_grad (): if self . discrete : action = self . idm ( torch . cat (( state , next_state ), dim = 1 )) action = torch . argmax ( action , dim = 1 ) else : action = self . idm ( torch . cat (( state , next_state ), dim = 1 )) self . optimizer_fn . zero_grad () predictions = self . forward ( state ) loss = self . loss_fn ( predictions , action . squeeze () . long ()) loss . backward () accumulated_loss . append ( loss . item ()) self . optimizer_fn . step () accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) metrics = { \"idm_loss\" : np . mean ( idm_accumulated_loss ), \"idm_accuracy\" : np . mean ( idm_accumulated_accuracy ), \"loss\" : np . mean ( accumulated_loss ), \"accuracy\" : np . mean ( accumulated_accuracy ) } return metrics def _eval ( self , dataset : DataLoader ) -> Metrics : \"\"\"Evaluation loop. Args: dataset (DataLoader): data to eval. \"\"\" if self . policy . training : self . policy . eval () accumulated_accuracy = [] for batch in dataset : state , action , _ = batch state = state . to ( self . device ) with torch . no_grad (): predictions = self . policy ( state ) accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) return { \"accuracy\" : np . mean ( accumulated_accuracy )} def _enjoy ( self , render : bool = False , teacher_reward : Number = None , random_reward : Number = None , return_ipos : bool = False , ) -> Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]]: \"\"\"Function for evaluation of the policy in the environment Args: render (bool): Whether it should render. Defaults to False. teacher_reward (Number): reward for teacher policy. random_reward (Number): reward for a random policy. return_ipos (bool): whether it should return data to append to I_pos. Returns: Metrics: aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. I_pos: states (List[Number]): states before action. actions (List[Number]): action given states. next_states (List[Number]): next state given states and actions. \"\"\" environment = GymWrapper ( self . environment ) average_reward = [] i_pos = defaultdict ( list ) for _ in range ( 100 ): done = False obs = environment . reset () accumulated_reward = 0 while not done : if render : environment . render () action = self . predict ( obs ) i_pos [ 'states' ] . append ( obs ) i_pos [ 'actions' ] . append ( action ) obs , reward , done , * _ = environment . step ( action ) accumulated_reward += reward i_pos [ 'next_states' ] . append ( obs ) average_reward . append ( accumulated_reward ) metrics = average_episodic_reward ( average_reward ) if teacher_reward is not None and random_reward is not None : metrics . update ( performance ( average_reward , teacher_reward , random_reward )) i_pos = { key : np . array ( value ) for key , value in i_pos . items ()} if return_ipos : return metrics , i_pos return metrics __init__ ( environment , enjoy_criteria = 100 , verbose = False ) Initialize BCO method. src/benchmark/methods/bco.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , environment : Env , enjoy_criteria : int = 100 , verbose : bool = False ) -> None : \"\"\"Initialize BCO method.\"\"\" self . enjoy_criteria = enjoy_criteria self . verbose = verbose self . environment_name = environment . spec . name self . save_path = f \"./tmp/bco/ { self . environment_name } /\" self . hyperparameters = import_hyperparameters ( CONFIG_FILE , environment . spec . id , ) super () . __init__ ( environment , self . hyperparameters ) idm = self . hyperparameters . get ( 'idm' , 'MlpPolicy' ) if idm == 'MlpPolicy' : self . idm = MLP ( self . observation_size * 2 , self . action_size ) elif idm == 'MlpWithAttention' : self . idm = MlpWithAttention ( self . observation_size * 2 , self . action_size ) self . idm_optimizer = optim . Adam ( self . idm . parameters (), lr = self . hyperparameters [ 'idm_lr' ]) self . idm_loss = nn . CrossEntropyLoss () if self . discrete else nn . MSELoss () _append_samples ( train_dataset ) Append samples to DataLoader. Parameters: train_dataset ( DataLoader ) \u2013 current train dataset. Returns: train_dataset ( DataLoader ) \u2013 new train dataset. src/benchmark/methods/bco.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def _append_samples ( self , train_dataset : DataLoader ) -> DataLoader : \"\"\"Append samples to DataLoader. Args: train_dataset (DataLoader): current train dataset. Returns: train_dataset (DataLoader): new train dataset. \"\"\" _ , i_pos = self . _enjoy ( return_ipos = True ) train_dataset [ 'idm_dataset' ] . dataset . states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . states , torch . from_numpy ( i_pos [ 'states' ])), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . next_states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . next_states , torch . from_numpy ( i_pos [ 'next_states' ])), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . actions = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . actions , torch . from_numpy ( i_pos [ 'actions' ] . reshape (( - 1 , 1 )))), dim = 0 ) return train_dataset _enjoy ( render = False , teacher_reward = None , random_reward = None , return_ipos = False ) Function for evaluation of the policy in the environment Parameters: render ( bool , default: False ) \u2013 Whether it should render. Defaults to False. teacher_reward ( Number , default: None ) \u2013 reward for teacher policy. random_reward ( Number , default: None ) \u2013 reward for a random policy. return_ipos ( bool , default: False ) \u2013 whether it should return data to append to I_pos. Returns: Metrics ( Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]] ) \u2013 aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. I_pos ( Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]] ) \u2013 states (List[Number]): states before action. actions (List[Number]): action given states. next_states (List[Number]): next state given states and actions. src/benchmark/methods/bco.py 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def _enjoy ( self , render : bool = False , teacher_reward : Number = None , random_reward : Number = None , return_ipos : bool = False , ) -> Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]]: \"\"\"Function for evaluation of the policy in the environment Args: render (bool): Whether it should render. Defaults to False. teacher_reward (Number): reward for teacher policy. random_reward (Number): reward for a random policy. return_ipos (bool): whether it should return data to append to I_pos. Returns: Metrics: aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. I_pos: states (List[Number]): states before action. actions (List[Number]): action given states. next_states (List[Number]): next state given states and actions. \"\"\" environment = GymWrapper ( self . environment ) average_reward = [] i_pos = defaultdict ( list ) for _ in range ( 100 ): done = False obs = environment . reset () accumulated_reward = 0 while not done : if render : environment . render () action = self . predict ( obs ) i_pos [ 'states' ] . append ( obs ) i_pos [ 'actions' ] . append ( action ) obs , reward , done , * _ = environment . step ( action ) accumulated_reward += reward i_pos [ 'next_states' ] . append ( obs ) average_reward . append ( accumulated_reward ) metrics = average_episodic_reward ( average_reward ) if teacher_reward is not None and random_reward is not None : metrics . update ( performance ( average_reward , teacher_reward , random_reward )) i_pos = { key : np . array ( value ) for key , value in i_pos . items ()} if return_ipos : return metrics , i_pos return metrics _eval ( dataset ) Evaluation loop. Parameters: dataset ( DataLoader ) \u2013 data to eval. src/benchmark/methods/bco.py 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 def _eval ( self , dataset : DataLoader ) -> Metrics : \"\"\"Evaluation loop. Args: dataset (DataLoader): data to eval. \"\"\" if self . policy . training : self . policy . eval () accumulated_accuracy = [] for batch in dataset : state , action , _ = batch state = state . to ( self . device ) with torch . no_grad (): predictions = self . policy ( state ) accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) return { \"accuracy\" : np . mean ( accumulated_accuracy )} _train ( idm_dataset , expert_dataset ) Train loop. Parameters: dataset ( DataLoader ) \u2013 train data. src/benchmark/methods/bco.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def _train ( self , idm_dataset : DataLoader , expert_dataset : DataLoader ) -> Metrics : \"\"\"Train loop. Args: dataset (DataLoader): train data. \"\"\" if not self . idm . training : self . idm . train () if not self . policy . training : self . policy . train () idm_accumulated_loss = [] idm_accumulated_accuracy = [] accumulated_loss = [] accumulated_accuracy = [] for batch in idm_dataset : state , action , next_state = batch state = state . to ( self . device ) action = action . to ( self . device ) next_state = next_state . to ( self . device ) self . idm_optimizer . zero_grad () predictions = self . idm ( torch . cat (( state , next_state ), dim = 1 )) loss = self . idm_loss ( predictions , action . squeeze () . long ()) loss . backward () idm_accumulated_loss . append ( loss . item ()) self . idm_optimizer . step () accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () idm_accumulated_accuracy . append ( accuracy ) self . idm . eval () for batch in expert_dataset : state , _ , next_state = batch state = state . to ( self . device ) next_state = next_state . to ( self . device ) with torch . no_grad (): if self . discrete : action = self . idm ( torch . cat (( state , next_state ), dim = 1 )) action = torch . argmax ( action , dim = 1 ) else : action = self . idm ( torch . cat (( state , next_state ), dim = 1 )) self . optimizer_fn . zero_grad () predictions = self . forward ( state ) loss = self . loss_fn ( predictions , action . squeeze () . long ()) loss . backward () accumulated_loss . append ( loss . item ()) self . optimizer_fn . step () accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) metrics = { \"idm_loss\" : np . mean ( idm_accumulated_loss ), \"idm_accuracy\" : np . mean ( idm_accumulated_accuracy ), \"loss\" : np . mean ( accumulated_loss ), \"accuracy\" : np . mean ( accumulated_accuracy ) } return metrics forward ( x ) Forward method for the method. Parameters: x ( Tensor ) \u2013 input. Returns: x ( Tensor ) \u2013 logits output. src/benchmark/methods/bco.py 65 66 67 68 69 70 71 72 73 74 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method for the method. Args: x (torch.Tensor): input. Returns: x (torch.Tensor): logits output. \"\"\" return self . policy ( x ) load ( path = None ) Load all model weights. Parameters: path ( str , default: None ) \u2013 where to look for the model's weights. Defaults to None. Raises: ValueError \u2013 if the path does not exist. src/benchmark/methods/bco.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def load ( self , path : str = None ) -> Self : \"\"\"Load all model weights. Args: path (str): where to look for the model's weights. Defaults to None. Raises: ValueError: if the path does not exist. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): raise ValueError ( \"Path does not exists.\" ) self . policy . load_state_dict ( torch . load ( f \" { path } best_model.ckpt\" , map_location = torch . device ( self . device ) ) ) self . idm . load_state_dict ( torch . load ( f \" { path } /idm.ckpt\" , map_location = torch . device ( self . device ) ) ) return self save ( path = None ) Save all models weights. Parameters: path ( str , default: None ) \u2013 where to save the models. Defaults to None. src/benchmark/methods/bco.py 76 77 78 79 80 81 82 83 84 85 86 87 def save ( self , path : str = None ) -> None : \"\"\"Save all models weights. Args: path (str): where to save the models. Defaults to None. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): os . makedirs ( path ) torch . save ( self . policy . state_dict (), f \" { path } /best_model.ckpt\" ) torch . save ( self . idm . state_dict (), f \" { path } /idm.ckpt\" ) train ( n_epochs , train_dataset , eval_dataset = None , folder = None ) Train process. Parameters: n_epochs ( int ) \u2013 amount of epoch to run. train_dataset ( DataLoader ) \u2013 data to train. eval_dataset ( DataLoader , default: None ) \u2013 data to eval. Defaults to None. Returns: method ( Self ) \u2013 trained method. src/benchmark/methods/bco.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def train ( self , n_epochs : int , train_dataset : Dict [ str , DataLoader ], eval_dataset : Dict [ str , DataLoader ] = None , folder : str = None ) -> Self : \"\"\"Train process. Args: n_epochs (int): amount of epoch to run. train_dataset (DataLoader): data to train. eval_dataset (DataLoader): data to eval. Defaults to None. Returns: method (Self): trained method. \"\"\" if folder is None : folder = f \"../benchmark_results/bco/ { self . environment_name } \" if not os . path . exists ( folder ): os . makedirs ( f \" { folder } /\" ) board = Tensorboard ( path = folder ) self . policy . to ( self . device ) self . idm . to ( self . device ) best_model = - np . inf if not isinstance ( train_dataset , dict ): train_dataset = { \"expert_dataset\" : train_dataset } if \"idm_dataset\" not in train_dataset . keys (): print ( \"No random dataset found\" ) random_path = f \"./dataset/random_ { self . environment . spec . id } \" if not os . path . exists ( random_path ): print ( \"Creating random dataset from scratch\" ) train_dataset [ \"idm_dataset\" ] = get_random_dataset ( environment_name = self . environment . spec . id , episodes = self . hyperparameters [ \"random_episodes\" ] ) else : print ( \"Loading local random dataset\" ) train_dataset [ \"idm_dataset\" ] = BaselineDataset ( f \" { random_path } /teacher.npz\" ) train_dataset [ \"idm_dataset\" ] = DataLoader ( train_dataset [ \"idm_dataset\" ], batch_size = train_dataset [ \"expert_dataset\" ] . batch_size , shuffle = True ) pbar = range ( n_epochs ) if self . verbose : pbar = tqdm ( pbar ) for epoch in pbar : train_metrics = self . _train ( ** train_dataset ) board . add_scalars ( \"Train\" , epoch = \"train\" , ** train_metrics ) if eval_dataset is not None : eval_metrics = self . _eval ( eval_dataset ) board . add_scalars ( \"Eval\" , epoch = \"eval\" , ** eval_metrics ) board . step ([ \"train\" , \"eval\" ]) else : board . step ( \"train\" ) if epoch % self . enjoy_criteria == 0 : train_dataset = self . _append_samples ( train_dataset ) if epoch % self . enjoy_criteria == 0 or epoch + 1 == n_epochs : metrics = self . _enjoy () board . add_scalars ( \"Enjoy\" , epoch = \"enjoy\" , ** metrics ) board . step ( \"enjoy\" ) if best_model < metrics [ \"aer\" ]: self . save () return self","title":"Behavioural Cloning from Observation"},{"location":"algorithms/bco/#behavioural-cloning-from-observation","text":"Bases: Method Behavioural Cloning from Observation method based on (Torabi et. al., 2018) Source code in src/benchmark/methods/bco.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 class BCO ( Method ): \"\"\"Behavioural Cloning from Observation method based on (Torabi et. al., 2018)\"\"\" __version__ = \"1.0.0\" __author__ = \"Torabi et. al.\" __method_name__ = \"Behavioural Cloning from Observation\" def __init__ ( self , environment : Env , enjoy_criteria : int = 100 , verbose : bool = False ) -> None : \"\"\"Initialize BCO method.\"\"\" self . enjoy_criteria = enjoy_criteria self . verbose = verbose self . environment_name = environment . spec . name self . save_path = f \"./tmp/bco/ { self . environment_name } /\" self . hyperparameters = import_hyperparameters ( CONFIG_FILE , environment . spec . id , ) super () . __init__ ( environment , self . hyperparameters ) idm = self . hyperparameters . get ( 'idm' , 'MlpPolicy' ) if idm == 'MlpPolicy' : self . idm = MLP ( self . observation_size * 2 , self . action_size ) elif idm == 'MlpWithAttention' : self . idm = MlpWithAttention ( self . observation_size * 2 , self . action_size ) self . idm_optimizer = optim . Adam ( self . idm . parameters (), lr = self . hyperparameters [ 'idm_lr' ]) self . idm_loss = nn . CrossEntropyLoss () if self . discrete else nn . MSELoss () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method for the method. Args: x (torch.Tensor): input. Returns: x (torch.Tensor): logits output. \"\"\" return self . policy ( x ) def save ( self , path : str = None ) -> None : \"\"\"Save all models weights. Args: path (str): where to save the models. Defaults to None. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): os . makedirs ( path ) torch . save ( self . policy . state_dict (), f \" { path } /best_model.ckpt\" ) torch . save ( self . idm . state_dict (), f \" { path } /idm.ckpt\" ) def load ( self , path : str = None ) -> Self : \"\"\"Load all model weights. Args: path (str): where to look for the model's weights. Defaults to None. Raises: ValueError: if the path does not exist. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): raise ValueError ( \"Path does not exists.\" ) self . policy . load_state_dict ( torch . load ( f \" { path } best_model.ckpt\" , map_location = torch . device ( self . device ) ) ) self . idm . load_state_dict ( torch . load ( f \" { path } /idm.ckpt\" , map_location = torch . device ( self . device ) ) ) return self def train ( self , n_epochs : int , train_dataset : Dict [ str , DataLoader ], eval_dataset : Dict [ str , DataLoader ] = None , folder : str = None ) -> Self : \"\"\"Train process. Args: n_epochs (int): amount of epoch to run. train_dataset (DataLoader): data to train. eval_dataset (DataLoader): data to eval. Defaults to None. Returns: method (Self): trained method. \"\"\" if folder is None : folder = f \"../benchmark_results/bco/ { self . environment_name } \" if not os . path . exists ( folder ): os . makedirs ( f \" { folder } /\" ) board = Tensorboard ( path = folder ) self . policy . to ( self . device ) self . idm . to ( self . device ) best_model = - np . inf if not isinstance ( train_dataset , dict ): train_dataset = { \"expert_dataset\" : train_dataset } if \"idm_dataset\" not in train_dataset . keys (): print ( \"No random dataset found\" ) random_path = f \"./dataset/random_ { self . environment . spec . id } \" if not os . path . exists ( random_path ): print ( \"Creating random dataset from scratch\" ) train_dataset [ \"idm_dataset\" ] = get_random_dataset ( environment_name = self . environment . spec . id , episodes = self . hyperparameters [ \"random_episodes\" ] ) else : print ( \"Loading local random dataset\" ) train_dataset [ \"idm_dataset\" ] = BaselineDataset ( f \" { random_path } /teacher.npz\" ) train_dataset [ \"idm_dataset\" ] = DataLoader ( train_dataset [ \"idm_dataset\" ], batch_size = train_dataset [ \"expert_dataset\" ] . batch_size , shuffle = True ) pbar = range ( n_epochs ) if self . verbose : pbar = tqdm ( pbar ) for epoch in pbar : train_metrics = self . _train ( ** train_dataset ) board . add_scalars ( \"Train\" , epoch = \"train\" , ** train_metrics ) if eval_dataset is not None : eval_metrics = self . _eval ( eval_dataset ) board . add_scalars ( \"Eval\" , epoch = \"eval\" , ** eval_metrics ) board . step ([ \"train\" , \"eval\" ]) else : board . step ( \"train\" ) if epoch % self . enjoy_criteria == 0 : train_dataset = self . _append_samples ( train_dataset ) if epoch % self . enjoy_criteria == 0 or epoch + 1 == n_epochs : metrics = self . _enjoy () board . add_scalars ( \"Enjoy\" , epoch = \"enjoy\" , ** metrics ) board . step ( \"enjoy\" ) if best_model < metrics [ \"aer\" ]: self . save () return self def _append_samples ( self , train_dataset : DataLoader ) -> DataLoader : \"\"\"Append samples to DataLoader. Args: train_dataset (DataLoader): current train dataset. Returns: train_dataset (DataLoader): new train dataset. \"\"\" _ , i_pos = self . _enjoy ( return_ipos = True ) train_dataset [ 'idm_dataset' ] . dataset . states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . states , torch . from_numpy ( i_pos [ 'states' ])), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . next_states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . next_states , torch . from_numpy ( i_pos [ 'next_states' ])), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . actions = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . actions , torch . from_numpy ( i_pos [ 'actions' ] . reshape (( - 1 , 1 )))), dim = 0 ) return train_dataset def _train ( self , idm_dataset : DataLoader , expert_dataset : DataLoader ) -> Metrics : \"\"\"Train loop. Args: dataset (DataLoader): train data. \"\"\" if not self . idm . training : self . idm . train () if not self . policy . training : self . policy . train () idm_accumulated_loss = [] idm_accumulated_accuracy = [] accumulated_loss = [] accumulated_accuracy = [] for batch in idm_dataset : state , action , next_state = batch state = state . to ( self . device ) action = action . to ( self . device ) next_state = next_state . to ( self . device ) self . idm_optimizer . zero_grad () predictions = self . idm ( torch . cat (( state , next_state ), dim = 1 )) loss = self . idm_loss ( predictions , action . squeeze () . long ()) loss . backward () idm_accumulated_loss . append ( loss . item ()) self . idm_optimizer . step () accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () idm_accumulated_accuracy . append ( accuracy ) self . idm . eval () for batch in expert_dataset : state , _ , next_state = batch state = state . to ( self . device ) next_state = next_state . to ( self . device ) with torch . no_grad (): if self . discrete : action = self . idm ( torch . cat (( state , next_state ), dim = 1 )) action = torch . argmax ( action , dim = 1 ) else : action = self . idm ( torch . cat (( state , next_state ), dim = 1 )) self . optimizer_fn . zero_grad () predictions = self . forward ( state ) loss = self . loss_fn ( predictions , action . squeeze () . long ()) loss . backward () accumulated_loss . append ( loss . item ()) self . optimizer_fn . step () accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) metrics = { \"idm_loss\" : np . mean ( idm_accumulated_loss ), \"idm_accuracy\" : np . mean ( idm_accumulated_accuracy ), \"loss\" : np . mean ( accumulated_loss ), \"accuracy\" : np . mean ( accumulated_accuracy ) } return metrics def _eval ( self , dataset : DataLoader ) -> Metrics : \"\"\"Evaluation loop. Args: dataset (DataLoader): data to eval. \"\"\" if self . policy . training : self . policy . eval () accumulated_accuracy = [] for batch in dataset : state , action , _ = batch state = state . to ( self . device ) with torch . no_grad (): predictions = self . policy ( state ) accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) return { \"accuracy\" : np . mean ( accumulated_accuracy )} def _enjoy ( self , render : bool = False , teacher_reward : Number = None , random_reward : Number = None , return_ipos : bool = False , ) -> Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]]: \"\"\"Function for evaluation of the policy in the environment Args: render (bool): Whether it should render. Defaults to False. teacher_reward (Number): reward for teacher policy. random_reward (Number): reward for a random policy. return_ipos (bool): whether it should return data to append to I_pos. Returns: Metrics: aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. I_pos: states (List[Number]): states before action. actions (List[Number]): action given states. next_states (List[Number]): next state given states and actions. \"\"\" environment = GymWrapper ( self . environment ) average_reward = [] i_pos = defaultdict ( list ) for _ in range ( 100 ): done = False obs = environment . reset () accumulated_reward = 0 while not done : if render : environment . render () action = self . predict ( obs ) i_pos [ 'states' ] . append ( obs ) i_pos [ 'actions' ] . append ( action ) obs , reward , done , * _ = environment . step ( action ) accumulated_reward += reward i_pos [ 'next_states' ] . append ( obs ) average_reward . append ( accumulated_reward ) metrics = average_episodic_reward ( average_reward ) if teacher_reward is not None and random_reward is not None : metrics . update ( performance ( average_reward , teacher_reward , random_reward )) i_pos = { key : np . array ( value ) for key , value in i_pos . items ()} if return_ipos : return metrics , i_pos return metrics","title":"Behavioural Cloning from Observation"},{"location":"algorithms/bco/#benchmark.methods.bco.BCO.__init__","text":"Initialize BCO method. src/benchmark/methods/bco.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , environment : Env , enjoy_criteria : int = 100 , verbose : bool = False ) -> None : \"\"\"Initialize BCO method.\"\"\" self . enjoy_criteria = enjoy_criteria self . verbose = verbose self . environment_name = environment . spec . name self . save_path = f \"./tmp/bco/ { self . environment_name } /\" self . hyperparameters = import_hyperparameters ( CONFIG_FILE , environment . spec . id , ) super () . __init__ ( environment , self . hyperparameters ) idm = self . hyperparameters . get ( 'idm' , 'MlpPolicy' ) if idm == 'MlpPolicy' : self . idm = MLP ( self . observation_size * 2 , self . action_size ) elif idm == 'MlpWithAttention' : self . idm = MlpWithAttention ( self . observation_size * 2 , self . action_size ) self . idm_optimizer = optim . Adam ( self . idm . parameters (), lr = self . hyperparameters [ 'idm_lr' ]) self . idm_loss = nn . CrossEntropyLoss () if self . discrete else nn . MSELoss ()","title":"__init__()"},{"location":"algorithms/bco/#benchmark.methods.bco.BCO._append_samples","text":"Append samples to DataLoader. Parameters: train_dataset ( DataLoader ) \u2013 current train dataset. Returns: train_dataset ( DataLoader ) \u2013 new train dataset. src/benchmark/methods/bco.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def _append_samples ( self , train_dataset : DataLoader ) -> DataLoader : \"\"\"Append samples to DataLoader. Args: train_dataset (DataLoader): current train dataset. Returns: train_dataset (DataLoader): new train dataset. \"\"\" _ , i_pos = self . _enjoy ( return_ipos = True ) train_dataset [ 'idm_dataset' ] . dataset . states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . states , torch . from_numpy ( i_pos [ 'states' ])), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . next_states = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . next_states , torch . from_numpy ( i_pos [ 'next_states' ])), dim = 0 ) train_dataset [ 'idm_dataset' ] . dataset . actions = torch . cat (( train_dataset [ 'idm_dataset' ] . dataset . actions , torch . from_numpy ( i_pos [ 'actions' ] . reshape (( - 1 , 1 )))), dim = 0 ) return train_dataset","title":"_append_samples()"},{"location":"algorithms/bco/#benchmark.methods.bco.BCO._enjoy","text":"Function for evaluation of the policy in the environment Parameters: render ( bool , default: False ) \u2013 Whether it should render. Defaults to False. teacher_reward ( Number , default: None ) \u2013 reward for teacher policy. random_reward ( Number , default: None ) \u2013 reward for a random policy. return_ipos ( bool , default: False ) \u2013 whether it should return data to append to I_pos. Returns: Metrics ( Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]] ) \u2013 aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. I_pos ( Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]] ) \u2013 states (List[Number]): states before action. actions (List[Number]): action given states. next_states (List[Number]): next state given states and actions. src/benchmark/methods/bco.py 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def _enjoy ( self , render : bool = False , teacher_reward : Number = None , random_reward : Number = None , return_ipos : bool = False , ) -> Union [ Metrics , Tuple [ Metrics , Dict [ str , List [ float ]]]]: \"\"\"Function for evaluation of the policy in the environment Args: render (bool): Whether it should render. Defaults to False. teacher_reward (Number): reward for teacher policy. random_reward (Number): reward for a random policy. return_ipos (bool): whether it should return data to append to I_pos. Returns: Metrics: aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. I_pos: states (List[Number]): states before action. actions (List[Number]): action given states. next_states (List[Number]): next state given states and actions. \"\"\" environment = GymWrapper ( self . environment ) average_reward = [] i_pos = defaultdict ( list ) for _ in range ( 100 ): done = False obs = environment . reset () accumulated_reward = 0 while not done : if render : environment . render () action = self . predict ( obs ) i_pos [ 'states' ] . append ( obs ) i_pos [ 'actions' ] . append ( action ) obs , reward , done , * _ = environment . step ( action ) accumulated_reward += reward i_pos [ 'next_states' ] . append ( obs ) average_reward . append ( accumulated_reward ) metrics = average_episodic_reward ( average_reward ) if teacher_reward is not None and random_reward is not None : metrics . update ( performance ( average_reward , teacher_reward , random_reward )) i_pos = { key : np . array ( value ) for key , value in i_pos . items ()} if return_ipos : return metrics , i_pos return metrics","title":"_enjoy()"},{"location":"algorithms/bco/#benchmark.methods.bco.BCO._eval","text":"Evaluation loop. Parameters: dataset ( DataLoader ) \u2013 data to eval. src/benchmark/methods/bco.py 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 def _eval ( self , dataset : DataLoader ) -> Metrics : \"\"\"Evaluation loop. Args: dataset (DataLoader): data to eval. \"\"\" if self . policy . training : self . policy . eval () accumulated_accuracy = [] for batch in dataset : state , action , _ = batch state = state . to ( self . device ) with torch . no_grad (): predictions = self . policy ( state ) accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) return { \"accuracy\" : np . mean ( accumulated_accuracy )}","title":"_eval()"},{"location":"algorithms/bco/#benchmark.methods.bco.BCO._train","text":"Train loop. Parameters: dataset ( DataLoader ) \u2013 train data. src/benchmark/methods/bco.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def _train ( self , idm_dataset : DataLoader , expert_dataset : DataLoader ) -> Metrics : \"\"\"Train loop. Args: dataset (DataLoader): train data. \"\"\" if not self . idm . training : self . idm . train () if not self . policy . training : self . policy . train () idm_accumulated_loss = [] idm_accumulated_accuracy = [] accumulated_loss = [] accumulated_accuracy = [] for batch in idm_dataset : state , action , next_state = batch state = state . to ( self . device ) action = action . to ( self . device ) next_state = next_state . to ( self . device ) self . idm_optimizer . zero_grad () predictions = self . idm ( torch . cat (( state , next_state ), dim = 1 )) loss = self . idm_loss ( predictions , action . squeeze () . long ()) loss . backward () idm_accumulated_loss . append ( loss . item ()) self . idm_optimizer . step () accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () idm_accumulated_accuracy . append ( accuracy ) self . idm . eval () for batch in expert_dataset : state , _ , next_state = batch state = state . to ( self . device ) next_state = next_state . to ( self . device ) with torch . no_grad (): if self . discrete : action = self . idm ( torch . cat (( state , next_state ), dim = 1 )) action = torch . argmax ( action , dim = 1 ) else : action = self . idm ( torch . cat (( state , next_state ), dim = 1 )) self . optimizer_fn . zero_grad () predictions = self . forward ( state ) loss = self . loss_fn ( predictions , action . squeeze () . long ()) loss . backward () accumulated_loss . append ( loss . item ()) self . optimizer_fn . step () accuracy : Number = None if self . discrete : accuracy = accuracy_fn ( predictions , action . squeeze ()) else : accuracy = ( action - predictions ) . pow ( 2 ) . sum ( 1 ) . sqrt () . mean () . item () accumulated_accuracy . append ( accuracy ) metrics = { \"idm_loss\" : np . mean ( idm_accumulated_loss ), \"idm_accuracy\" : np . mean ( idm_accumulated_accuracy ), \"loss\" : np . mean ( accumulated_loss ), \"accuracy\" : np . mean ( accumulated_accuracy ) } return metrics","title":"_train()"},{"location":"algorithms/bco/#benchmark.methods.bco.BCO.forward","text":"Forward method for the method. Parameters: x ( Tensor ) \u2013 input. Returns: x ( Tensor ) \u2013 logits output. src/benchmark/methods/bco.py 65 66 67 68 69 70 71 72 73 74 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method for the method. Args: x (torch.Tensor): input. Returns: x (torch.Tensor): logits output. \"\"\" return self . policy ( x )","title":"forward()"},{"location":"algorithms/bco/#benchmark.methods.bco.BCO.load","text":"Load all model weights. Parameters: path ( str , default: None ) \u2013 where to look for the model's weights. Defaults to None. Raises: ValueError \u2013 if the path does not exist. src/benchmark/methods/bco.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def load ( self , path : str = None ) -> Self : \"\"\"Load all model weights. Args: path (str): where to look for the model's weights. Defaults to None. Raises: ValueError: if the path does not exist. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): raise ValueError ( \"Path does not exists.\" ) self . policy . load_state_dict ( torch . load ( f \" { path } best_model.ckpt\" , map_location = torch . device ( self . device ) ) ) self . idm . load_state_dict ( torch . load ( f \" { path } /idm.ckpt\" , map_location = torch . device ( self . device ) ) ) return self","title":"load()"},{"location":"algorithms/bco/#benchmark.methods.bco.BCO.save","text":"Save all models weights. Parameters: path ( str , default: None ) \u2013 where to save the models. Defaults to None. src/benchmark/methods/bco.py 76 77 78 79 80 81 82 83 84 85 86 87 def save ( self , path : str = None ) -> None : \"\"\"Save all models weights. Args: path (str): where to save the models. Defaults to None. \"\"\" path = self . save_path if path is None else path if not os . path . exists ( path ): os . makedirs ( path ) torch . save ( self . policy . state_dict (), f \" { path } /best_model.ckpt\" ) torch . save ( self . idm . state_dict (), f \" { path } /idm.ckpt\" )","title":"save()"},{"location":"algorithms/bco/#benchmark.methods.bco.BCO.train","text":"Train process. Parameters: n_epochs ( int ) \u2013 amount of epoch to run. train_dataset ( DataLoader ) \u2013 data to train. eval_dataset ( DataLoader , default: None ) \u2013 data to eval. Defaults to None. Returns: method ( Self ) \u2013 trained method. src/benchmark/methods/bco.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def train ( self , n_epochs : int , train_dataset : Dict [ str , DataLoader ], eval_dataset : Dict [ str , DataLoader ] = None , folder : str = None ) -> Self : \"\"\"Train process. Args: n_epochs (int): amount of epoch to run. train_dataset (DataLoader): data to train. eval_dataset (DataLoader): data to eval. Defaults to None. Returns: method (Self): trained method. \"\"\" if folder is None : folder = f \"../benchmark_results/bco/ { self . environment_name } \" if not os . path . exists ( folder ): os . makedirs ( f \" { folder } /\" ) board = Tensorboard ( path = folder ) self . policy . to ( self . device ) self . idm . to ( self . device ) best_model = - np . inf if not isinstance ( train_dataset , dict ): train_dataset = { \"expert_dataset\" : train_dataset } if \"idm_dataset\" not in train_dataset . keys (): print ( \"No random dataset found\" ) random_path = f \"./dataset/random_ { self . environment . spec . id } \" if not os . path . exists ( random_path ): print ( \"Creating random dataset from scratch\" ) train_dataset [ \"idm_dataset\" ] = get_random_dataset ( environment_name = self . environment . spec . id , episodes = self . hyperparameters [ \"random_episodes\" ] ) else : print ( \"Loading local random dataset\" ) train_dataset [ \"idm_dataset\" ] = BaselineDataset ( f \" { random_path } /teacher.npz\" ) train_dataset [ \"idm_dataset\" ] = DataLoader ( train_dataset [ \"idm_dataset\" ], batch_size = train_dataset [ \"expert_dataset\" ] . batch_size , shuffle = True ) pbar = range ( n_epochs ) if self . verbose : pbar = tqdm ( pbar ) for epoch in pbar : train_metrics = self . _train ( ** train_dataset ) board . add_scalars ( \"Train\" , epoch = \"train\" , ** train_metrics ) if eval_dataset is not None : eval_metrics = self . _eval ( eval_dataset ) board . add_scalars ( \"Eval\" , epoch = \"eval\" , ** eval_metrics ) board . step ([ \"train\" , \"eval\" ]) else : board . step ( \"train\" ) if epoch % self . enjoy_criteria == 0 : train_dataset = self . _append_samples ( train_dataset ) if epoch % self . enjoy_criteria == 0 or epoch + 1 == n_epochs : metrics = self . _enjoy () board . add_scalars ( \"Enjoy\" , epoch = \"enjoy\" , ** metrics ) board . step ( \"enjoy\" ) if best_model < metrics [ \"aer\" ]: self . save () return self","title":"train()"},{"location":"algorithms/method/","text":"Method Bases: ABC Base class for all methods. Source code in src/benchmark/methods/method.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 class Method ( ABC ): \"\"\"Base class for all methods.\"\"\" __version__ = \"1.0.0\" __author__ = \"Nathan Gavenski\" __method_name__ = \"Abstract Method\" def __init__ ( self , environment : Env , environment_parameters : Dict [ str , Any ], discrete_loss : nn . Module = nn . CrossEntropyLoss , continuous_loss : nn . Module = nn . MSELoss , optimizer_fn : optim . Optimizer = optim . Adam , ) -> None : \"\"\"Initialize base class.\"\"\" super () . __init__ () self . environment = environment self . discrete = isinstance ( environment . action_space , spaces . Discrete ) self . discrete |= isinstance ( environment . action_space , gym_spaces . Discrete ) self . device = \"cuda\" if torch . cuda . is_available () else \"cpu\" self . observation_size = environment . observation_space . shape [ 0 ] if self . discrete : self . action_size = environment . action_space . n self . loss_fn = discrete_loss () else : self . action_size = environment . action_space . shape [ 0 ] self . loss_fn = continuous_loss () policy = self . hyperparameters . get ( 'policy' , 'MlpPolicy' ) if policy == 'MlpPolicy' : self . policy = MLP ( self . observation_size , self . action_size ) elif policy == 'MlpWithAttention' : self . policy = MlpWithAttention ( self . observation_size , self . action_size ) self . optimizer_fn = optimizer_fn ( self . policy . parameters (), lr = environment_parameters [ 'lr' ] ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method for the method. Args: x (torch.Tensor): input. Returns: x (torch.Tensor): logits output. \"\"\" raise NotImplementedError () def predict ( self , obs : Union [ np . ndarray , torch . Tensor ]) -> Union [ List [ Number ], Number ]: \"\"\"Predict method. Args: obs (Union[np.ndarray, torch.Tensor]): input observation. Returns: action (Union[List[Number], Number): predicted action. \"\"\" self . policy . eval () if isinstance ( obs , np . ndarray ): obs = torch . from_numpy ( obs ) if len ( obs . shape ) == 1 : obs = obs [ None ] obs = obs . to ( self . device ) with torch . no_grad (): actions = self . forward ( obs ) actions = actions [ 0 ] if self . discrete : return torch . argmax ( actions ) . cpu () . numpy () return actions . cpu () . numpy () def save ( self , path : str = None ) -> None : \"\"\"Save all model weights. Args: path (str): where to save the models. Defaults to None. \"\"\" raise NotImplementedError () def load ( self , path : str = None ) -> Self : \"\"\"Load all model weights. Args: path (str): where to look for the model's weights. Defaults to None. \"\"\" raise NotImplementedError () # pylint: disable=W0221 def train ( self , n_epochs : int , train_dataset : DataLoader , eval_dataset : DataLoader = None ) -> Self : \"\"\"Train process. Args: n_epochs (int): amount of epoch to run. train_dataset (DataLoader): data to train. eval_dataset (DataLoader): data to eval. Defaults to None. Returns: method (Self): trained method. \"\"\" raise NotImplementedError () def _train ( self , dataset : DataLoader [ BaselineDataset ]) -> Metrics : \"\"\"Train loop. Args: dataset (DataLoader): train data. \"\"\" raise NotImplementedError () def _eval ( self , dataset : DataLoader [ BaselineDataset ]) -> Metrics : \"\"\"Evaluation loop. Args: dataset (DataLoader): data to eval. \"\"\" raise NotImplementedError () def _enjoy ( self , render : bool = False , teacher_reward : Number = None , random_reward : Number = None ) -> Metrics : \"\"\"Function for evaluation of the policy in the environment Args: render (bool): Whether it should render. Defaults to False. teacher_reward (Number): reward for teacher policy. random_reward (Number): reward for a random policy. Returns: Metrics: aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. \"\"\" environment = GymWrapper ( self . environment ) average_reward = [] for _ in range ( 100 ): done = False obs = environment . reset () accumulated_reward = 0 while not done : if render : environment . render () action = self . predict ( obs ) obs , reward , done , * _ = environment . step ( action ) accumulated_reward += reward average_reward . append ( accumulated_reward ) metrics = average_episodic_reward ( average_reward ) if teacher_reward is not None and random_reward is not None : metrics . update ( performance ( average_reward , teacher_reward , random_reward )) return metrics __init__ ( environment , environment_parameters , discrete_loss = nn . CrossEntropyLoss , continuous_loss = nn . MSELoss , optimizer_fn = optim . Adam ) Initialize base class. src/benchmark/methods/method.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , environment : Env , environment_parameters : Dict [ str , Any ], discrete_loss : nn . Module = nn . CrossEntropyLoss , continuous_loss : nn . Module = nn . MSELoss , optimizer_fn : optim . Optimizer = optim . Adam , ) -> None : \"\"\"Initialize base class.\"\"\" super () . __init__ () self . environment = environment self . discrete = isinstance ( environment . action_space , spaces . Discrete ) self . discrete |= isinstance ( environment . action_space , gym_spaces . Discrete ) self . device = \"cuda\" if torch . cuda . is_available () else \"cpu\" self . observation_size = environment . observation_space . shape [ 0 ] if self . discrete : self . action_size = environment . action_space . n self . loss_fn = discrete_loss () else : self . action_size = environment . action_space . shape [ 0 ] self . loss_fn = continuous_loss () policy = self . hyperparameters . get ( 'policy' , 'MlpPolicy' ) if policy == 'MlpPolicy' : self . policy = MLP ( self . observation_size , self . action_size ) elif policy == 'MlpWithAttention' : self . policy = MlpWithAttention ( self . observation_size , self . action_size ) self . optimizer_fn = optimizer_fn ( self . policy . parameters (), lr = environment_parameters [ 'lr' ] ) _enjoy ( render = False , teacher_reward = None , random_reward = None ) Function for evaluation of the policy in the environment Parameters: render ( bool , default: False ) \u2013 Whether it should render. Defaults to False. teacher_reward ( Number , default: None ) \u2013 reward for teacher policy. random_reward ( Number , default: None ) \u2013 reward for a random policy. Returns: Metrics ( Metrics ) \u2013 aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. src/benchmark/methods/method.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def _enjoy ( self , render : bool = False , teacher_reward : Number = None , random_reward : Number = None ) -> Metrics : \"\"\"Function for evaluation of the policy in the environment Args: render (bool): Whether it should render. Defaults to False. teacher_reward (Number): reward for teacher policy. random_reward (Number): reward for a random policy. Returns: Metrics: aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. \"\"\" environment = GymWrapper ( self . environment ) average_reward = [] for _ in range ( 100 ): done = False obs = environment . reset () accumulated_reward = 0 while not done : if render : environment . render () action = self . predict ( obs ) obs , reward , done , * _ = environment . step ( action ) accumulated_reward += reward average_reward . append ( accumulated_reward ) metrics = average_episodic_reward ( average_reward ) if teacher_reward is not None and random_reward is not None : metrics . update ( performance ( average_reward , teacher_reward , random_reward )) return metrics _eval ( dataset ) Evaluation loop. Parameters: dataset ( DataLoader ) \u2013 data to eval. src/benchmark/methods/method.py 151 152 153 154 155 156 157 def _eval ( self , dataset : DataLoader [ BaselineDataset ]) -> Metrics : \"\"\"Evaluation loop. Args: dataset (DataLoader): data to eval. \"\"\" raise NotImplementedError () _train ( dataset ) Train loop. Parameters: dataset ( DataLoader ) \u2013 train data. src/benchmark/methods/method.py 143 144 145 146 147 148 149 def _train ( self , dataset : DataLoader [ BaselineDataset ]) -> Metrics : \"\"\"Train loop. Args: dataset (DataLoader): train data. \"\"\" raise NotImplementedError () forward ( x ) Forward method for the method. Parameters: x ( Tensor ) \u2013 input. Returns: x ( Tensor ) \u2013 logits output. src/benchmark/methods/method.py 70 71 72 73 74 75 76 77 78 79 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method for the method. Args: x (torch.Tensor): input. Returns: x (torch.Tensor): logits output. \"\"\" raise NotImplementedError () load ( path = None ) Load all model weights. Parameters: path ( str , default: None ) \u2013 where to look for the model's weights. Defaults to None. src/benchmark/methods/method.py 116 117 118 119 120 121 122 def load ( self , path : str = None ) -> Self : \"\"\"Load all model weights. Args: path (str): where to look for the model's weights. Defaults to None. \"\"\" raise NotImplementedError () predict ( obs ) Predict method. Parameters: obs ( Union [ ndarray , Tensor ] ) \u2013 input observation. Returns: action ( Union[List[Number], Number ) \u2013 predicted action. src/benchmark/methods/method.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def predict ( self , obs : Union [ np . ndarray , torch . Tensor ]) -> Union [ List [ Number ], Number ]: \"\"\"Predict method. Args: obs (Union[np.ndarray, torch.Tensor]): input observation. Returns: action (Union[List[Number], Number): predicted action. \"\"\" self . policy . eval () if isinstance ( obs , np . ndarray ): obs = torch . from_numpy ( obs ) if len ( obs . shape ) == 1 : obs = obs [ None ] obs = obs . to ( self . device ) with torch . no_grad (): actions = self . forward ( obs ) actions = actions [ 0 ] if self . discrete : return torch . argmax ( actions ) . cpu () . numpy () return actions . cpu () . numpy () save ( path = None ) Save all model weights. Parameters: path ( str , default: None ) \u2013 where to save the models. Defaults to None. src/benchmark/methods/method.py 108 109 110 111 112 113 114 def save ( self , path : str = None ) -> None : \"\"\"Save all model weights. Args: path (str): where to save the models. Defaults to None. \"\"\" raise NotImplementedError () train ( n_epochs , train_dataset , eval_dataset = None ) Train process. Parameters: n_epochs ( int ) \u2013 amount of epoch to run. train_dataset ( DataLoader ) \u2013 data to train. eval_dataset ( DataLoader , default: None ) \u2013 data to eval. Defaults to None. Returns: method ( Self ) \u2013 trained method. src/benchmark/methods/method.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def train ( self , n_epochs : int , train_dataset : DataLoader , eval_dataset : DataLoader = None ) -> Self : \"\"\"Train process. Args: n_epochs (int): amount of epoch to run. train_dataset (DataLoader): data to train. eval_dataset (DataLoader): data to eval. Defaults to None. Returns: method (Self): trained method. \"\"\" raise NotImplementedError ()","title":"Default Method Class"},{"location":"algorithms/method/#method","text":"Bases: ABC Base class for all methods. Source code in src/benchmark/methods/method.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 class Method ( ABC ): \"\"\"Base class for all methods.\"\"\" __version__ = \"1.0.0\" __author__ = \"Nathan Gavenski\" __method_name__ = \"Abstract Method\" def __init__ ( self , environment : Env , environment_parameters : Dict [ str , Any ], discrete_loss : nn . Module = nn . CrossEntropyLoss , continuous_loss : nn . Module = nn . MSELoss , optimizer_fn : optim . Optimizer = optim . Adam , ) -> None : \"\"\"Initialize base class.\"\"\" super () . __init__ () self . environment = environment self . discrete = isinstance ( environment . action_space , spaces . Discrete ) self . discrete |= isinstance ( environment . action_space , gym_spaces . Discrete ) self . device = \"cuda\" if torch . cuda . is_available () else \"cpu\" self . observation_size = environment . observation_space . shape [ 0 ] if self . discrete : self . action_size = environment . action_space . n self . loss_fn = discrete_loss () else : self . action_size = environment . action_space . shape [ 0 ] self . loss_fn = continuous_loss () policy = self . hyperparameters . get ( 'policy' , 'MlpPolicy' ) if policy == 'MlpPolicy' : self . policy = MLP ( self . observation_size , self . action_size ) elif policy == 'MlpWithAttention' : self . policy = MlpWithAttention ( self . observation_size , self . action_size ) self . optimizer_fn = optimizer_fn ( self . policy . parameters (), lr = environment_parameters [ 'lr' ] ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method for the method. Args: x (torch.Tensor): input. Returns: x (torch.Tensor): logits output. \"\"\" raise NotImplementedError () def predict ( self , obs : Union [ np . ndarray , torch . Tensor ]) -> Union [ List [ Number ], Number ]: \"\"\"Predict method. Args: obs (Union[np.ndarray, torch.Tensor]): input observation. Returns: action (Union[List[Number], Number): predicted action. \"\"\" self . policy . eval () if isinstance ( obs , np . ndarray ): obs = torch . from_numpy ( obs ) if len ( obs . shape ) == 1 : obs = obs [ None ] obs = obs . to ( self . device ) with torch . no_grad (): actions = self . forward ( obs ) actions = actions [ 0 ] if self . discrete : return torch . argmax ( actions ) . cpu () . numpy () return actions . cpu () . numpy () def save ( self , path : str = None ) -> None : \"\"\"Save all model weights. Args: path (str): where to save the models. Defaults to None. \"\"\" raise NotImplementedError () def load ( self , path : str = None ) -> Self : \"\"\"Load all model weights. Args: path (str): where to look for the model's weights. Defaults to None. \"\"\" raise NotImplementedError () # pylint: disable=W0221 def train ( self , n_epochs : int , train_dataset : DataLoader , eval_dataset : DataLoader = None ) -> Self : \"\"\"Train process. Args: n_epochs (int): amount of epoch to run. train_dataset (DataLoader): data to train. eval_dataset (DataLoader): data to eval. Defaults to None. Returns: method (Self): trained method. \"\"\" raise NotImplementedError () def _train ( self , dataset : DataLoader [ BaselineDataset ]) -> Metrics : \"\"\"Train loop. Args: dataset (DataLoader): train data. \"\"\" raise NotImplementedError () def _eval ( self , dataset : DataLoader [ BaselineDataset ]) -> Metrics : \"\"\"Evaluation loop. Args: dataset (DataLoader): data to eval. \"\"\" raise NotImplementedError () def _enjoy ( self , render : bool = False , teacher_reward : Number = None , random_reward : Number = None ) -> Metrics : \"\"\"Function for evaluation of the policy in the environment Args: render (bool): Whether it should render. Defaults to False. teacher_reward (Number): reward for teacher policy. random_reward (Number): reward for a random policy. Returns: Metrics: aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. \"\"\" environment = GymWrapper ( self . environment ) average_reward = [] for _ in range ( 100 ): done = False obs = environment . reset () accumulated_reward = 0 while not done : if render : environment . render () action = self . predict ( obs ) obs , reward , done , * _ = environment . step ( action ) accumulated_reward += reward average_reward . append ( accumulated_reward ) metrics = average_episodic_reward ( average_reward ) if teacher_reward is not None and random_reward is not None : metrics . update ( performance ( average_reward , teacher_reward , random_reward )) return metrics","title":"Method"},{"location":"algorithms/method/#benchmark.methods.method.Method.__init__","text":"Initialize base class. src/benchmark/methods/method.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , environment : Env , environment_parameters : Dict [ str , Any ], discrete_loss : nn . Module = nn . CrossEntropyLoss , continuous_loss : nn . Module = nn . MSELoss , optimizer_fn : optim . Optimizer = optim . Adam , ) -> None : \"\"\"Initialize base class.\"\"\" super () . __init__ () self . environment = environment self . discrete = isinstance ( environment . action_space , spaces . Discrete ) self . discrete |= isinstance ( environment . action_space , gym_spaces . Discrete ) self . device = \"cuda\" if torch . cuda . is_available () else \"cpu\" self . observation_size = environment . observation_space . shape [ 0 ] if self . discrete : self . action_size = environment . action_space . n self . loss_fn = discrete_loss () else : self . action_size = environment . action_space . shape [ 0 ] self . loss_fn = continuous_loss () policy = self . hyperparameters . get ( 'policy' , 'MlpPolicy' ) if policy == 'MlpPolicy' : self . policy = MLP ( self . observation_size , self . action_size ) elif policy == 'MlpWithAttention' : self . policy = MlpWithAttention ( self . observation_size , self . action_size ) self . optimizer_fn = optimizer_fn ( self . policy . parameters (), lr = environment_parameters [ 'lr' ] )","title":"__init__()"},{"location":"algorithms/method/#benchmark.methods.method.Method._enjoy","text":"Function for evaluation of the policy in the environment Parameters: render ( bool , default: False ) \u2013 Whether it should render. Defaults to False. teacher_reward ( Number , default: None ) \u2013 reward for teacher policy. random_reward ( Number , default: None ) \u2013 reward for a random policy. Returns: Metrics ( Metrics ) \u2013 aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. src/benchmark/methods/method.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def _enjoy ( self , render : bool = False , teacher_reward : Number = None , random_reward : Number = None ) -> Metrics : \"\"\"Function for evaluation of the policy in the environment Args: render (bool): Whether it should render. Defaults to False. teacher_reward (Number): reward for teacher policy. random_reward (Number): reward for a random policy. Returns: Metrics: aer (Number): average reward for 100 episodes. aer_std (Number): standard deviation for aer. performance (Number): if teacher_reward and random_reward are informed than the performance metric is calculated. perforamance_std (Number): standard deviation for performance. \"\"\" environment = GymWrapper ( self . environment ) average_reward = [] for _ in range ( 100 ): done = False obs = environment . reset () accumulated_reward = 0 while not done : if render : environment . render () action = self . predict ( obs ) obs , reward , done , * _ = environment . step ( action ) accumulated_reward += reward average_reward . append ( accumulated_reward ) metrics = average_episodic_reward ( average_reward ) if teacher_reward is not None and random_reward is not None : metrics . update ( performance ( average_reward , teacher_reward , random_reward )) return metrics","title":"_enjoy()"},{"location":"algorithms/method/#benchmark.methods.method.Method._eval","text":"Evaluation loop. Parameters: dataset ( DataLoader ) \u2013 data to eval. src/benchmark/methods/method.py 151 152 153 154 155 156 157 def _eval ( self , dataset : DataLoader [ BaselineDataset ]) -> Metrics : \"\"\"Evaluation loop. Args: dataset (DataLoader): data to eval. \"\"\" raise NotImplementedError ()","title":"_eval()"},{"location":"algorithms/method/#benchmark.methods.method.Method._train","text":"Train loop. Parameters: dataset ( DataLoader ) \u2013 train data. src/benchmark/methods/method.py 143 144 145 146 147 148 149 def _train ( self , dataset : DataLoader [ BaselineDataset ]) -> Metrics : \"\"\"Train loop. Args: dataset (DataLoader): train data. \"\"\" raise NotImplementedError ()","title":"_train()"},{"location":"algorithms/method/#benchmark.methods.method.Method.forward","text":"Forward method for the method. Parameters: x ( Tensor ) \u2013 input. Returns: x ( Tensor ) \u2013 logits output. src/benchmark/methods/method.py 70 71 72 73 74 75 76 77 78 79 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method for the method. Args: x (torch.Tensor): input. Returns: x (torch.Tensor): logits output. \"\"\" raise NotImplementedError ()","title":"forward()"},{"location":"algorithms/method/#benchmark.methods.method.Method.load","text":"Load all model weights. Parameters: path ( str , default: None ) \u2013 where to look for the model's weights. Defaults to None. src/benchmark/methods/method.py 116 117 118 119 120 121 122 def load ( self , path : str = None ) -> Self : \"\"\"Load all model weights. Args: path (str): where to look for the model's weights. Defaults to None. \"\"\" raise NotImplementedError ()","title":"load()"},{"location":"algorithms/method/#benchmark.methods.method.Method.predict","text":"Predict method. Parameters: obs ( Union [ ndarray , Tensor ] ) \u2013 input observation. Returns: action ( Union[List[Number], Number ) \u2013 predicted action. src/benchmark/methods/method.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def predict ( self , obs : Union [ np . ndarray , torch . Tensor ]) -> Union [ List [ Number ], Number ]: \"\"\"Predict method. Args: obs (Union[np.ndarray, torch.Tensor]): input observation. Returns: action (Union[List[Number], Number): predicted action. \"\"\" self . policy . eval () if isinstance ( obs , np . ndarray ): obs = torch . from_numpy ( obs ) if len ( obs . shape ) == 1 : obs = obs [ None ] obs = obs . to ( self . device ) with torch . no_grad (): actions = self . forward ( obs ) actions = actions [ 0 ] if self . discrete : return torch . argmax ( actions ) . cpu () . numpy () return actions . cpu () . numpy ()","title":"predict()"},{"location":"algorithms/method/#benchmark.methods.method.Method.save","text":"Save all model weights. Parameters: path ( str , default: None ) \u2013 where to save the models. Defaults to None. src/benchmark/methods/method.py 108 109 110 111 112 113 114 def save ( self , path : str = None ) -> None : \"\"\"Save all model weights. Args: path (str): where to save the models. Defaults to None. \"\"\" raise NotImplementedError ()","title":"save()"},{"location":"algorithms/method/#benchmark.methods.method.Method.train","text":"Train process. Parameters: n_epochs ( int ) \u2013 amount of epoch to run. train_dataset ( DataLoader ) \u2013 data to train. eval_dataset ( DataLoader , default: None ) \u2013 data to eval. Defaults to None. Returns: method ( Self ) \u2013 trained method. src/benchmark/methods/method.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def train ( self , n_epochs : int , train_dataset : DataLoader , eval_dataset : DataLoader = None ) -> Self : \"\"\"Train process. Args: n_epochs (int): amount of epoch to run. train_dataset (DataLoader): data to train. eval_dataset (DataLoader): data to eval. Defaults to None. Returns: method (Self): trained method. \"\"\" raise NotImplementedError ()","title":"train()"},{"location":"api/benchmark/","text":"Benchmark benchmark create_dataloader Creates dataloader based on the BaselineDataset. Parameters: path(str) \u2013 HuggingFace path to dataset. Returns: dataloader ( DataLoader ) \u2013 dataloader to use for training. src/benchmark/benchmark.py 19 20 21 22 23 24 25 26 27 28 29 30 def create_dataloader ( path : str ) -> DataLoader : \"\"\"Creates dataloader based on the BaselineDataset. Args: path(str): HuggingFace path to dataset. Returns: dataloader (DataLoader): dataloader to use for training. \"\"\" dataset = BaselineDataset ( path , source = \"huggingface\" ) dataloader = DataLoader ( dataset , batch_size = 2048 , shuffle = True ) return dataloader benchmark_method Function for training a method and evaluating. Parameters: method ( Method ) \u2013 class for a method. environment ( Env ) \u2013 environment to train the method. dataloader ( DataLoader ) \u2013 dataloader to train the method. teacher_reward ( Number ) \u2013 teacher reward to compute performance. random_reward ( Number ) \u2013 random reward to compute performance. Returns: metrics ( Metrics ) \u2013 resulting metrics for best checkpoint. aer (Dict[str, str]): average episodic reward. performance (Dict[str, str]) performance. src/benchmark/benchmark.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def benchmark_method ( method : Method , environment : gym . Env , dataloader : DataLoader , teacher_reward : Number , random_reward : Number ) -> Metrics : \"\"\"Function for training a method and evaluating. Args: method (Method): class for a method. environment (Env): environment to train the method. dataloader (DataLoader): dataloader to train the method. teacher_reward (Number): teacher reward to compute performance. random_reward (Number): random reward to compute performance. Returns: metrics (Metrics): resulting metrics for best checkpoint. aer (Dict[str, str]): average episodic reward. performance (Dict[str, str]) performance. \"\"\" policy : Method = method ( environment , verbose = True , enjoy_criteria = 100 ) metrics = policy . train ( 5000 , train_dataset = dataloader ) \\ . load () \\ . _enjoy ( teacher_reward = teacher_reward , random_reward = random_reward ) aer = f \" { round ( metrics [ 'aer' ], 4 ) } \u00b1 { round ( metrics [ 'aer_std' ], 4 ) } \" performance = f \" { round ( metrics [ 'performance' ], 4 ) } \u00b1 { round ( metrics [ 'performance_std' ], 4 ) } \" return { \"aer\" : aer , \"performance\" : performance } benchmark Benchmark for all methods and environments listed on registers.py Parameters: benchmark_methods ( List [ Method ] ) \u2013 list of benchmark methods to run. src/benchmark/benchmark.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def benchmark ( benchmark_methods : List [ Method ]) -> None : \"\"\"Benchmark for all methods and environments listed on registers.py Args: benchmark_methods (List[Method]): list of benchmark methods to run. \"\"\" file_name = \"benchmark_results_\" file_name += \"-\" . join ([ str ( method . __name__ ) for method in benchmark_methods ]) benchmark_results = [] for environments in tqdm ( benchmark_environments , desc = \"Benchmark Environments\" ): for name , info in environments . items (): path , random_reward = info . values () try : environment = gym . make ( name ) except VersionNotFound : logging . error ( \"benchmark: Version for environment does not exist\" ) except NameNotFound : logging . error ( \"benchmark: Environment name does not exist\" ) except Exception : logging . error ( \"benchmark: Generic error raised, probably dependency related\" ) try : dataloader = create_dataloader ( path ) except FileNotFoundError : logging . error ( \"benchmark: HuggingFace path is not valid\" ) continue for method in tqdm ( benchmark_methods , desc = f \"Methods for environment: { name } \" ): try : metrics = benchmark_method ( method , environment , dataloader , dataloader . dataset . average_reward , random_reward ) except Exception as exception : logging . error ( \"benchmark: Method %s did raise an exception during training\" , method . __method_name__ ) logging . error ( exception ) continue benchmark_results . append ([ name , method . __method_name__ , * metrics . values () ]) table = tabulate ( benchmark_results , headers = [ \"Environment\" , \"Method\" , \"AER\" , \"Performance\" ], tablefmt = \"github\" ) with open ( f \"./ { file_name } .md\" , \"w\" , encoding = \"utf-8\" ) as _file : _file . write ( table ) registers get_methods Get methods from string list. Parameters: names ( List [ str ] ) \u2013 list of method names. Returns: benchmark_methods ( List [ Method ] ) \u2013 list of methods. src/benchmark/registers.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def get_methods ( names : List [ str ]) -> List [ Method ]: \"\"\"Get methods from string list. Args: names (List[str]): list of method names. Returns: benchmark_methods (List[Method]): list of methods. \"\"\" if len ( names ) == 1 and names [ 0 ] == \"all\" : return benchmark_methods partial_benchmark_methods = [] for name in names : partial_benchmark_methods . append ( eval ( name . upper ())) return partial_benchmark_methods","title":"Benchmark"},{"location":"api/benchmark/#benchmark","text":"","title":"Benchmark"},{"location":"api/benchmark/#benchmark_1","text":"","title":"benchmark"},{"location":"api/benchmark/#create_dataloader","text":"Creates dataloader based on the BaselineDataset. Parameters: path(str) \u2013 HuggingFace path to dataset. Returns: dataloader ( DataLoader ) \u2013 dataloader to use for training. src/benchmark/benchmark.py 19 20 21 22 23 24 25 26 27 28 29 30 def create_dataloader ( path : str ) -> DataLoader : \"\"\"Creates dataloader based on the BaselineDataset. Args: path(str): HuggingFace path to dataset. Returns: dataloader (DataLoader): dataloader to use for training. \"\"\" dataset = BaselineDataset ( path , source = \"huggingface\" ) dataloader = DataLoader ( dataset , batch_size = 2048 , shuffle = True ) return dataloader","title":"create_dataloader"},{"location":"api/benchmark/#benchmark_method","text":"Function for training a method and evaluating. Parameters: method ( Method ) \u2013 class for a method. environment ( Env ) \u2013 environment to train the method. dataloader ( DataLoader ) \u2013 dataloader to train the method. teacher_reward ( Number ) \u2013 teacher reward to compute performance. random_reward ( Number ) \u2013 random reward to compute performance. Returns: metrics ( Metrics ) \u2013 resulting metrics for best checkpoint. aer (Dict[str, str]): average episodic reward. performance (Dict[str, str]) performance. src/benchmark/benchmark.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def benchmark_method ( method : Method , environment : gym . Env , dataloader : DataLoader , teacher_reward : Number , random_reward : Number ) -> Metrics : \"\"\"Function for training a method and evaluating. Args: method (Method): class for a method. environment (Env): environment to train the method. dataloader (DataLoader): dataloader to train the method. teacher_reward (Number): teacher reward to compute performance. random_reward (Number): random reward to compute performance. Returns: metrics (Metrics): resulting metrics for best checkpoint. aer (Dict[str, str]): average episodic reward. performance (Dict[str, str]) performance. \"\"\" policy : Method = method ( environment , verbose = True , enjoy_criteria = 100 ) metrics = policy . train ( 5000 , train_dataset = dataloader ) \\ . load () \\ . _enjoy ( teacher_reward = teacher_reward , random_reward = random_reward ) aer = f \" { round ( metrics [ 'aer' ], 4 ) } \u00b1 { round ( metrics [ 'aer_std' ], 4 ) } \" performance = f \" { round ( metrics [ 'performance' ], 4 ) } \u00b1 { round ( metrics [ 'performance_std' ], 4 ) } \" return { \"aer\" : aer , \"performance\" : performance }","title":"benchmark_method"},{"location":"api/benchmark/#benchmark_2","text":"Benchmark for all methods and environments listed on registers.py Parameters: benchmark_methods ( List [ Method ] ) \u2013 list of benchmark methods to run. src/benchmark/benchmark.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def benchmark ( benchmark_methods : List [ Method ]) -> None : \"\"\"Benchmark for all methods and environments listed on registers.py Args: benchmark_methods (List[Method]): list of benchmark methods to run. \"\"\" file_name = \"benchmark_results_\" file_name += \"-\" . join ([ str ( method . __name__ ) for method in benchmark_methods ]) benchmark_results = [] for environments in tqdm ( benchmark_environments , desc = \"Benchmark Environments\" ): for name , info in environments . items (): path , random_reward = info . values () try : environment = gym . make ( name ) except VersionNotFound : logging . error ( \"benchmark: Version for environment does not exist\" ) except NameNotFound : logging . error ( \"benchmark: Environment name does not exist\" ) except Exception : logging . error ( \"benchmark: Generic error raised, probably dependency related\" ) try : dataloader = create_dataloader ( path ) except FileNotFoundError : logging . error ( \"benchmark: HuggingFace path is not valid\" ) continue for method in tqdm ( benchmark_methods , desc = f \"Methods for environment: { name } \" ): try : metrics = benchmark_method ( method , environment , dataloader , dataloader . dataset . average_reward , random_reward ) except Exception as exception : logging . error ( \"benchmark: Method %s did raise an exception during training\" , method . __method_name__ ) logging . error ( exception ) continue benchmark_results . append ([ name , method . __method_name__ , * metrics . values () ]) table = tabulate ( benchmark_results , headers = [ \"Environment\" , \"Method\" , \"AER\" , \"Performance\" ], tablefmt = \"github\" ) with open ( f \"./ { file_name } .md\" , \"w\" , encoding = \"utf-8\" ) as _file : _file . write ( table )","title":"benchmark"},{"location":"api/benchmark/#registers","text":"","title":"registers"},{"location":"api/benchmark/#get_methods","text":"Get methods from string list. Parameters: names ( List [ str ] ) \u2013 list of method names. Returns: benchmark_methods ( List [ Method ] ) \u2013 list of methods. src/benchmark/registers.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def get_methods ( names : List [ str ]) -> List [ Method ]: \"\"\"Get methods from string list. Args: names (List[str]): list of method names. Returns: benchmark_methods (List[Method]): list of methods. \"\"\" if len ( names ) == 1 and names [ 0 ] == \"all\" : return benchmark_methods partial_benchmark_methods = [] for name in names : partial_benchmark_methods . append ( eval ( name . upper ())) return partial_benchmark_methods","title":"get_methods"},{"location":"api/imitation/","text":"imitation_datasets controller Controller for running experiments. Source code in src/imitation_datasets/controller.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 class Controller : \"\"\"Controller for running experiments.\"\"\" def __init__ ( self , enjoy : EnjoyFunction , collate : CollateFunction , amount : int , threads : int = 1 , path : str = './dataset/' ) -> None : \"\"\"Initialize the controller. Args: enjoy (EnjoyFunction): Function to run the expert. collate (CollateFunction): Function to collate the data. amount (int): Amount of episodes to run. threads (int, optional): Amount of threads to use. Defaults to 1. path (str, optional): Path to save the dataset. Defaults to './dataset/'. \"\"\" self . enjoy = enjoy self . collate = collate self . threads = CPUS ( threads ) self . experiments = Experiment ( amount ) self . path = path self . pbar = None set_start_method ( 'spawn' , force = True ) def create_folder ( self , path : str ) -> None : \"\"\"Create a folder if it does not exist. Args: path (str): Path to the folder. \"\"\" if not os . path . exists ( path ): os . makedirs ( path ) async def set_cpu ( self , cpu : int ) -> None : \"\"\"Set the cpu affinity for the current process. Args: cpu (int): CPU index to use. \"\"\" try : proc = psutil . Process () proc . cpu_affinity ([ int ( cpu )]) if 'linux' in platform : os . sched_setaffinity ( proc . pid , [ int ( cpu )]) except OSError : pass def enjoy_closure ( self , opt : Namespace ) -> EnjoyFunction : \"\"\"Create a closure for the enjoy function. Args: opt (Namespace): Namespace with the arguments. Returns: EnjoyFunction: Enjoy function with part of the arguments. \"\"\" os . system ( \"set LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so\" ) os . system ( \"set LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia\" ) return partial ( self . enjoy , expert = Experts . get_expert ( opt . game )) def collate_closure ( self , opt : Namespace ) -> CollateFunction : \"\"\"Create a closure for the collate function. Args: opt (Namespace): Namespace with the arguments. Returns: CollateFunction: Collate function with part of the arguments. \"\"\" path = f ' { self . path }{ opt . game } /' files = list ( listdir ( path )) return partial ( self . collate , data = files , path = path ) async def enjoy_sequence ( self , future : EnjoyFunction , executor : ProcessPoolExecutor ) -> bool : \"\"\"_summary_ Args: future (EnjoyFunction): Enjoy function already with async future. executor (ProcessPoolExecutor): Executor to run the future. Returns: bool: Result of the future. True if the expert was able to solve the game. False otherwise. \"\"\" # Pre cpu = await self . threads . cpu_allock () await self . experiments . start () await self . set_cpu ( cpu ) # Enjoy result = await asyncio . get_event_loop () . run_in_executor ( executor , future ) # Post self . threads . cpu_release ( cpu ) await self . experiments . stop ( result ) self . pbar . update ( 1 if result else 0 ) return result if result else await asyncio . gather ( self . enjoy_sequence ( future , executor )) async def run ( self , opt ) -> None : \"\"\"Run the experiments. Args: opt (Namespace): Namespace with the arguments. \"\"\" path = f ' { self . path }{ opt . game } /' self . create_folder ( path ) tasks = [] with ProcessPoolExecutor () as executor : for idx in range ( self . experiments . amount ): enjoy = self . enjoy_closure ( opt ) enjoy = partial ( enjoy , path = path , context = Context ( self . experiments , idx )) task = asyncio . ensure_future ( self . enjoy_sequence ( enjoy , executor ) ) tasks . append ( task ) await asyncio . gather ( * tasks ) def start ( self , opt : Namespace ): \"\"\"Start the experiments. Args: opt (Namespace): Namespace with the arguments. Raises: exception: Exception (general) raised during the execution. \"\"\" try : if opt . mode in [ 'all' , 'play' ]: self . pbar = tqdm ( range ( self . experiments . amount ), desc = 'Running episodes' ) asyncio . run ( self . run ( opt )) if opt . mode in [ 'all' , 'collate' ]: self . pbar = tqdm ( range ( self . experiments . amount ), desc = 'Running collate' ) collate = self . collate_closure ( opt ) collate () except Exception as exception : self . experiments . add_log ( - 99 , exception ) raise exception finally : self . experiments . write_log () __init__ ( enjoy , collate , amount , threads = 1 , path = './dataset/' ) Initialize the controller. Parameters: enjoy ( EnjoyFunction ) \u2013 Function to run the expert. collate ( CollateFunction ) \u2013 Function to collate the data. amount ( int ) \u2013 Amount of episodes to run. threads ( int , default: 1 ) \u2013 Amount of threads to use. Defaults to 1. path ( str , default: './dataset/' ) \u2013 Path to save the dataset. Defaults to './dataset/'. src/imitation_datasets/controller.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , enjoy : EnjoyFunction , collate : CollateFunction , amount : int , threads : int = 1 , path : str = './dataset/' ) -> None : \"\"\"Initialize the controller. Args: enjoy (EnjoyFunction): Function to run the expert. collate (CollateFunction): Function to collate the data. amount (int): Amount of episodes to run. threads (int, optional): Amount of threads to use. Defaults to 1. path (str, optional): Path to save the dataset. Defaults to './dataset/'. \"\"\" self . enjoy = enjoy self . collate = collate self . threads = CPUS ( threads ) self . experiments = Experiment ( amount ) self . path = path self . pbar = None set_start_method ( 'spawn' , force = True ) collate_closure ( opt ) Create a closure for the collate function. Parameters: opt ( Namespace ) \u2013 Namespace with the arguments. Returns: CollateFunction ( CollateFunction ) \u2013 Collate function with part of the arguments. src/imitation_datasets/controller.py 83 84 85 86 87 88 89 90 91 92 93 94 def collate_closure ( self , opt : Namespace ) -> CollateFunction : \"\"\"Create a closure for the collate function. Args: opt (Namespace): Namespace with the arguments. Returns: CollateFunction: Collate function with part of the arguments. \"\"\" path = f ' { self . path }{ opt . game } /' files = list ( listdir ( path )) return partial ( self . collate , data = files , path = path ) create_folder ( path ) Create a folder if it does not exist. Parameters: path ( str ) \u2013 Path to the folder. src/imitation_datasets/controller.py 47 48 49 50 51 52 53 54 def create_folder ( self , path : str ) -> None : \"\"\"Create a folder if it does not exist. Args: path (str): Path to the folder. \"\"\" if not os . path . exists ( path ): os . makedirs ( path ) enjoy_closure ( opt ) Create a closure for the enjoy function. Parameters: opt ( Namespace ) \u2013 Namespace with the arguments. Returns: EnjoyFunction ( EnjoyFunction ) \u2013 Enjoy function with part of the arguments. src/imitation_datasets/controller.py 70 71 72 73 74 75 76 77 78 79 80 81 def enjoy_closure ( self , opt : Namespace ) -> EnjoyFunction : \"\"\"Create a closure for the enjoy function. Args: opt (Namespace): Namespace with the arguments. Returns: EnjoyFunction: Enjoy function with part of the arguments. \"\"\" os . system ( \"set LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so\" ) os . system ( \"set LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia\" ) return partial ( self . enjoy , expert = Experts . get_expert ( opt . game )) enjoy_sequence ( future , executor ) async summary Parameters: future ( EnjoyFunction ) \u2013 Enjoy function already with async future. executor ( ProcessPoolExecutor ) \u2013 Executor to run the future. Returns: bool ( bool ) \u2013 Result of the future. True if the expert was able to solve the game. False otherwise. src/imitation_datasets/controller.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 async def enjoy_sequence ( self , future : EnjoyFunction , executor : ProcessPoolExecutor ) -> bool : \"\"\"_summary_ Args: future (EnjoyFunction): Enjoy function already with async future. executor (ProcessPoolExecutor): Executor to run the future. Returns: bool: Result of the future. True if the expert was able to solve the game. False otherwise. \"\"\" # Pre cpu = await self . threads . cpu_allock () await self . experiments . start () await self . set_cpu ( cpu ) # Enjoy result = await asyncio . get_event_loop () . run_in_executor ( executor , future ) # Post self . threads . cpu_release ( cpu ) await self . experiments . stop ( result ) self . pbar . update ( 1 if result else 0 ) return result if result else await asyncio . gather ( self . enjoy_sequence ( future , executor )) run ( opt ) async Run the experiments. Parameters: opt ( Namespace ) \u2013 Namespace with the arguments. src/imitation_datasets/controller.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 async def run ( self , opt ) -> None : \"\"\"Run the experiments. Args: opt (Namespace): Namespace with the arguments. \"\"\" path = f ' { self . path }{ opt . game } /' self . create_folder ( path ) tasks = [] with ProcessPoolExecutor () as executor : for idx in range ( self . experiments . amount ): enjoy = self . enjoy_closure ( opt ) enjoy = partial ( enjoy , path = path , context = Context ( self . experiments , idx )) task = asyncio . ensure_future ( self . enjoy_sequence ( enjoy , executor ) ) tasks . append ( task ) await asyncio . gather ( * tasks ) set_cpu ( cpu ) async Set the cpu affinity for the current process. Parameters: cpu ( int ) \u2013 CPU index to use. src/imitation_datasets/controller.py 56 57 58 59 60 61 62 63 64 65 66 67 68 async def set_cpu ( self , cpu : int ) -> None : \"\"\"Set the cpu affinity for the current process. Args: cpu (int): CPU index to use. \"\"\" try : proc = psutil . Process () proc . cpu_affinity ([ int ( cpu )]) if 'linux' in platform : os . sched_setaffinity ( proc . pid , [ int ( cpu )]) except OSError : pass start ( opt ) Start the experiments. Parameters: opt ( Namespace ) \u2013 Namespace with the arguments. Raises: exception \u2013 Exception (general) raised during the execution. src/imitation_datasets/controller.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def start ( self , opt : Namespace ): \"\"\"Start the experiments. Args: opt (Namespace): Namespace with the arguments. Raises: exception: Exception (general) raised during the execution. \"\"\" try : if opt . mode in [ 'all' , 'play' ]: self . pbar = tqdm ( range ( self . experiments . amount ), desc = 'Running episodes' ) asyncio . run ( self . run ( opt )) if opt . mode in [ 'all' , 'collate' ]: self . pbar = tqdm ( range ( self . experiments . amount ), desc = 'Running collate' ) collate = self . collate_closure ( opt ) collate () except Exception as exception : self . experiments . add_log ( - 99 , exception ) raise exception finally : self . experiments . write_log () experts Policy Policy dataclass to load and use expert policies. Source code in src/imitation_datasets/experts.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 @dataclass class Policy : \"\"\"Policy dataclass to load and use expert policies.\"\"\" name : str repo_id : str filename : str threshold : float algo : BaseAlgorithm policy : BaseAlgorithm = field ( init = False , default = None ) internal_state : Any = field ( init = False , default = None ) environment : Any = field ( init = False , default = None ) def load ( self ) -> BaseAlgorithm : \"\"\" Load policy from HuggingFace hub. It uses a custom_object to replicate stable_baselines behaviour. custom_objects = { \"learning_rate\": 0.0, \"lr_schedule\": lambda _: 0.0, \"clip_range\": lambda _: 0.0 } Returns: BaseAlgorithm: Stable baseline policy loaded from HuggingFace hub. \"\"\" checkpoint = load_from_hub ( repo_id = self . repo_id , filename = self . filename , ) custom_objects = { \"learning_rate\" : 0.0 , \"lr_schedule\" : lambda _ : 0.0 , \"clip_range\" : lambda _ : 0.0 } self . policy = self . algo . load ( checkpoint , custom_objects = custom_objects ) return self . policy def predict ( self , obs : List [ Union [ int , float ]], deterministic : bool = True ) -> Tuple [ Union [ int , float , List [ Union [ int , float ]]], Union [ int , float , List [ Union [ int , float ]]] ]: \"\"\" Predict action given observation. Args: obs (List[int | float]): observation from environment. deterministic (bool, optional): Use exploration to predict action. Defaults to True. Returns: action (Union[int, float, List[Union[int, float]]]): action predicted by the policy. internal_states (Union[int, float, List[Union[int, float]]]): internal states of the policy. Note: typing depends on the environment. \"\"\" action , internal_states = self . policy . predict ( obs , state = self . internal_state , deterministic = deterministic , ) self . internal_state = internal_states return action , internal_states def get_environment ( self ) -> str : \"\"\"Return environment name. Returns: str: environment name. \"\"\" if self . environment is None : self . environment = gym . make ( self . name , render_mode = \"rgb_array\" ) return self . environment get_environment () Return environment name. Returns: str ( str ) \u2013 environment name. src/imitation_datasets/experts.py 87 88 89 90 91 92 93 94 95 def get_environment ( self ) -> str : \"\"\"Return environment name. Returns: str: environment name. \"\"\" if self . environment is None : self . environment = gym . make ( self . name , render_mode = \"rgb_array\" ) return self . environment load () Load policy from HuggingFace hub. It uses a custom_object to replicate stable_baselines behaviour. custom_objects = { \"learning_rate\": 0.0, \"lr_schedule\": lambda : 0.0, \"clip_range\": lambda : 0.0 } Returns: BaseAlgorithm ( BaseAlgorithm ) \u2013 Stable baseline policy loaded from HuggingFace hub. src/imitation_datasets/experts.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def load ( self ) -> BaseAlgorithm : \"\"\" Load policy from HuggingFace hub. It uses a custom_object to replicate stable_baselines behaviour. custom_objects = { \"learning_rate\": 0.0, \"lr_schedule\": lambda _: 0.0, \"clip_range\": lambda _: 0.0 } Returns: BaseAlgorithm: Stable baseline policy loaded from HuggingFace hub. \"\"\" checkpoint = load_from_hub ( repo_id = self . repo_id , filename = self . filename , ) custom_objects = { \"learning_rate\" : 0.0 , \"lr_schedule\" : lambda _ : 0.0 , \"clip_range\" : lambda _ : 0.0 } self . policy = self . algo . load ( checkpoint , custom_objects = custom_objects ) return self . policy predict ( obs , deterministic = True ) Predict action given observation. Parameters: obs ( List [ int | float ] ) \u2013 observation from environment. deterministic ( bool , default: True ) \u2013 Use exploration to predict action. Defaults to True. Returns: action ( Union [ int , float , List [ Union [ int , float ]]] ) \u2013 action predicted by the policy. internal_states ( Union [ int , float , List [ Union [ int , float ]]] ) \u2013 internal states of the policy. Note: typing depends on the environment. src/imitation_datasets/experts.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def predict ( self , obs : List [ Union [ int , float ]], deterministic : bool = True ) -> Tuple [ Union [ int , float , List [ Union [ int , float ]]], Union [ int , float , List [ Union [ int , float ]]] ]: \"\"\" Predict action given observation. Args: obs (List[int | float]): observation from environment. deterministic (bool, optional): Use exploration to predict action. Defaults to True. Returns: action (Union[int, float, List[Union[int, float]]]): action predicted by the policy. internal_states (Union[int, float, List[Union[int, float]]]): internal states of the policy. Note: typing depends on the environment. \"\"\" action , internal_states = self . policy . predict ( obs , state = self . internal_state , deterministic = deterministic , ) self . internal_state = internal_states return action , internal_states Experts Helper class to register and get expert policies. Source code in src/imitation_datasets/experts.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class Experts : \"\"\"Helper class to register and get expert policies.\"\"\" experts : Dict [ str , Policy ] = { key : Policy ( ** value ) for env in [ atari , classic , mujoco ] for key , value in env . items () } @classmethod def register ( cls , identifier : str , policy : Policy ) -> None : \"\"\"Register a new policy.\"\"\" if not isinstance ( policy . threshold , float ): policy . threshold = float ( policy . threshold ) cls . experts [ identifier ] = policy @classmethod def get_expert ( cls , identifier : str ) -> Policy : \"\"\"Return expert policy. Args: identifier (str): identifier of the policy. Returns: Policy: dataclass with expert policy information. \"\"\" try : return cls . experts [ identifier ] except KeyError : return None @classmethod def get_register ( cls ) -> None : \"\"\"Print entire register of expert policies.\"\"\" print ( cls . experts ) get_expert ( identifier ) classmethod Return expert policy. Parameters: identifier ( str ) \u2013 identifier of the policy. Returns: Policy ( Policy ) \u2013 dataclass with expert policy information. src/imitation_datasets/experts.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @classmethod def get_expert ( cls , identifier : str ) -> Policy : \"\"\"Return expert policy. Args: identifier (str): identifier of the policy. Returns: Policy: dataclass with expert policy information. \"\"\" try : return cls . experts [ identifier ] except KeyError : return None get_register () classmethod Print entire register of expert policies. src/imitation_datasets/experts.py 127 128 129 130 @classmethod def get_register ( cls ) -> None : \"\"\"Print entire register of expert policies.\"\"\" print ( cls . experts ) register ( identifier , policy ) classmethod Register a new policy. src/imitation_datasets/experts.py 104 105 106 107 108 109 110 @classmethod def register ( cls , identifier : str , policy : Policy ) -> None : \"\"\"Register a new policy.\"\"\" if not isinstance ( policy . threshold , float ): policy . threshold = float ( policy . threshold ) cls . experts [ identifier ] = policy functions enjoy This is a simple enjoy function example. It has three arguments and should return a boolean. src/imitation_datasets/functions.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def enjoy ( expert : Policy , path : str , context : Context ) -> bool : \"\"\" This is a simple enjoy function example. It has three arguments and should return a boolean. \"\"\" done = False expert . load () env = GymWrapper ( expert . get_environment (), version = \"newest\" ) states , actions = [], [] acc_reward , state = 0 , env . reset () while not done : action , _ = expert . predict ( state ) state , reward , done , _ = env . step ( action ) acc_reward += reward states . append ( state ) actions . append ( action ) env . close () episode = { 'states' : np . array ( states ), 'actions' : np . array ( actions ) } if acc_reward >= expert . threshold : np . savez ( f ' { path }{ context . index } ' , ** episode ) context . add_log ( f 'Accumulated reward { acc_reward } ' ) return acc_reward >= expert . threshold baseline_enjoy Enjoy following StableBaseline output. src/imitation_datasets/functions.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def baseline_enjoy ( expert : Policy , path : str , context : Context ) -> bool : \"\"\"Enjoy following StableBaseline output.\"\"\" done = False expert . load () env = GymWrapper ( expert . get_environment (), version = \"newest\" ) states = [] actions = [] rewards = [] state = env . reset () acc_reward = 0 while not done : action , _ = expert . predict ( state ) states . append ( state ) actions . append ( action ) state , reward , done , _ = env . step ( action ) acc_reward += reward rewards . append ( reward ) env . close () episode_returns = np . array ([ acc_reward ]) episode = { 'obs' : np . array ( states ), 'actions' : np . array ( actions ), 'rewards' : np . array ( rewards ), 'episode_returns' : episode_returns } if acc_reward >= expert . threshold : np . savez ( f ' { path }{ context . index } ' , ** episode ) context . add_log ( f 'Accumulated reward { acc_reward } ' ) return acc_reward >= expert . threshold collate This function is a simple collate function. src/imitation_datasets/functions.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def collate ( path , data ) -> bool : \"\"\"This function is a simple collate function.\"\"\" episodes_starts = [] states , actions = [], [] for file in data : episode = np . load ( f ' { path }{ file } ' ) states . append ( episode [ 'states' ]) actions . append ( episode [ 'actions' ]) episode_starts = np . zeros ( episode [ 'actions' ] . shape ) episode_starts [ 0 ] = 1 episodes_starts . append ( episode_starts ) states = np . array ( states ) states = states . reshape (( - 1 , states . shape [ - 1 ])) actions = np . array ( actions ) . reshape ( - 1 ) episodes_starts = np . array ( episodes_starts ) . reshape ( - 1 ) episode = { 'states' : states , 'actions' : actions , 'episode_starts' : episodes_starts } np . savez ( f ' { path } teacher' , ** episode ) for file in data : os . remove ( f ' { path }{ file } ' ) return True baseline_collate Collate that outputs the same as StableBaseline. src/imitation_datasets/functions.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def baseline_collate ( path : str , data : List [ str ]) -> bool : \"\"\"Collate that outputs the same as StableBaseline.\"\"\" episode = np . load ( f ' { path }{ data [ 0 ] } ' ) observation_space = episode [ \"obs\" ] . shape [ 1 ] states = np . ndarray ( shape = ( 0 , observation_space )) episodes_starts = [] actions = [] rewards = [] episode_returns = [] for file in data : episode = np . load ( f ' { path }{ file } ' ) states = np . append ( states , episode [ 'obs' ], axis = 0 ) actions += episode [ 'actions' ] . tolist () rewards += episode [ 'rewards' ] . tolist () episode_returns += episode [ 'episode_returns' ] . tolist () episode_starts = np . zeros ( episode [ 'actions' ] . shape ) episode_starts [ 0 ] = 1 episodes_starts += episode_starts . tolist () states = states . reshape (( - 1 , states . shape [ - 1 ])) actions = np . array ( actions ) . reshape ( - 1 ) episodes_starts = np . array ( episodes_starts ) . reshape ( - 1 ) rewards = np . array ( rewards ) . reshape ( - 1 ) episode_returns = np . array ( episode_returns ) . squeeze () episode = { 'obs' : states , 'actions' : actions , 'rewards' : rewards , 'episode_returns' : episode_returns , 'episode_starts' : episodes_starts } np . savez ( f ' { path } teacher' , ** episode ) for file in data : os . remove ( f ' { path }{ file } ' ) return True utils Experiment Experiment dataclass to keep track of the experiments. Source code in src/imitation_datasets/utils.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 @dataclass class Experiment : \"\"\"Experiment dataclass to keep track of the experiments.\"\"\" amount : int path : str = './logs.txt' waiting : int = field ( init = False , default_factory = int ) logs : DefaultDict [ int , list ] = field ( init = False , default_factory = lambda : defaultdict ( list ) ) experiment_semaphore : asyncio . Lock = field ( init = False , default = asyncio . BoundedSemaphore ( value = 1 ) ) def __post_init__ ( self ) -> None : \"\"\"Write in log file that the dataset creation has started.\"\"\" if os . path . exists ( self . path ): os . remove ( self . path ) if not os . path . exists ( self . path ): with open ( self . path , 'w' , encoding = 'utf8' ) as log_file : log_file . write ( '#### Starting dataset creation #### \\n ' ) def is_done ( self ) -> bool : \"\"\"Check if the experiment is done. Returns: bool: True if the experiment is done, False otherwise. \"\"\" return self . amount == 0 async def start ( self , amount : int = 1 ) -> Tuple [ bool , int ]: \"\"\"Start an experiment. Args: amount (int, optional): How many experiments are left to run. Defaults to 1. Returns: status (bool): True if the experiment can be started, False otherwise. amount (int): How many experiments are left to run. \"\"\" await self . experiment_semaphore . acquire () if self . amount > 0 : self . waiting += amount self . amount -= amount self . experiment_semaphore . release () return True , self . amount self . experiment_semaphore . release () return False , - 1 async def stop ( self , status : bool , amount : int = 1 ) -> None : \"\"\"Stop an experiment. Args: status (bool): True if the experiment was successful, False otherwise. amount (int, optional): How many experiments are left to run. Defaults to 1. \"\"\" await self . experiment_semaphore . acquire () self . amount += 0 if status else amount self . waiting -= amount self . experiment_semaphore . release () def add_log ( self , experiment : int , log : str ) -> None : \"\"\"Add a log to the experiment. Args: experiment (int): Experiment index. log (str): Log to add. \"\"\" self . logs [ experiment ] . append ( log ) def write_log ( self ) -> None : \"\"\"Write the logs in the log file.\"\"\" with open ( './logs.txt' , 'a' , encoding = 'utf8' ) as log_file : for idx , logs in self . logs . items (): for log in logs : log_file . write ( f ' \\n Experiment { idx } : { log } ' ) log_file . write ( ' \\n ' ) __post_init__ () Write in log file that the dataset creation has started. src/imitation_datasets/utils.py 53 54 55 56 57 58 59 60 def __post_init__ ( self ) -> None : \"\"\"Write in log file that the dataset creation has started.\"\"\" if os . path . exists ( self . path ): os . remove ( self . path ) if not os . path . exists ( self . path ): with open ( self . path , 'w' , encoding = 'utf8' ) as log_file : log_file . write ( '#### Starting dataset creation #### \\n ' ) add_log ( experiment , log ) Add a log to the experiment. Parameters: experiment ( int ) \u2013 Experiment index. log ( str ) \u2013 Log to add. src/imitation_datasets/utils.py 102 103 104 105 106 107 108 109 def add_log ( self , experiment : int , log : str ) -> None : \"\"\"Add a log to the experiment. Args: experiment (int): Experiment index. log (str): Log to add. \"\"\" self . logs [ experiment ] . append ( log ) is_done () Check if the experiment is done. Returns: bool ( bool ) \u2013 True if the experiment is done, False otherwise. src/imitation_datasets/utils.py 62 63 64 65 66 67 68 def is_done ( self ) -> bool : \"\"\"Check if the experiment is done. Returns: bool: True if the experiment is done, False otherwise. \"\"\" return self . amount == 0 start ( amount = 1 ) async Start an experiment. Parameters: amount ( int , default: 1 ) \u2013 How many experiments are left to run. Defaults to 1. Returns: status ( bool ) \u2013 True if the experiment can be started, False otherwise. amount ( int ) \u2013 How many experiments are left to run. src/imitation_datasets/utils.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 async def start ( self , amount : int = 1 ) -> Tuple [ bool , int ]: \"\"\"Start an experiment. Args: amount (int, optional): How many experiments are left to run. Defaults to 1. Returns: status (bool): True if the experiment can be started, False otherwise. amount (int): How many experiments are left to run. \"\"\" await self . experiment_semaphore . acquire () if self . amount > 0 : self . waiting += amount self . amount -= amount self . experiment_semaphore . release () return True , self . amount self . experiment_semaphore . release () return False , - 1 stop ( status , amount = 1 ) async Stop an experiment. Parameters: status ( bool ) \u2013 True if the experiment was successful, False otherwise. amount ( int , default: 1 ) \u2013 How many experiments are left to run. Defaults to 1. src/imitation_datasets/utils.py 90 91 92 93 94 95 96 97 98 99 100 async def stop ( self , status : bool , amount : int = 1 ) -> None : \"\"\"Stop an experiment. Args: status (bool): True if the experiment was successful, False otherwise. amount (int, optional): How many experiments are left to run. Defaults to 1. \"\"\" await self . experiment_semaphore . acquire () self . amount += 0 if status else amount self . waiting -= amount self . experiment_semaphore . release () write_log () Write the logs in the log file. src/imitation_datasets/utils.py 111 112 113 114 115 116 117 def write_log ( self ) -> None : \"\"\"Write the logs in the log file.\"\"\" with open ( './logs.txt' , 'a' , encoding = 'utf8' ) as log_file : for idx , logs in self . logs . items (): for log in logs : log_file . write ( f ' \\n Experiment { idx } : { log } ' ) log_file . write ( ' \\n ' ) Context Context dataclass to keep track of the context of the experiment. Source code in src/imitation_datasets/utils.py 120 121 122 123 124 125 126 127 128 @dataclass class Context : \"\"\"Context dataclass to keep track of the context of the experiment.\"\"\" experiments : Experiment index : int def add_log ( self , log : str ) -> None : \"\"\"Add a log to the experiment.\"\"\" self . experiments . add_log ( self . index , log ) add_log ( log ) Add a log to the experiment. src/imitation_datasets/utils.py 126 127 128 def add_log ( self , log : str ) -> None : \"\"\"Add a log to the experiment.\"\"\" self . experiments . add_log ( self . index , log ) CPUS CPUS dataclass to keep track of the available CPUs. Source code in src/imitation_datasets/utils.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 @dataclass class CPUS ( metaclass = Singleton ): \"\"\"CPUS dataclass to keep track of the available CPUs.\"\"\" available_cpus : int = field ( default_factory = multiprocessing . cpu_count ()) cpus : DefaultDict [ int , bool ] = field ( init = False , default_factory = lambda : defaultdict ( bool )) cpu_semaphore : asyncio . Lock = field ( init = False ) def __post_init__ ( self ) -> None : \"\"\"Initialize the cpu_semaphore.\"\"\" if self . available_cpus > multiprocessing . cpu_count () - 1 : self . available_cpus = multiprocessing . cpu_count () - 1 self . cpu_semaphore = asyncio . BoundedSemaphore ( value = self . available_cpus ) async def cpu_allock ( self ) -> int : \"\"\"Acquire a CPU. Returns: int: CPU index. \"\"\" await self . cpu_semaphore . acquire () for idx in range ( self . available_cpus ): if not self . cpus [ idx ]: self . cpus [ idx ] = True return idx def cpu_release ( self , cpu_idx : int ) -> None : \"\"\"Release a CPU. Args: cpu_idx (int): CPU index. \"\"\" try : self . cpus [ cpu_idx ] = False self . cpu_semaphore . release () except ValueError : pass __post_init__ () Initialize the cpu_semaphore. src/imitation_datasets/utils.py 139 140 141 142 143 def __post_init__ ( self ) -> None : \"\"\"Initialize the cpu_semaphore.\"\"\" if self . available_cpus > multiprocessing . cpu_count () - 1 : self . available_cpus = multiprocessing . cpu_count () - 1 self . cpu_semaphore = asyncio . BoundedSemaphore ( value = self . available_cpus ) cpu_allock () async Acquire a CPU. Returns: int ( int ) \u2013 CPU index. src/imitation_datasets/utils.py 145 146 147 148 149 150 151 152 153 154 155 async def cpu_allock ( self ) -> int : \"\"\"Acquire a CPU. Returns: int: CPU index. \"\"\" await self . cpu_semaphore . acquire () for idx in range ( self . available_cpus ): if not self . cpus [ idx ]: self . cpus [ idx ] = True return idx cpu_release ( cpu_idx ) Release a CPU. Parameters: cpu_idx ( int ) \u2013 CPU index. src/imitation_datasets/utils.py 157 158 159 160 161 162 163 164 165 166 167 def cpu_release ( self , cpu_idx : int ) -> None : \"\"\"Release a CPU. Args: cpu_idx (int): CPU index. \"\"\" try : self . cpus [ cpu_idx ] = False self . cpu_semaphore . release () except ValueError : pass GymWrapper Wrapper for gym environment. Since Gymnasium and Gym version 0.26 there are some environments that were working under Gym-v.0.21 stopped working. This wrapper just makes sure that the output for the environment will always work with the version the user wants. Source code in src/imitation_datasets/utils.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 class GymWrapper : \"\"\" Wrapper for gym environment. Since Gymnasium and Gym version 0.26 there are some environments that were working under Gym-v.0.21 stopped working. This wrapper just makes sure that the output for the environment will always work with the version the user wants. \"\"\" def __init__ ( self , environment : Any , version : str = \"newest\" ) -> None : \"\"\" Args: name: gym environment name version: [\"newest\", \"older\"] refers to the compatibility version. In this case, \"newest\" is 0.26 and \"older\" is 0.21. \"\"\" if version not in [ \"newest\" , \"older\" ]: raise ValueError ( \"Version has to be : ['newest', 'older']\" ) self . env = environment state = environment . reset () if version == \"older\" and not isinstance ( state [ 0 ], np . floating ): raise WrapperException ( \"Incopatible environment version and wrapper version.\" ) if version == \"newest\" and not isinstance ( state [ 0 ], np . ndarray ): raise WrapperException ( \"Incopatible environment version and wrapper version.\" ) self . version = version @property def action_space ( self ): \"\"\"Map gym action_space attribute to wrapper.\"\"\" return self . env . action_space @property def observation_space ( self ): \"\"\"Map gym env_space attribute to wrapper.\"\"\" return self . env . observation_space def set_seed ( self , seed : int ) -> None : \"\"\"Set seed for all packages (Pytorch, Numpy and Python). Args: seed (optional, int): seed number to use for the random generator. \"\"\" torch . manual_seed ( seed ) np . random . seed ( seed ) random . seed ( seed ) def reset ( self ) -> Union [ Tuple [ List [ float ], Dict [ str , Any ]], List [ float ]]: \"\"\"Resets the framework and return the appropriate return.\"\"\" state = self . env . reset () if self . version == \"newest\" : return state [ 0 ] return state def step ( self , action : Union [ float , int ] ) -> Union [ Tuple [ List [ float ], float , bool , bool , Dict [ str , Any ]], Tuple [ List [ float ], float , bool , Dict [ str , Any ]] ]: \"\"\" Perform an action in the environment and return the appropriate return according to version. \"\"\" gym_return = self . env . step ( action ) if self . version == \"newest\" : state , reward , terminated , truncated , info = gym_return return state , reward , terminated or truncated , info return gym_return def render ( self , mode = \"rgb_array\" ): \"\"\"Return the render for the environment.\"\"\" if self . version == \"newest\" : state = self . env . render () if state is None and self . env . render_mode != \"human\" : raise WrapperException ( \"No render mode set.\" ) return state return self . env . render ( mode ) def close ( self ) -> None : \"\"\"Close the environment.\"\"\" self . env . close () action_space property Map gym action_space attribute to wrapper. observation_space property Map gym env_space attribute to wrapper. __init__ ( environment , version = 'newest' ) Parameters: name \u2013 gym environment name version ( str , default: 'newest' ) \u2013 [\"newest\", \"older\"] refers to the compatibility version. In this case, \"newest\" is 0.26 and \"older\" is 0.21. src/imitation_datasets/utils.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def __init__ ( self , environment : Any , version : str = \"newest\" ) -> None : \"\"\" Args: name: gym environment name version: [\"newest\", \"older\"] refers to the compatibility version. In this case, \"newest\" is 0.26 and \"older\" is 0.21. \"\"\" if version not in [ \"newest\" , \"older\" ]: raise ValueError ( \"Version has to be : ['newest', 'older']\" ) self . env = environment state = environment . reset () if version == \"older\" and not isinstance ( state [ 0 ], np . floating ): raise WrapperException ( \"Incopatible environment version and wrapper version.\" ) if version == \"newest\" and not isinstance ( state [ 0 ], np . ndarray ): raise WrapperException ( \"Incopatible environment version and wrapper version.\" ) self . version = version close () Close the environment. src/imitation_datasets/utils.py 266 267 268 def close ( self ) -> None : \"\"\"Close the environment.\"\"\" self . env . close () render ( mode = 'rgb_array' ) Return the render for the environment. src/imitation_datasets/utils.py 256 257 258 259 260 261 262 263 264 def render ( self , mode = \"rgb_array\" ): \"\"\"Return the render for the environment.\"\"\" if self . version == \"newest\" : state = self . env . render () if state is None and self . env . render_mode != \"human\" : raise WrapperException ( \"No render mode set.\" ) return state return self . env . render ( mode ) reset () Resets the framework and return the appropriate return. src/imitation_datasets/utils.py 231 232 233 234 235 236 def reset ( self ) -> Union [ Tuple [ List [ float ], Dict [ str , Any ]], List [ float ]]: \"\"\"Resets the framework and return the appropriate return.\"\"\" state = self . env . reset () if self . version == \"newest\" : return state [ 0 ] return state set_seed ( seed ) Set seed for all packages (Pytorch, Numpy and Python). Parameters: seed ( ( optional , int ) ) \u2013 seed number to use for the random generator. src/imitation_datasets/utils.py 221 222 223 224 225 226 227 228 229 def set_seed ( self , seed : int ) -> None : \"\"\"Set seed for all packages (Pytorch, Numpy and Python). Args: seed (optional, int): seed number to use for the random generator. \"\"\" torch . manual_seed ( seed ) np . random . seed ( seed ) random . seed ( seed ) step ( action ) Perform an action in the environment and return the appropriate return according to version. src/imitation_datasets/utils.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def step ( self , action : Union [ float , int ] ) -> Union [ Tuple [ List [ float ], float , bool , bool , Dict [ str , Any ]], Tuple [ List [ float ], float , bool , Dict [ str , Any ]] ]: \"\"\" Perform an action in the environment and return the appropriate return according to version. \"\"\" gym_return = self . env . step ( action ) if self . version == \"newest\" : state , reward , terminated , truncated , info = gym_return return state , reward , terminated or truncated , info return gym_return WrapperException Bases: Exception Wrapper exception for all exceptions related to the wrapper. Source code in src/imitation_datasets/utils.py 174 175 176 177 178 179 class WrapperException ( Exception ): \"\"\"Wrapper exception for all exceptions related to the wrapper.\"\"\" def __init__ ( self , message : str ) -> None : self . message = message super () . __init__ ( self . message ) dataset BaselineDataset Bases: Dataset Teacher dataset for IL methods. Source code in src/imitation_datasets/dataset/dataset.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class BaselineDataset ( Dataset ): \"\"\"Teacher dataset for IL methods.\"\"\" def __init__ ( self , path : str , source : str = \"local\" , split : str = \"train\" , n_episodes : int = None ) -> None : \"\"\"Initialize dataset. Args: path (Str): path to the dataset. source (str): whether is a HuggingFace or a local dataset. Defaults to 'local'. Raises: ValueError: if path does not exist. \"\"\" if source == \"local\" and not os . path . exists ( path ): raise ValueError ( f \"No dataset at: { path } \" ) if source == \"local\" : self . data = np . load ( path , allow_pickle = True ) self . average_reward = np . mean ( self . data [ \"episode_returns\" ]) else : dataset = load_dataset ( path , split = \"train\" ) self . data = huggingface_to_baseline ( dataset ) self . average_reward = [] self . states = np . ndarray ( shape = ( 0 , self . data [ \"obs\" ] . shape [ - 1 ])) self . next_states = np . ndarray ( shape = ( 0 , self . data [ \"obs\" ] . shape [ - 1 ])) if len ( self . data [ \"actions\" ] . shape ) == 1 : action_size = 1 else : action_size = self . data [ \"actions\" ] . shape [ - 1 ] self . actions = np . ndarray ( shape = ( 0 , action_size )) episode_starts = list ( np . where ( self . data [ \"episode_starts\" ] == 1 )[ 0 ]) episode_starts . append ( len ( self . data [ \"episode_starts\" ])) if n_episodes is not None : if split == \"train\" : episode_starts = episode_starts [: n_episodes + 1 ] else : episode_starts = episode_starts [ n_episodes :] for start , end in zip ( episode_starts , tqdm ( episode_starts [ 1 :], desc = \"Creating dataset\" )): episode = self . data [ \"obs\" ][ start : end ] actions = self . data [ \"actions\" ][ start : end ] . reshape (( - 1 , 1 )) self . actions = np . append ( self . actions , actions [: - 1 ], axis = 0 ) self . states = np . append ( self . states , episode [: - 1 ], axis = 0 ) self . next_states = np . append ( self . next_states , episode [ 1 :], axis = 0 ) if source != \"local\" : self . average_reward . append ( self . data [ \"rewards\" ][ start : end ] . sum ()) if isinstance ( self . average_reward , list ): self . average_reward = np . mean ( self . average_reward ) assert self . states . shape [ 0 ] == self . actions . shape [ 0 ] == self . next_states . shape [ 0 ] self . states = torch . from_numpy ( self . states ) self . actions = torch . from_numpy ( self . actions ) self . next_states = torch . from_numpy ( self . next_states ) def __len__ ( self ) -> int : \"\"\"Dataset length. Returns: length (int): length. \"\"\" return self . states . shape [ 0 ] def __getitem__ ( self , index : int ) -> Tuple [ torch . Tensor ]: \"\"\"Get item from dataset. Args: index (int): index. Returns: state (torch.Tensor): state for timestep t. action (torch.Tensor): action for timestep t. next_state (torch.Tensor): state for timestep t + 1. \"\"\" state = self . states [ index ] action = self . actions [ index ] next_state = self . next_states [ index ] return state , action , next_state __getitem__ ( index ) Get item from dataset. Parameters: index ( int ) \u2013 index. Returns: state ( Tensor ) \u2013 state for timestep t. action ( Tensor ) \u2013 action for timestep t. next_state ( Tensor ) \u2013 state for timestep t + 1. src/imitation_datasets/dataset/dataset.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def __getitem__ ( self , index : int ) -> Tuple [ torch . Tensor ]: \"\"\"Get item from dataset. Args: index (int): index. Returns: state (torch.Tensor): state for timestep t. action (torch.Tensor): action for timestep t. next_state (torch.Tensor): state for timestep t + 1. \"\"\" state = self . states [ index ] action = self . actions [ index ] next_state = self . next_states [ index ] return state , action , next_state __init__ ( path , source = 'local' , split = 'train' , n_episodes = None ) Initialize dataset. Parameters: path ( Str ) \u2013 path to the dataset. source ( str , default: 'local' ) \u2013 whether is a HuggingFace or a local dataset. Defaults to 'local'. Raises: ValueError \u2013 if path does not exist. src/imitation_datasets/dataset/dataset.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def __init__ ( self , path : str , source : str = \"local\" , split : str = \"train\" , n_episodes : int = None ) -> None : \"\"\"Initialize dataset. Args: path (Str): path to the dataset. source (str): whether is a HuggingFace or a local dataset. Defaults to 'local'. Raises: ValueError: if path does not exist. \"\"\" if source == \"local\" and not os . path . exists ( path ): raise ValueError ( f \"No dataset at: { path } \" ) if source == \"local\" : self . data = np . load ( path , allow_pickle = True ) self . average_reward = np . mean ( self . data [ \"episode_returns\" ]) else : dataset = load_dataset ( path , split = \"train\" ) self . data = huggingface_to_baseline ( dataset ) self . average_reward = [] self . states = np . ndarray ( shape = ( 0 , self . data [ \"obs\" ] . shape [ - 1 ])) self . next_states = np . ndarray ( shape = ( 0 , self . data [ \"obs\" ] . shape [ - 1 ])) if len ( self . data [ \"actions\" ] . shape ) == 1 : action_size = 1 else : action_size = self . data [ \"actions\" ] . shape [ - 1 ] self . actions = np . ndarray ( shape = ( 0 , action_size )) episode_starts = list ( np . where ( self . data [ \"episode_starts\" ] == 1 )[ 0 ]) episode_starts . append ( len ( self . data [ \"episode_starts\" ])) if n_episodes is not None : if split == \"train\" : episode_starts = episode_starts [: n_episodes + 1 ] else : episode_starts = episode_starts [ n_episodes :] for start , end in zip ( episode_starts , tqdm ( episode_starts [ 1 :], desc = \"Creating dataset\" )): episode = self . data [ \"obs\" ][ start : end ] actions = self . data [ \"actions\" ][ start : end ] . reshape (( - 1 , 1 )) self . actions = np . append ( self . actions , actions [: - 1 ], axis = 0 ) self . states = np . append ( self . states , episode [: - 1 ], axis = 0 ) self . next_states = np . append ( self . next_states , episode [ 1 :], axis = 0 ) if source != \"local\" : self . average_reward . append ( self . data [ \"rewards\" ][ start : end ] . sum ()) if isinstance ( self . average_reward , list ): self . average_reward = np . mean ( self . average_reward ) assert self . states . shape [ 0 ] == self . actions . shape [ 0 ] == self . next_states . shape [ 0 ] self . states = torch . from_numpy ( self . states ) self . actions = torch . from_numpy ( self . actions ) self . next_states = torch . from_numpy ( self . next_states ) __len__ () Dataset length. Returns: length ( int ) \u2013 length. src/imitation_datasets/dataset/dataset.py 82 83 84 85 86 87 88 def __len__ ( self ) -> int : \"\"\"Dataset length. Returns: length (int): length. \"\"\" return self . states . shape [ 0 ] huggingface baseline_to_huggingface Loads baseline dataset from NpzFile, converts into a dict and save it into a JSONL file for upload. Parameters: dataset_path ( str ) \u2013 path to the npz file. new_path ( str ) \u2013 path to the new dataset. Raises: ValueError \u2013 if one of the paths does not exist. src/imitation_datasets/dataset/huggingface.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def baseline_to_huggingface ( dataset_path : str , new_path : str ) -> None : \"\"\"Loads baseline dataset from NpzFile, converts into a dict and save it into a JSONL file for upload. Args: dataset_path (str): path to the npz file. new_path (str): path to the new dataset. Raises: ValueError: if one of the paths does not exist. \"\"\" path = \"/\" . join ( dataset_path . split ( \"/\" )[: - 1 ]) if not os . path . exists ( path ): raise ValueError ( f \"' { path } ' does not exist.\" ) path = \"/\" . join ( new_path . split ( \"/\" )[: - 1 ]) if not os . path . exists ( path ): raise ValueError ( f \"' { path } ' does not exist.\" ) dataset = np . load ( dataset_path , allow_pickle = True ) dataset = convert_baseline_dataset_to_dict ( dataset ) save_dataset_into_huggingface_format ( dataset , new_path ) baseline_to_huggingface Loads baseline dataset from NpzFile, converts into a dict and save it into a JSONL file for upload. Parameters: dataset_path ( str ) \u2013 path to the npz file. new_path ( str ) \u2013 path to the new dataset. Raises: ValueError \u2013 if one of the paths does not exist. src/imitation_datasets/dataset/huggingface.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def baseline_to_huggingface ( dataset_path : str , new_path : str ) -> None : \"\"\"Loads baseline dataset from NpzFile, converts into a dict and save it into a JSONL file for upload. Args: dataset_path (str): path to the npz file. new_path (str): path to the new dataset. Raises: ValueError: if one of the paths does not exist. \"\"\" path = \"/\" . join ( dataset_path . split ( \"/\" )[: - 1 ]) if not os . path . exists ( path ): raise ValueError ( f \"' { path } ' does not exist.\" ) path = \"/\" . join ( new_path . split ( \"/\" )[: - 1 ]) if not os . path . exists ( path ): raise ValueError ( f \"' { path } ' does not exist.\" ) dataset = np . load ( dataset_path , allow_pickle = True ) dataset = convert_baseline_dataset_to_dict ( dataset ) save_dataset_into_huggingface_format ( dataset , new_path ) metrics performance Compute the performance for the agent. Performance normalises between random and expert policies rewards, where performance 0 corresponds to random policy performance, and 1 are for expert policy performance. performance = (X - X_min) / (X_max - X_min), where X_min is the random_reward, and X_max is the teacher_reward. Parameters: agent_reward ( Number ) \u2013 agent accumulated reward. teacher_reward ( Number ) \u2013 teacher accumulated reward. random_reward ( Number ) \u2013 random agent accumulated reward. Raises: ValueError \u2013 if the teacher reward is inferior to the random agent. ValueError \u2013 Teacher and Random rewards should be Numbers. Returns: performance ( Number ) \u2013 performance metric. src/imitation_datasets/dataset/metrics.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def performance ( agent_reward : Union [ Number , List , np . ndarray ], teacher_reward : Number , random_reward : Number ) -> Dict [ str , Number ]: \"\"\"Compute the performance for the agent. Performance normalises between random and expert policies rewards, where performance 0 corresponds to random policy performance, and 1 are for expert policy performance. performance = (X - X_min) / (X_max - X_min), where X_min is the random_reward, and X_max is the teacher_reward. Args: agent_reward (Number): agent accumulated reward. teacher_reward (Number): teacher accumulated reward. random_reward (Number): random agent accumulated reward. Raises: ValueError: if the teacher reward is inferior to the random agent. ValueError: Teacher and Random rewards should be Numbers. Returns: performance (Number): performance metric. \"\"\" if isinstance ( teacher_reward , ( list , np . ndarray )): raise ValueError ( \"Teacher reward should not be a list\" ) if isinstance ( random_reward , ( list , np . ndarray )): raise ValueError ( \"Random reward should not be a list\" ) if teacher_reward < random_reward : raise ValueError ( \"Random reward should lesser than the teacher's.\" ) if isinstance ( agent_reward , list ): agent_reward = np . array ( agent_reward ) perf = ( agent_reward - random_reward ) / ( teacher_reward - random_reward ) if isinstance ( perf , np . ndarray ): return { \"performance\" : perf . mean (), \"performance_std\" : perf . std ()} return { \"performance\" : perf , \"performance_std\" : 0 } average_episodic_reward Compute the average episodic reward for the agent. AER is the average of 'n' episodes for each agent in each environment. Parameters: agent_reward ( List [ Number ] ) \u2013 list of each episode accumulated reward. Returns: AER ( Number ) \u2013 average episodic reward metric. src/imitation_datasets/dataset/metrics.py 52 53 54 55 56 57 58 59 60 61 62 63 64 def average_episodic_reward ( agent_reward : List [ Number ]) -> Dict [ str , Number ]: \"\"\"Compute the average episodic reward for the agent. AER is the average of 'n' episodes for each agent in each environment. Args: agent_reward (List[Number]): list of each episode accumulated reward. Returns: AER (Number): average episodic reward metric. \"\"\" if isinstance ( agent_reward , list ): agent_reward = np . array ( agent_reward ) return { \"aer\" : agent_reward . mean (), \"aer_std\" : agent_reward . std ()} accuracy Compute the accuracy for a model. The accuracy returned is the percentage from 0 to 100. Parameters: prediction ( Tensor ) \u2013 logits from a model. ground_truth ( Tensor ) \u2013 ground truth class. Raises: ValueError \u2013 if predictions and ground_truth are not torch.Tensor. ValueError \u2013 if predictions are not two dimensional. ValueError \u2013 if ground_truth is not one dimensional. Returns: accuracy ( Number ) \u2013 accuracy between 0 and 100 for a model. src/imitation_datasets/dataset/metrics.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def accuracy ( prediction : Tensor , ground_truth : Tensor ) -> Number : \"\"\"Compute the accuracy for a model. The accuracy returned is the percentage from 0 to 100. Args: prediction (torch.Tensor): logits from a model. ground_truth (torch.Tensor): ground truth class. Raises: ValueError: if predictions and ground_truth are not torch.Tensor. ValueError: if predictions are not two dimensional. ValueError: if ground_truth is not one dimensional. Returns: accuracy (Number): accuracy between 0 and 100 for a model. \"\"\" if not isinstance ( prediction , Tensor ) or not isinstance ( ground_truth , Tensor ): raise ValueError ( \"'prediction' and 'ground truth' should be a tensor\" ) if len ( prediction . size ()) != 2 : raise ValueError ( \"'prediction' and 'ground truth' need to be 2 dimensional.\" ) if len ( ground_truth . size ()) != 1 : raise ValueError ( \"'ground truth' need to be 1 dimensional.\" ) return (( argmax ( prediction , 1 ) == ground_truth ) . sum () . item () / ground_truth . size ( 0 )) * 100","title":"Imitation Datasets"},{"location":"api/imitation/#imitation_datasets","text":"","title":"imitation_datasets"},{"location":"api/imitation/#controller","text":"Controller for running experiments. Source code in src/imitation_datasets/controller.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 class Controller : \"\"\"Controller for running experiments.\"\"\" def __init__ ( self , enjoy : EnjoyFunction , collate : CollateFunction , amount : int , threads : int = 1 , path : str = './dataset/' ) -> None : \"\"\"Initialize the controller. Args: enjoy (EnjoyFunction): Function to run the expert. collate (CollateFunction): Function to collate the data. amount (int): Amount of episodes to run. threads (int, optional): Amount of threads to use. Defaults to 1. path (str, optional): Path to save the dataset. Defaults to './dataset/'. \"\"\" self . enjoy = enjoy self . collate = collate self . threads = CPUS ( threads ) self . experiments = Experiment ( amount ) self . path = path self . pbar = None set_start_method ( 'spawn' , force = True ) def create_folder ( self , path : str ) -> None : \"\"\"Create a folder if it does not exist. Args: path (str): Path to the folder. \"\"\" if not os . path . exists ( path ): os . makedirs ( path ) async def set_cpu ( self , cpu : int ) -> None : \"\"\"Set the cpu affinity for the current process. Args: cpu (int): CPU index to use. \"\"\" try : proc = psutil . Process () proc . cpu_affinity ([ int ( cpu )]) if 'linux' in platform : os . sched_setaffinity ( proc . pid , [ int ( cpu )]) except OSError : pass def enjoy_closure ( self , opt : Namespace ) -> EnjoyFunction : \"\"\"Create a closure for the enjoy function. Args: opt (Namespace): Namespace with the arguments. Returns: EnjoyFunction: Enjoy function with part of the arguments. \"\"\" os . system ( \"set LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so\" ) os . system ( \"set LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia\" ) return partial ( self . enjoy , expert = Experts . get_expert ( opt . game )) def collate_closure ( self , opt : Namespace ) -> CollateFunction : \"\"\"Create a closure for the collate function. Args: opt (Namespace): Namespace with the arguments. Returns: CollateFunction: Collate function with part of the arguments. \"\"\" path = f ' { self . path }{ opt . game } /' files = list ( listdir ( path )) return partial ( self . collate , data = files , path = path ) async def enjoy_sequence ( self , future : EnjoyFunction , executor : ProcessPoolExecutor ) -> bool : \"\"\"_summary_ Args: future (EnjoyFunction): Enjoy function already with async future. executor (ProcessPoolExecutor): Executor to run the future. Returns: bool: Result of the future. True if the expert was able to solve the game. False otherwise. \"\"\" # Pre cpu = await self . threads . cpu_allock () await self . experiments . start () await self . set_cpu ( cpu ) # Enjoy result = await asyncio . get_event_loop () . run_in_executor ( executor , future ) # Post self . threads . cpu_release ( cpu ) await self . experiments . stop ( result ) self . pbar . update ( 1 if result else 0 ) return result if result else await asyncio . gather ( self . enjoy_sequence ( future , executor )) async def run ( self , opt ) -> None : \"\"\"Run the experiments. Args: opt (Namespace): Namespace with the arguments. \"\"\" path = f ' { self . path }{ opt . game } /' self . create_folder ( path ) tasks = [] with ProcessPoolExecutor () as executor : for idx in range ( self . experiments . amount ): enjoy = self . enjoy_closure ( opt ) enjoy = partial ( enjoy , path = path , context = Context ( self . experiments , idx )) task = asyncio . ensure_future ( self . enjoy_sequence ( enjoy , executor ) ) tasks . append ( task ) await asyncio . gather ( * tasks ) def start ( self , opt : Namespace ): \"\"\"Start the experiments. Args: opt (Namespace): Namespace with the arguments. Raises: exception: Exception (general) raised during the execution. \"\"\" try : if opt . mode in [ 'all' , 'play' ]: self . pbar = tqdm ( range ( self . experiments . amount ), desc = 'Running episodes' ) asyncio . run ( self . run ( opt )) if opt . mode in [ 'all' , 'collate' ]: self . pbar = tqdm ( range ( self . experiments . amount ), desc = 'Running collate' ) collate = self . collate_closure ( opt ) collate () except Exception as exception : self . experiments . add_log ( - 99 , exception ) raise exception finally : self . experiments . write_log ()","title":"controller"},{"location":"api/imitation/#imitation_datasets.controller.Controller.__init__","text":"Initialize the controller. Parameters: enjoy ( EnjoyFunction ) \u2013 Function to run the expert. collate ( CollateFunction ) \u2013 Function to collate the data. amount ( int ) \u2013 Amount of episodes to run. threads ( int , default: 1 ) \u2013 Amount of threads to use. Defaults to 1. path ( str , default: './dataset/' ) \u2013 Path to save the dataset. Defaults to './dataset/'. src/imitation_datasets/controller.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , enjoy : EnjoyFunction , collate : CollateFunction , amount : int , threads : int = 1 , path : str = './dataset/' ) -> None : \"\"\"Initialize the controller. Args: enjoy (EnjoyFunction): Function to run the expert. collate (CollateFunction): Function to collate the data. amount (int): Amount of episodes to run. threads (int, optional): Amount of threads to use. Defaults to 1. path (str, optional): Path to save the dataset. Defaults to './dataset/'. \"\"\" self . enjoy = enjoy self . collate = collate self . threads = CPUS ( threads ) self . experiments = Experiment ( amount ) self . path = path self . pbar = None set_start_method ( 'spawn' , force = True )","title":"__init__()"},{"location":"api/imitation/#imitation_datasets.controller.Controller.collate_closure","text":"Create a closure for the collate function. Parameters: opt ( Namespace ) \u2013 Namespace with the arguments. Returns: CollateFunction ( CollateFunction ) \u2013 Collate function with part of the arguments. src/imitation_datasets/controller.py 83 84 85 86 87 88 89 90 91 92 93 94 def collate_closure ( self , opt : Namespace ) -> CollateFunction : \"\"\"Create a closure for the collate function. Args: opt (Namespace): Namespace with the arguments. Returns: CollateFunction: Collate function with part of the arguments. \"\"\" path = f ' { self . path }{ opt . game } /' files = list ( listdir ( path )) return partial ( self . collate , data = files , path = path )","title":"collate_closure()"},{"location":"api/imitation/#imitation_datasets.controller.Controller.create_folder","text":"Create a folder if it does not exist. Parameters: path ( str ) \u2013 Path to the folder. src/imitation_datasets/controller.py 47 48 49 50 51 52 53 54 def create_folder ( self , path : str ) -> None : \"\"\"Create a folder if it does not exist. Args: path (str): Path to the folder. \"\"\" if not os . path . exists ( path ): os . makedirs ( path )","title":"create_folder()"},{"location":"api/imitation/#imitation_datasets.controller.Controller.enjoy_closure","text":"Create a closure for the enjoy function. Parameters: opt ( Namespace ) \u2013 Namespace with the arguments. Returns: EnjoyFunction ( EnjoyFunction ) \u2013 Enjoy function with part of the arguments. src/imitation_datasets/controller.py 70 71 72 73 74 75 76 77 78 79 80 81 def enjoy_closure ( self , opt : Namespace ) -> EnjoyFunction : \"\"\"Create a closure for the enjoy function. Args: opt (Namespace): Namespace with the arguments. Returns: EnjoyFunction: Enjoy function with part of the arguments. \"\"\" os . system ( \"set LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so\" ) os . system ( \"set LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia\" ) return partial ( self . enjoy , expert = Experts . get_expert ( opt . game ))","title":"enjoy_closure()"},{"location":"api/imitation/#imitation_datasets.controller.Controller.enjoy_sequence","text":"summary Parameters: future ( EnjoyFunction ) \u2013 Enjoy function already with async future. executor ( ProcessPoolExecutor ) \u2013 Executor to run the future. Returns: bool ( bool ) \u2013 Result of the future. True if the expert was able to solve the game. False otherwise. src/imitation_datasets/controller.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 async def enjoy_sequence ( self , future : EnjoyFunction , executor : ProcessPoolExecutor ) -> bool : \"\"\"_summary_ Args: future (EnjoyFunction): Enjoy function already with async future. executor (ProcessPoolExecutor): Executor to run the future. Returns: bool: Result of the future. True if the expert was able to solve the game. False otherwise. \"\"\" # Pre cpu = await self . threads . cpu_allock () await self . experiments . start () await self . set_cpu ( cpu ) # Enjoy result = await asyncio . get_event_loop () . run_in_executor ( executor , future ) # Post self . threads . cpu_release ( cpu ) await self . experiments . stop ( result ) self . pbar . update ( 1 if result else 0 ) return result if result else await asyncio . gather ( self . enjoy_sequence ( future , executor ))","title":"enjoy_sequence()"},{"location":"api/imitation/#imitation_datasets.controller.Controller.run","text":"Run the experiments. Parameters: opt ( Namespace ) \u2013 Namespace with the arguments. src/imitation_datasets/controller.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 async def run ( self , opt ) -> None : \"\"\"Run the experiments. Args: opt (Namespace): Namespace with the arguments. \"\"\" path = f ' { self . path }{ opt . game } /' self . create_folder ( path ) tasks = [] with ProcessPoolExecutor () as executor : for idx in range ( self . experiments . amount ): enjoy = self . enjoy_closure ( opt ) enjoy = partial ( enjoy , path = path , context = Context ( self . experiments , idx )) task = asyncio . ensure_future ( self . enjoy_sequence ( enjoy , executor ) ) tasks . append ( task ) await asyncio . gather ( * tasks )","title":"run()"},{"location":"api/imitation/#imitation_datasets.controller.Controller.set_cpu","text":"Set the cpu affinity for the current process. Parameters: cpu ( int ) \u2013 CPU index to use. src/imitation_datasets/controller.py 56 57 58 59 60 61 62 63 64 65 66 67 68 async def set_cpu ( self , cpu : int ) -> None : \"\"\"Set the cpu affinity for the current process. Args: cpu (int): CPU index to use. \"\"\" try : proc = psutil . Process () proc . cpu_affinity ([ int ( cpu )]) if 'linux' in platform : os . sched_setaffinity ( proc . pid , [ int ( cpu )]) except OSError : pass","title":"set_cpu()"},{"location":"api/imitation/#imitation_datasets.controller.Controller.start","text":"Start the experiments. Parameters: opt ( Namespace ) \u2013 Namespace with the arguments. Raises: exception \u2013 Exception (general) raised during the execution. src/imitation_datasets/controller.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def start ( self , opt : Namespace ): \"\"\"Start the experiments. Args: opt (Namespace): Namespace with the arguments. Raises: exception: Exception (general) raised during the execution. \"\"\" try : if opt . mode in [ 'all' , 'play' ]: self . pbar = tqdm ( range ( self . experiments . amount ), desc = 'Running episodes' ) asyncio . run ( self . run ( opt )) if opt . mode in [ 'all' , 'collate' ]: self . pbar = tqdm ( range ( self . experiments . amount ), desc = 'Running collate' ) collate = self . collate_closure ( opt ) collate () except Exception as exception : self . experiments . add_log ( - 99 , exception ) raise exception finally : self . experiments . write_log ()","title":"start()"},{"location":"api/imitation/#experts","text":"","title":"experts"},{"location":"api/imitation/#policy","text":"Policy dataclass to load and use expert policies. Source code in src/imitation_datasets/experts.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 @dataclass class Policy : \"\"\"Policy dataclass to load and use expert policies.\"\"\" name : str repo_id : str filename : str threshold : float algo : BaseAlgorithm policy : BaseAlgorithm = field ( init = False , default = None ) internal_state : Any = field ( init = False , default = None ) environment : Any = field ( init = False , default = None ) def load ( self ) -> BaseAlgorithm : \"\"\" Load policy from HuggingFace hub. It uses a custom_object to replicate stable_baselines behaviour. custom_objects = { \"learning_rate\": 0.0, \"lr_schedule\": lambda _: 0.0, \"clip_range\": lambda _: 0.0 } Returns: BaseAlgorithm: Stable baseline policy loaded from HuggingFace hub. \"\"\" checkpoint = load_from_hub ( repo_id = self . repo_id , filename = self . filename , ) custom_objects = { \"learning_rate\" : 0.0 , \"lr_schedule\" : lambda _ : 0.0 , \"clip_range\" : lambda _ : 0.0 } self . policy = self . algo . load ( checkpoint , custom_objects = custom_objects ) return self . policy def predict ( self , obs : List [ Union [ int , float ]], deterministic : bool = True ) -> Tuple [ Union [ int , float , List [ Union [ int , float ]]], Union [ int , float , List [ Union [ int , float ]]] ]: \"\"\" Predict action given observation. Args: obs (List[int | float]): observation from environment. deterministic (bool, optional): Use exploration to predict action. Defaults to True. Returns: action (Union[int, float, List[Union[int, float]]]): action predicted by the policy. internal_states (Union[int, float, List[Union[int, float]]]): internal states of the policy. Note: typing depends on the environment. \"\"\" action , internal_states = self . policy . predict ( obs , state = self . internal_state , deterministic = deterministic , ) self . internal_state = internal_states return action , internal_states def get_environment ( self ) -> str : \"\"\"Return environment name. Returns: str: environment name. \"\"\" if self . environment is None : self . environment = gym . make ( self . name , render_mode = \"rgb_array\" ) return self . environment","title":"Policy"},{"location":"api/imitation/#imitation_datasets.experts.Policy.get_environment","text":"Return environment name. Returns: str ( str ) \u2013 environment name. src/imitation_datasets/experts.py 87 88 89 90 91 92 93 94 95 def get_environment ( self ) -> str : \"\"\"Return environment name. Returns: str: environment name. \"\"\" if self . environment is None : self . environment = gym . make ( self . name , render_mode = \"rgb_array\" ) return self . environment","title":"get_environment()"},{"location":"api/imitation/#imitation_datasets.experts.Policy.load","text":"Load policy from HuggingFace hub. It uses a custom_object to replicate stable_baselines behaviour. custom_objects = { \"learning_rate\": 0.0, \"lr_schedule\": lambda : 0.0, \"clip_range\": lambda : 0.0 } Returns: BaseAlgorithm ( BaseAlgorithm ) \u2013 Stable baseline policy loaded from HuggingFace hub. src/imitation_datasets/experts.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def load ( self ) -> BaseAlgorithm : \"\"\" Load policy from HuggingFace hub. It uses a custom_object to replicate stable_baselines behaviour. custom_objects = { \"learning_rate\": 0.0, \"lr_schedule\": lambda _: 0.0, \"clip_range\": lambda _: 0.0 } Returns: BaseAlgorithm: Stable baseline policy loaded from HuggingFace hub. \"\"\" checkpoint = load_from_hub ( repo_id = self . repo_id , filename = self . filename , ) custom_objects = { \"learning_rate\" : 0.0 , \"lr_schedule\" : lambda _ : 0.0 , \"clip_range\" : lambda _ : 0.0 } self . policy = self . algo . load ( checkpoint , custom_objects = custom_objects ) return self . policy","title":"load()"},{"location":"api/imitation/#imitation_datasets.experts.Policy.predict","text":"Predict action given observation. Parameters: obs ( List [ int | float ] ) \u2013 observation from environment. deterministic ( bool , default: True ) \u2013 Use exploration to predict action. Defaults to True. Returns: action ( Union [ int , float , List [ Union [ int , float ]]] ) \u2013 action predicted by the policy. internal_states ( Union [ int , float , List [ Union [ int , float ]]] ) \u2013 internal states of the policy. Note: typing depends on the environment. src/imitation_datasets/experts.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def predict ( self , obs : List [ Union [ int , float ]], deterministic : bool = True ) -> Tuple [ Union [ int , float , List [ Union [ int , float ]]], Union [ int , float , List [ Union [ int , float ]]] ]: \"\"\" Predict action given observation. Args: obs (List[int | float]): observation from environment. deterministic (bool, optional): Use exploration to predict action. Defaults to True. Returns: action (Union[int, float, List[Union[int, float]]]): action predicted by the policy. internal_states (Union[int, float, List[Union[int, float]]]): internal states of the policy. Note: typing depends on the environment. \"\"\" action , internal_states = self . policy . predict ( obs , state = self . internal_state , deterministic = deterministic , ) self . internal_state = internal_states return action , internal_states","title":"predict()"},{"location":"api/imitation/#experts_1","text":"Helper class to register and get expert policies. Source code in src/imitation_datasets/experts.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class Experts : \"\"\"Helper class to register and get expert policies.\"\"\" experts : Dict [ str , Policy ] = { key : Policy ( ** value ) for env in [ atari , classic , mujoco ] for key , value in env . items () } @classmethod def register ( cls , identifier : str , policy : Policy ) -> None : \"\"\"Register a new policy.\"\"\" if not isinstance ( policy . threshold , float ): policy . threshold = float ( policy . threshold ) cls . experts [ identifier ] = policy @classmethod def get_expert ( cls , identifier : str ) -> Policy : \"\"\"Return expert policy. Args: identifier (str): identifier of the policy. Returns: Policy: dataclass with expert policy information. \"\"\" try : return cls . experts [ identifier ] except KeyError : return None @classmethod def get_register ( cls ) -> None : \"\"\"Print entire register of expert policies.\"\"\" print ( cls . experts )","title":"Experts"},{"location":"api/imitation/#imitation_datasets.experts.Experts.get_expert","text":"Return expert policy. Parameters: identifier ( str ) \u2013 identifier of the policy. Returns: Policy ( Policy ) \u2013 dataclass with expert policy information. src/imitation_datasets/experts.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @classmethod def get_expert ( cls , identifier : str ) -> Policy : \"\"\"Return expert policy. Args: identifier (str): identifier of the policy. Returns: Policy: dataclass with expert policy information. \"\"\" try : return cls . experts [ identifier ] except KeyError : return None","title":"get_expert()"},{"location":"api/imitation/#imitation_datasets.experts.Experts.get_register","text":"Print entire register of expert policies. src/imitation_datasets/experts.py 127 128 129 130 @classmethod def get_register ( cls ) -> None : \"\"\"Print entire register of expert policies.\"\"\" print ( cls . experts )","title":"get_register()"},{"location":"api/imitation/#imitation_datasets.experts.Experts.register","text":"Register a new policy. src/imitation_datasets/experts.py 104 105 106 107 108 109 110 @classmethod def register ( cls , identifier : str , policy : Policy ) -> None : \"\"\"Register a new policy.\"\"\" if not isinstance ( policy . threshold , float ): policy . threshold = float ( policy . threshold ) cls . experts [ identifier ] = policy","title":"register()"},{"location":"api/imitation/#functions","text":"","title":"functions"},{"location":"api/imitation/#enjoy","text":"This is a simple enjoy function example. It has three arguments and should return a boolean. src/imitation_datasets/functions.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def enjoy ( expert : Policy , path : str , context : Context ) -> bool : \"\"\" This is a simple enjoy function example. It has three arguments and should return a boolean. \"\"\" done = False expert . load () env = GymWrapper ( expert . get_environment (), version = \"newest\" ) states , actions = [], [] acc_reward , state = 0 , env . reset () while not done : action , _ = expert . predict ( state ) state , reward , done , _ = env . step ( action ) acc_reward += reward states . append ( state ) actions . append ( action ) env . close () episode = { 'states' : np . array ( states ), 'actions' : np . array ( actions ) } if acc_reward >= expert . threshold : np . savez ( f ' { path }{ context . index } ' , ** episode ) context . add_log ( f 'Accumulated reward { acc_reward } ' ) return acc_reward >= expert . threshold","title":"enjoy"},{"location":"api/imitation/#baseline_enjoy","text":"Enjoy following StableBaseline output. src/imitation_datasets/functions.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def baseline_enjoy ( expert : Policy , path : str , context : Context ) -> bool : \"\"\"Enjoy following StableBaseline output.\"\"\" done = False expert . load () env = GymWrapper ( expert . get_environment (), version = \"newest\" ) states = [] actions = [] rewards = [] state = env . reset () acc_reward = 0 while not done : action , _ = expert . predict ( state ) states . append ( state ) actions . append ( action ) state , reward , done , _ = env . step ( action ) acc_reward += reward rewards . append ( reward ) env . close () episode_returns = np . array ([ acc_reward ]) episode = { 'obs' : np . array ( states ), 'actions' : np . array ( actions ), 'rewards' : np . array ( rewards ), 'episode_returns' : episode_returns } if acc_reward >= expert . threshold : np . savez ( f ' { path }{ context . index } ' , ** episode ) context . add_log ( f 'Accumulated reward { acc_reward } ' ) return acc_reward >= expert . threshold","title":"baseline_enjoy"},{"location":"api/imitation/#collate","text":"This function is a simple collate function. src/imitation_datasets/functions.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def collate ( path , data ) -> bool : \"\"\"This function is a simple collate function.\"\"\" episodes_starts = [] states , actions = [], [] for file in data : episode = np . load ( f ' { path }{ file } ' ) states . append ( episode [ 'states' ]) actions . append ( episode [ 'actions' ]) episode_starts = np . zeros ( episode [ 'actions' ] . shape ) episode_starts [ 0 ] = 1 episodes_starts . append ( episode_starts ) states = np . array ( states ) states = states . reshape (( - 1 , states . shape [ - 1 ])) actions = np . array ( actions ) . reshape ( - 1 ) episodes_starts = np . array ( episodes_starts ) . reshape ( - 1 ) episode = { 'states' : states , 'actions' : actions , 'episode_starts' : episodes_starts } np . savez ( f ' { path } teacher' , ** episode ) for file in data : os . remove ( f ' { path }{ file } ' ) return True","title":"collate"},{"location":"api/imitation/#baseline_collate","text":"Collate that outputs the same as StableBaseline. src/imitation_datasets/functions.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def baseline_collate ( path : str , data : List [ str ]) -> bool : \"\"\"Collate that outputs the same as StableBaseline.\"\"\" episode = np . load ( f ' { path }{ data [ 0 ] } ' ) observation_space = episode [ \"obs\" ] . shape [ 1 ] states = np . ndarray ( shape = ( 0 , observation_space )) episodes_starts = [] actions = [] rewards = [] episode_returns = [] for file in data : episode = np . load ( f ' { path }{ file } ' ) states = np . append ( states , episode [ 'obs' ], axis = 0 ) actions += episode [ 'actions' ] . tolist () rewards += episode [ 'rewards' ] . tolist () episode_returns += episode [ 'episode_returns' ] . tolist () episode_starts = np . zeros ( episode [ 'actions' ] . shape ) episode_starts [ 0 ] = 1 episodes_starts += episode_starts . tolist () states = states . reshape (( - 1 , states . shape [ - 1 ])) actions = np . array ( actions ) . reshape ( - 1 ) episodes_starts = np . array ( episodes_starts ) . reshape ( - 1 ) rewards = np . array ( rewards ) . reshape ( - 1 ) episode_returns = np . array ( episode_returns ) . squeeze () episode = { 'obs' : states , 'actions' : actions , 'rewards' : rewards , 'episode_returns' : episode_returns , 'episode_starts' : episodes_starts } np . savez ( f ' { path } teacher' , ** episode ) for file in data : os . remove ( f ' { path }{ file } ' ) return True","title":"baseline_collate"},{"location":"api/imitation/#utils","text":"","title":"utils"},{"location":"api/imitation/#experiment","text":"Experiment dataclass to keep track of the experiments. Source code in src/imitation_datasets/utils.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 @dataclass class Experiment : \"\"\"Experiment dataclass to keep track of the experiments.\"\"\" amount : int path : str = './logs.txt' waiting : int = field ( init = False , default_factory = int ) logs : DefaultDict [ int , list ] = field ( init = False , default_factory = lambda : defaultdict ( list ) ) experiment_semaphore : asyncio . Lock = field ( init = False , default = asyncio . BoundedSemaphore ( value = 1 ) ) def __post_init__ ( self ) -> None : \"\"\"Write in log file that the dataset creation has started.\"\"\" if os . path . exists ( self . path ): os . remove ( self . path ) if not os . path . exists ( self . path ): with open ( self . path , 'w' , encoding = 'utf8' ) as log_file : log_file . write ( '#### Starting dataset creation #### \\n ' ) def is_done ( self ) -> bool : \"\"\"Check if the experiment is done. Returns: bool: True if the experiment is done, False otherwise. \"\"\" return self . amount == 0 async def start ( self , amount : int = 1 ) -> Tuple [ bool , int ]: \"\"\"Start an experiment. Args: amount (int, optional): How many experiments are left to run. Defaults to 1. Returns: status (bool): True if the experiment can be started, False otherwise. amount (int): How many experiments are left to run. \"\"\" await self . experiment_semaphore . acquire () if self . amount > 0 : self . waiting += amount self . amount -= amount self . experiment_semaphore . release () return True , self . amount self . experiment_semaphore . release () return False , - 1 async def stop ( self , status : bool , amount : int = 1 ) -> None : \"\"\"Stop an experiment. Args: status (bool): True if the experiment was successful, False otherwise. amount (int, optional): How many experiments are left to run. Defaults to 1. \"\"\" await self . experiment_semaphore . acquire () self . amount += 0 if status else amount self . waiting -= amount self . experiment_semaphore . release () def add_log ( self , experiment : int , log : str ) -> None : \"\"\"Add a log to the experiment. Args: experiment (int): Experiment index. log (str): Log to add. \"\"\" self . logs [ experiment ] . append ( log ) def write_log ( self ) -> None : \"\"\"Write the logs in the log file.\"\"\" with open ( './logs.txt' , 'a' , encoding = 'utf8' ) as log_file : for idx , logs in self . logs . items (): for log in logs : log_file . write ( f ' \\n Experiment { idx } : { log } ' ) log_file . write ( ' \\n ' )","title":"Experiment"},{"location":"api/imitation/#imitation_datasets.utils.Experiment.__post_init__","text":"Write in log file that the dataset creation has started. src/imitation_datasets/utils.py 53 54 55 56 57 58 59 60 def __post_init__ ( self ) -> None : \"\"\"Write in log file that the dataset creation has started.\"\"\" if os . path . exists ( self . path ): os . remove ( self . path ) if not os . path . exists ( self . path ): with open ( self . path , 'w' , encoding = 'utf8' ) as log_file : log_file . write ( '#### Starting dataset creation #### \\n ' )","title":"__post_init__()"},{"location":"api/imitation/#imitation_datasets.utils.Experiment.add_log","text":"Add a log to the experiment. Parameters: experiment ( int ) \u2013 Experiment index. log ( str ) \u2013 Log to add. src/imitation_datasets/utils.py 102 103 104 105 106 107 108 109 def add_log ( self , experiment : int , log : str ) -> None : \"\"\"Add a log to the experiment. Args: experiment (int): Experiment index. log (str): Log to add. \"\"\" self . logs [ experiment ] . append ( log )","title":"add_log()"},{"location":"api/imitation/#imitation_datasets.utils.Experiment.is_done","text":"Check if the experiment is done. Returns: bool ( bool ) \u2013 True if the experiment is done, False otherwise. src/imitation_datasets/utils.py 62 63 64 65 66 67 68 def is_done ( self ) -> bool : \"\"\"Check if the experiment is done. Returns: bool: True if the experiment is done, False otherwise. \"\"\" return self . amount == 0","title":"is_done()"},{"location":"api/imitation/#imitation_datasets.utils.Experiment.start","text":"Start an experiment. Parameters: amount ( int , default: 1 ) \u2013 How many experiments are left to run. Defaults to 1. Returns: status ( bool ) \u2013 True if the experiment can be started, False otherwise. amount ( int ) \u2013 How many experiments are left to run. src/imitation_datasets/utils.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 async def start ( self , amount : int = 1 ) -> Tuple [ bool , int ]: \"\"\"Start an experiment. Args: amount (int, optional): How many experiments are left to run. Defaults to 1. Returns: status (bool): True if the experiment can be started, False otherwise. amount (int): How many experiments are left to run. \"\"\" await self . experiment_semaphore . acquire () if self . amount > 0 : self . waiting += amount self . amount -= amount self . experiment_semaphore . release () return True , self . amount self . experiment_semaphore . release () return False , - 1","title":"start()"},{"location":"api/imitation/#imitation_datasets.utils.Experiment.stop","text":"Stop an experiment. Parameters: status ( bool ) \u2013 True if the experiment was successful, False otherwise. amount ( int , default: 1 ) \u2013 How many experiments are left to run. Defaults to 1. src/imitation_datasets/utils.py 90 91 92 93 94 95 96 97 98 99 100 async def stop ( self , status : bool , amount : int = 1 ) -> None : \"\"\"Stop an experiment. Args: status (bool): True if the experiment was successful, False otherwise. amount (int, optional): How many experiments are left to run. Defaults to 1. \"\"\" await self . experiment_semaphore . acquire () self . amount += 0 if status else amount self . waiting -= amount self . experiment_semaphore . release ()","title":"stop()"},{"location":"api/imitation/#imitation_datasets.utils.Experiment.write_log","text":"Write the logs in the log file. src/imitation_datasets/utils.py 111 112 113 114 115 116 117 def write_log ( self ) -> None : \"\"\"Write the logs in the log file.\"\"\" with open ( './logs.txt' , 'a' , encoding = 'utf8' ) as log_file : for idx , logs in self . logs . items (): for log in logs : log_file . write ( f ' \\n Experiment { idx } : { log } ' ) log_file . write ( ' \\n ' )","title":"write_log()"},{"location":"api/imitation/#context","text":"Context dataclass to keep track of the context of the experiment. Source code in src/imitation_datasets/utils.py 120 121 122 123 124 125 126 127 128 @dataclass class Context : \"\"\"Context dataclass to keep track of the context of the experiment.\"\"\" experiments : Experiment index : int def add_log ( self , log : str ) -> None : \"\"\"Add a log to the experiment.\"\"\" self . experiments . add_log ( self . index , log )","title":"Context"},{"location":"api/imitation/#imitation_datasets.utils.Context.add_log","text":"Add a log to the experiment. src/imitation_datasets/utils.py 126 127 128 def add_log ( self , log : str ) -> None : \"\"\"Add a log to the experiment.\"\"\" self . experiments . add_log ( self . index , log )","title":"add_log()"},{"location":"api/imitation/#cpus","text":"CPUS dataclass to keep track of the available CPUs. Source code in src/imitation_datasets/utils.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 @dataclass class CPUS ( metaclass = Singleton ): \"\"\"CPUS dataclass to keep track of the available CPUs.\"\"\" available_cpus : int = field ( default_factory = multiprocessing . cpu_count ()) cpus : DefaultDict [ int , bool ] = field ( init = False , default_factory = lambda : defaultdict ( bool )) cpu_semaphore : asyncio . Lock = field ( init = False ) def __post_init__ ( self ) -> None : \"\"\"Initialize the cpu_semaphore.\"\"\" if self . available_cpus > multiprocessing . cpu_count () - 1 : self . available_cpus = multiprocessing . cpu_count () - 1 self . cpu_semaphore = asyncio . BoundedSemaphore ( value = self . available_cpus ) async def cpu_allock ( self ) -> int : \"\"\"Acquire a CPU. Returns: int: CPU index. \"\"\" await self . cpu_semaphore . acquire () for idx in range ( self . available_cpus ): if not self . cpus [ idx ]: self . cpus [ idx ] = True return idx def cpu_release ( self , cpu_idx : int ) -> None : \"\"\"Release a CPU. Args: cpu_idx (int): CPU index. \"\"\" try : self . cpus [ cpu_idx ] = False self . cpu_semaphore . release () except ValueError : pass","title":"CPUS"},{"location":"api/imitation/#imitation_datasets.utils.CPUS.__post_init__","text":"Initialize the cpu_semaphore. src/imitation_datasets/utils.py 139 140 141 142 143 def __post_init__ ( self ) -> None : \"\"\"Initialize the cpu_semaphore.\"\"\" if self . available_cpus > multiprocessing . cpu_count () - 1 : self . available_cpus = multiprocessing . cpu_count () - 1 self . cpu_semaphore = asyncio . BoundedSemaphore ( value = self . available_cpus )","title":"__post_init__()"},{"location":"api/imitation/#imitation_datasets.utils.CPUS.cpu_allock","text":"Acquire a CPU. Returns: int ( int ) \u2013 CPU index. src/imitation_datasets/utils.py 145 146 147 148 149 150 151 152 153 154 155 async def cpu_allock ( self ) -> int : \"\"\"Acquire a CPU. Returns: int: CPU index. \"\"\" await self . cpu_semaphore . acquire () for idx in range ( self . available_cpus ): if not self . cpus [ idx ]: self . cpus [ idx ] = True return idx","title":"cpu_allock()"},{"location":"api/imitation/#imitation_datasets.utils.CPUS.cpu_release","text":"Release a CPU. Parameters: cpu_idx ( int ) \u2013 CPU index. src/imitation_datasets/utils.py 157 158 159 160 161 162 163 164 165 166 167 def cpu_release ( self , cpu_idx : int ) -> None : \"\"\"Release a CPU. Args: cpu_idx (int): CPU index. \"\"\" try : self . cpus [ cpu_idx ] = False self . cpu_semaphore . release () except ValueError : pass","title":"cpu_release()"},{"location":"api/imitation/#gymwrapper","text":"Wrapper for gym environment. Since Gymnasium and Gym version 0.26 there are some environments that were working under Gym-v.0.21 stopped working. This wrapper just makes sure that the output for the environment will always work with the version the user wants. Source code in src/imitation_datasets/utils.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 class GymWrapper : \"\"\" Wrapper for gym environment. Since Gymnasium and Gym version 0.26 there are some environments that were working under Gym-v.0.21 stopped working. This wrapper just makes sure that the output for the environment will always work with the version the user wants. \"\"\" def __init__ ( self , environment : Any , version : str = \"newest\" ) -> None : \"\"\" Args: name: gym environment name version: [\"newest\", \"older\"] refers to the compatibility version. In this case, \"newest\" is 0.26 and \"older\" is 0.21. \"\"\" if version not in [ \"newest\" , \"older\" ]: raise ValueError ( \"Version has to be : ['newest', 'older']\" ) self . env = environment state = environment . reset () if version == \"older\" and not isinstance ( state [ 0 ], np . floating ): raise WrapperException ( \"Incopatible environment version and wrapper version.\" ) if version == \"newest\" and not isinstance ( state [ 0 ], np . ndarray ): raise WrapperException ( \"Incopatible environment version and wrapper version.\" ) self . version = version @property def action_space ( self ): \"\"\"Map gym action_space attribute to wrapper.\"\"\" return self . env . action_space @property def observation_space ( self ): \"\"\"Map gym env_space attribute to wrapper.\"\"\" return self . env . observation_space def set_seed ( self , seed : int ) -> None : \"\"\"Set seed for all packages (Pytorch, Numpy and Python). Args: seed (optional, int): seed number to use for the random generator. \"\"\" torch . manual_seed ( seed ) np . random . seed ( seed ) random . seed ( seed ) def reset ( self ) -> Union [ Tuple [ List [ float ], Dict [ str , Any ]], List [ float ]]: \"\"\"Resets the framework and return the appropriate return.\"\"\" state = self . env . reset () if self . version == \"newest\" : return state [ 0 ] return state def step ( self , action : Union [ float , int ] ) -> Union [ Tuple [ List [ float ], float , bool , bool , Dict [ str , Any ]], Tuple [ List [ float ], float , bool , Dict [ str , Any ]] ]: \"\"\" Perform an action in the environment and return the appropriate return according to version. \"\"\" gym_return = self . env . step ( action ) if self . version == \"newest\" : state , reward , terminated , truncated , info = gym_return return state , reward , terminated or truncated , info return gym_return def render ( self , mode = \"rgb_array\" ): \"\"\"Return the render for the environment.\"\"\" if self . version == \"newest\" : state = self . env . render () if state is None and self . env . render_mode != \"human\" : raise WrapperException ( \"No render mode set.\" ) return state return self . env . render ( mode ) def close ( self ) -> None : \"\"\"Close the environment.\"\"\" self . env . close ()","title":"GymWrapper"},{"location":"api/imitation/#imitation_datasets.utils.GymWrapper.action_space","text":"Map gym action_space attribute to wrapper.","title":"action_space"},{"location":"api/imitation/#imitation_datasets.utils.GymWrapper.observation_space","text":"Map gym env_space attribute to wrapper.","title":"observation_space"},{"location":"api/imitation/#imitation_datasets.utils.GymWrapper.__init__","text":"Parameters: name \u2013 gym environment name version ( str , default: 'newest' ) \u2013 [\"newest\", \"older\"] refers to the compatibility version. In this case, \"newest\" is 0.26 and \"older\" is 0.21. src/imitation_datasets/utils.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def __init__ ( self , environment : Any , version : str = \"newest\" ) -> None : \"\"\" Args: name: gym environment name version: [\"newest\", \"older\"] refers to the compatibility version. In this case, \"newest\" is 0.26 and \"older\" is 0.21. \"\"\" if version not in [ \"newest\" , \"older\" ]: raise ValueError ( \"Version has to be : ['newest', 'older']\" ) self . env = environment state = environment . reset () if version == \"older\" and not isinstance ( state [ 0 ], np . floating ): raise WrapperException ( \"Incopatible environment version and wrapper version.\" ) if version == \"newest\" and not isinstance ( state [ 0 ], np . ndarray ): raise WrapperException ( \"Incopatible environment version and wrapper version.\" ) self . version = version","title":"__init__()"},{"location":"api/imitation/#imitation_datasets.utils.GymWrapper.close","text":"Close the environment. src/imitation_datasets/utils.py 266 267 268 def close ( self ) -> None : \"\"\"Close the environment.\"\"\" self . env . close ()","title":"close()"},{"location":"api/imitation/#imitation_datasets.utils.GymWrapper.render","text":"Return the render for the environment. src/imitation_datasets/utils.py 256 257 258 259 260 261 262 263 264 def render ( self , mode = \"rgb_array\" ): \"\"\"Return the render for the environment.\"\"\" if self . version == \"newest\" : state = self . env . render () if state is None and self . env . render_mode != \"human\" : raise WrapperException ( \"No render mode set.\" ) return state return self . env . render ( mode )","title":"render()"},{"location":"api/imitation/#imitation_datasets.utils.GymWrapper.reset","text":"Resets the framework and return the appropriate return. src/imitation_datasets/utils.py 231 232 233 234 235 236 def reset ( self ) -> Union [ Tuple [ List [ float ], Dict [ str , Any ]], List [ float ]]: \"\"\"Resets the framework and return the appropriate return.\"\"\" state = self . env . reset () if self . version == \"newest\" : return state [ 0 ] return state","title":"reset()"},{"location":"api/imitation/#imitation_datasets.utils.GymWrapper.set_seed","text":"Set seed for all packages (Pytorch, Numpy and Python). Parameters: seed ( ( optional , int ) ) \u2013 seed number to use for the random generator. src/imitation_datasets/utils.py 221 222 223 224 225 226 227 228 229 def set_seed ( self , seed : int ) -> None : \"\"\"Set seed for all packages (Pytorch, Numpy and Python). Args: seed (optional, int): seed number to use for the random generator. \"\"\" torch . manual_seed ( seed ) np . random . seed ( seed ) random . seed ( seed )","title":"set_seed()"},{"location":"api/imitation/#imitation_datasets.utils.GymWrapper.step","text":"Perform an action in the environment and return the appropriate return according to version. src/imitation_datasets/utils.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def step ( self , action : Union [ float , int ] ) -> Union [ Tuple [ List [ float ], float , bool , bool , Dict [ str , Any ]], Tuple [ List [ float ], float , bool , Dict [ str , Any ]] ]: \"\"\" Perform an action in the environment and return the appropriate return according to version. \"\"\" gym_return = self . env . step ( action ) if self . version == \"newest\" : state , reward , terminated , truncated , info = gym_return return state , reward , terminated or truncated , info return gym_return","title":"step()"},{"location":"api/imitation/#wrapperexception","text":"Bases: Exception Wrapper exception for all exceptions related to the wrapper. Source code in src/imitation_datasets/utils.py 174 175 176 177 178 179 class WrapperException ( Exception ): \"\"\"Wrapper exception for all exceptions related to the wrapper.\"\"\" def __init__ ( self , message : str ) -> None : self . message = message super () . __init__ ( self . message )","title":"WrapperException"},{"location":"api/imitation/#dataset","text":"","title":"dataset"},{"location":"api/imitation/#baselinedataset","text":"Bases: Dataset Teacher dataset for IL methods. Source code in src/imitation_datasets/dataset/dataset.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class BaselineDataset ( Dataset ): \"\"\"Teacher dataset for IL methods.\"\"\" def __init__ ( self , path : str , source : str = \"local\" , split : str = \"train\" , n_episodes : int = None ) -> None : \"\"\"Initialize dataset. Args: path (Str): path to the dataset. source (str): whether is a HuggingFace or a local dataset. Defaults to 'local'. Raises: ValueError: if path does not exist. \"\"\" if source == \"local\" and not os . path . exists ( path ): raise ValueError ( f \"No dataset at: { path } \" ) if source == \"local\" : self . data = np . load ( path , allow_pickle = True ) self . average_reward = np . mean ( self . data [ \"episode_returns\" ]) else : dataset = load_dataset ( path , split = \"train\" ) self . data = huggingface_to_baseline ( dataset ) self . average_reward = [] self . states = np . ndarray ( shape = ( 0 , self . data [ \"obs\" ] . shape [ - 1 ])) self . next_states = np . ndarray ( shape = ( 0 , self . data [ \"obs\" ] . shape [ - 1 ])) if len ( self . data [ \"actions\" ] . shape ) == 1 : action_size = 1 else : action_size = self . data [ \"actions\" ] . shape [ - 1 ] self . actions = np . ndarray ( shape = ( 0 , action_size )) episode_starts = list ( np . where ( self . data [ \"episode_starts\" ] == 1 )[ 0 ]) episode_starts . append ( len ( self . data [ \"episode_starts\" ])) if n_episodes is not None : if split == \"train\" : episode_starts = episode_starts [: n_episodes + 1 ] else : episode_starts = episode_starts [ n_episodes :] for start , end in zip ( episode_starts , tqdm ( episode_starts [ 1 :], desc = \"Creating dataset\" )): episode = self . data [ \"obs\" ][ start : end ] actions = self . data [ \"actions\" ][ start : end ] . reshape (( - 1 , 1 )) self . actions = np . append ( self . actions , actions [: - 1 ], axis = 0 ) self . states = np . append ( self . states , episode [: - 1 ], axis = 0 ) self . next_states = np . append ( self . next_states , episode [ 1 :], axis = 0 ) if source != \"local\" : self . average_reward . append ( self . data [ \"rewards\" ][ start : end ] . sum ()) if isinstance ( self . average_reward , list ): self . average_reward = np . mean ( self . average_reward ) assert self . states . shape [ 0 ] == self . actions . shape [ 0 ] == self . next_states . shape [ 0 ] self . states = torch . from_numpy ( self . states ) self . actions = torch . from_numpy ( self . actions ) self . next_states = torch . from_numpy ( self . next_states ) def __len__ ( self ) -> int : \"\"\"Dataset length. Returns: length (int): length. \"\"\" return self . states . shape [ 0 ] def __getitem__ ( self , index : int ) -> Tuple [ torch . Tensor ]: \"\"\"Get item from dataset. Args: index (int): index. Returns: state (torch.Tensor): state for timestep t. action (torch.Tensor): action for timestep t. next_state (torch.Tensor): state for timestep t + 1. \"\"\" state = self . states [ index ] action = self . actions [ index ] next_state = self . next_states [ index ] return state , action , next_state","title":"BaselineDataset"},{"location":"api/imitation/#imitation_datasets.dataset.dataset.BaselineDataset.__getitem__","text":"Get item from dataset. Parameters: index ( int ) \u2013 index. Returns: state ( Tensor ) \u2013 state for timestep t. action ( Tensor ) \u2013 action for timestep t. next_state ( Tensor ) \u2013 state for timestep t + 1. src/imitation_datasets/dataset/dataset.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def __getitem__ ( self , index : int ) -> Tuple [ torch . Tensor ]: \"\"\"Get item from dataset. Args: index (int): index. Returns: state (torch.Tensor): state for timestep t. action (torch.Tensor): action for timestep t. next_state (torch.Tensor): state for timestep t + 1. \"\"\" state = self . states [ index ] action = self . actions [ index ] next_state = self . next_states [ index ] return state , action , next_state","title":"__getitem__()"},{"location":"api/imitation/#imitation_datasets.dataset.dataset.BaselineDataset.__init__","text":"Initialize dataset. Parameters: path ( Str ) \u2013 path to the dataset. source ( str , default: 'local' ) \u2013 whether is a HuggingFace or a local dataset. Defaults to 'local'. Raises: ValueError \u2013 if path does not exist. src/imitation_datasets/dataset/dataset.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def __init__ ( self , path : str , source : str = \"local\" , split : str = \"train\" , n_episodes : int = None ) -> None : \"\"\"Initialize dataset. Args: path (Str): path to the dataset. source (str): whether is a HuggingFace or a local dataset. Defaults to 'local'. Raises: ValueError: if path does not exist. \"\"\" if source == \"local\" and not os . path . exists ( path ): raise ValueError ( f \"No dataset at: { path } \" ) if source == \"local\" : self . data = np . load ( path , allow_pickle = True ) self . average_reward = np . mean ( self . data [ \"episode_returns\" ]) else : dataset = load_dataset ( path , split = \"train\" ) self . data = huggingface_to_baseline ( dataset ) self . average_reward = [] self . states = np . ndarray ( shape = ( 0 , self . data [ \"obs\" ] . shape [ - 1 ])) self . next_states = np . ndarray ( shape = ( 0 , self . data [ \"obs\" ] . shape [ - 1 ])) if len ( self . data [ \"actions\" ] . shape ) == 1 : action_size = 1 else : action_size = self . data [ \"actions\" ] . shape [ - 1 ] self . actions = np . ndarray ( shape = ( 0 , action_size )) episode_starts = list ( np . where ( self . data [ \"episode_starts\" ] == 1 )[ 0 ]) episode_starts . append ( len ( self . data [ \"episode_starts\" ])) if n_episodes is not None : if split == \"train\" : episode_starts = episode_starts [: n_episodes + 1 ] else : episode_starts = episode_starts [ n_episodes :] for start , end in zip ( episode_starts , tqdm ( episode_starts [ 1 :], desc = \"Creating dataset\" )): episode = self . data [ \"obs\" ][ start : end ] actions = self . data [ \"actions\" ][ start : end ] . reshape (( - 1 , 1 )) self . actions = np . append ( self . actions , actions [: - 1 ], axis = 0 ) self . states = np . append ( self . states , episode [: - 1 ], axis = 0 ) self . next_states = np . append ( self . next_states , episode [ 1 :], axis = 0 ) if source != \"local\" : self . average_reward . append ( self . data [ \"rewards\" ][ start : end ] . sum ()) if isinstance ( self . average_reward , list ): self . average_reward = np . mean ( self . average_reward ) assert self . states . shape [ 0 ] == self . actions . shape [ 0 ] == self . next_states . shape [ 0 ] self . states = torch . from_numpy ( self . states ) self . actions = torch . from_numpy ( self . actions ) self . next_states = torch . from_numpy ( self . next_states )","title":"__init__()"},{"location":"api/imitation/#imitation_datasets.dataset.dataset.BaselineDataset.__len__","text":"Dataset length. Returns: length ( int ) \u2013 length. src/imitation_datasets/dataset/dataset.py 82 83 84 85 86 87 88 def __len__ ( self ) -> int : \"\"\"Dataset length. Returns: length (int): length. \"\"\" return self . states . shape [ 0 ]","title":"__len__()"},{"location":"api/imitation/#huggingface","text":"","title":"huggingface"},{"location":"api/imitation/#baseline_to_huggingface","text":"Loads baseline dataset from NpzFile, converts into a dict and save it into a JSONL file for upload. Parameters: dataset_path ( str ) \u2013 path to the npz file. new_path ( str ) \u2013 path to the new dataset. Raises: ValueError \u2013 if one of the paths does not exist. src/imitation_datasets/dataset/huggingface.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def baseline_to_huggingface ( dataset_path : str , new_path : str ) -> None : \"\"\"Loads baseline dataset from NpzFile, converts into a dict and save it into a JSONL file for upload. Args: dataset_path (str): path to the npz file. new_path (str): path to the new dataset. Raises: ValueError: if one of the paths does not exist. \"\"\" path = \"/\" . join ( dataset_path . split ( \"/\" )[: - 1 ]) if not os . path . exists ( path ): raise ValueError ( f \"' { path } ' does not exist.\" ) path = \"/\" . join ( new_path . split ( \"/\" )[: - 1 ]) if not os . path . exists ( path ): raise ValueError ( f \"' { path } ' does not exist.\" ) dataset = np . load ( dataset_path , allow_pickle = True ) dataset = convert_baseline_dataset_to_dict ( dataset ) save_dataset_into_huggingface_format ( dataset , new_path )","title":"baseline_to_huggingface"},{"location":"api/imitation/#baseline_to_huggingface_1","text":"Loads baseline dataset from NpzFile, converts into a dict and save it into a JSONL file for upload. Parameters: dataset_path ( str ) \u2013 path to the npz file. new_path ( str ) \u2013 path to the new dataset. Raises: ValueError \u2013 if one of the paths does not exist. src/imitation_datasets/dataset/huggingface.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def baseline_to_huggingface ( dataset_path : str , new_path : str ) -> None : \"\"\"Loads baseline dataset from NpzFile, converts into a dict and save it into a JSONL file for upload. Args: dataset_path (str): path to the npz file. new_path (str): path to the new dataset. Raises: ValueError: if one of the paths does not exist. \"\"\" path = \"/\" . join ( dataset_path . split ( \"/\" )[: - 1 ]) if not os . path . exists ( path ): raise ValueError ( f \"' { path } ' does not exist.\" ) path = \"/\" . join ( new_path . split ( \"/\" )[: - 1 ]) if not os . path . exists ( path ): raise ValueError ( f \"' { path } ' does not exist.\" ) dataset = np . load ( dataset_path , allow_pickle = True ) dataset = convert_baseline_dataset_to_dict ( dataset ) save_dataset_into_huggingface_format ( dataset , new_path )","title":"baseline_to_huggingface"},{"location":"api/imitation/#metrics","text":"","title":"metrics"},{"location":"api/imitation/#performance","text":"Compute the performance for the agent. Performance normalises between random and expert policies rewards, where performance 0 corresponds to random policy performance, and 1 are for expert policy performance. performance = (X - X_min) / (X_max - X_min), where X_min is the random_reward, and X_max is the teacher_reward. Parameters: agent_reward ( Number ) \u2013 agent accumulated reward. teacher_reward ( Number ) \u2013 teacher accumulated reward. random_reward ( Number ) \u2013 random agent accumulated reward. Raises: ValueError \u2013 if the teacher reward is inferior to the random agent. ValueError \u2013 Teacher and Random rewards should be Numbers. Returns: performance ( Number ) \u2013 performance metric. src/imitation_datasets/dataset/metrics.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def performance ( agent_reward : Union [ Number , List , np . ndarray ], teacher_reward : Number , random_reward : Number ) -> Dict [ str , Number ]: \"\"\"Compute the performance for the agent. Performance normalises between random and expert policies rewards, where performance 0 corresponds to random policy performance, and 1 are for expert policy performance. performance = (X - X_min) / (X_max - X_min), where X_min is the random_reward, and X_max is the teacher_reward. Args: agent_reward (Number): agent accumulated reward. teacher_reward (Number): teacher accumulated reward. random_reward (Number): random agent accumulated reward. Raises: ValueError: if the teacher reward is inferior to the random agent. ValueError: Teacher and Random rewards should be Numbers. Returns: performance (Number): performance metric. \"\"\" if isinstance ( teacher_reward , ( list , np . ndarray )): raise ValueError ( \"Teacher reward should not be a list\" ) if isinstance ( random_reward , ( list , np . ndarray )): raise ValueError ( \"Random reward should not be a list\" ) if teacher_reward < random_reward : raise ValueError ( \"Random reward should lesser than the teacher's.\" ) if isinstance ( agent_reward , list ): agent_reward = np . array ( agent_reward ) perf = ( agent_reward - random_reward ) / ( teacher_reward - random_reward ) if isinstance ( perf , np . ndarray ): return { \"performance\" : perf . mean (), \"performance_std\" : perf . std ()} return { \"performance\" : perf , \"performance_std\" : 0 }","title":"performance"},{"location":"api/imitation/#average_episodic_reward","text":"Compute the average episodic reward for the agent. AER is the average of 'n' episodes for each agent in each environment. Parameters: agent_reward ( List [ Number ] ) \u2013 list of each episode accumulated reward. Returns: AER ( Number ) \u2013 average episodic reward metric. src/imitation_datasets/dataset/metrics.py 52 53 54 55 56 57 58 59 60 61 62 63 64 def average_episodic_reward ( agent_reward : List [ Number ]) -> Dict [ str , Number ]: \"\"\"Compute the average episodic reward for the agent. AER is the average of 'n' episodes for each agent in each environment. Args: agent_reward (List[Number]): list of each episode accumulated reward. Returns: AER (Number): average episodic reward metric. \"\"\" if isinstance ( agent_reward , list ): agent_reward = np . array ( agent_reward ) return { \"aer\" : agent_reward . mean (), \"aer_std\" : agent_reward . std ()}","title":"average_episodic_reward"},{"location":"api/imitation/#accuracy","text":"Compute the accuracy for a model. The accuracy returned is the percentage from 0 to 100. Parameters: prediction ( Tensor ) \u2013 logits from a model. ground_truth ( Tensor ) \u2013 ground truth class. Raises: ValueError \u2013 if predictions and ground_truth are not torch.Tensor. ValueError \u2013 if predictions are not two dimensional. ValueError \u2013 if ground_truth is not one dimensional. Returns: accuracy ( Number ) \u2013 accuracy between 0 and 100 for a model. src/imitation_datasets/dataset/metrics.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def accuracy ( prediction : Tensor , ground_truth : Tensor ) -> Number : \"\"\"Compute the accuracy for a model. The accuracy returned is the percentage from 0 to 100. Args: prediction (torch.Tensor): logits from a model. ground_truth (torch.Tensor): ground truth class. Raises: ValueError: if predictions and ground_truth are not torch.Tensor. ValueError: if predictions are not two dimensional. ValueError: if ground_truth is not one dimensional. Returns: accuracy (Number): accuracy between 0 and 100 for a model. \"\"\" if not isinstance ( prediction , Tensor ) or not isinstance ( ground_truth , Tensor ): raise ValueError ( \"'prediction' and 'ground truth' should be a tensor\" ) if len ( prediction . size ()) != 2 : raise ValueError ( \"'prediction' and 'ground truth' need to be 2 dimensional.\" ) if len ( ground_truth . size ()) != 1 : raise ValueError ( \"'ground truth' need to be 1 dimensional.\" ) return (( argmax ( prediction , 1 ) == ground_truth ) . sum () . item () / ground_truth . size ( 0 )) * 100","title":"accuracy"},{"location":"tutorials/benchmark/","text":"Benchmarking All benchmark results are available in the benchmark_results.md . If you wish to run all benchmarks locally, feel free to run them with: $ python src/bencmark/benchmark.py This command will run all implemented methods with all available datasets . Be careful, it will take a long time! If you wish to run a single method or run them in parallel, use: $ python src/bencmark/benchmark.py --methods <METHOD NAME> For example: $ python src/bencmark/benchmark.py --methods bc,bco will run Behavioural Cloning and Behavioural Clonning from Observation.","title":"Benchmarking"},{"location":"tutorials/benchmark/#benchmarking","text":"All benchmark results are available in the benchmark_results.md . If you wish to run all benchmarks locally, feel free to run them with: $ python src/bencmark/benchmark.py This command will run all implemented methods with all available datasets . Be careful, it will take a long time! If you wish to run a single method or run them in parallel, use: $ python src/bencmark/benchmark.py --methods <METHOD NAME> For example: $ python src/bencmark/benchmark.py --methods bc,bco will run Behavioural Cloning and Behavioural Clonning from Observation.","title":"Benchmarking"},{"location":"tutorials/creation/","text":"Creating a new dataset Creation of new Datasets use the Controller class, which requires two different function to work: * an enjoy function, which uses the agent to interact with the environment and record an episode); and * a collate function, which puts all files created by the enjoy function into a dataset file. Default functions The imitation_datasets module provides a set of default functions, so you don't need to implement an enjoy and a collate function in every project. The resulting dataset will be a NpzFile with the following data: \"\"\" Data: obs (list[list[float]): gym environment observation. Size [steps, observations space]. actions (list[float]): agent action. Size [steps, action] (1 if single action, n if multiple actions). rewards (list[int]): reward from the action with the observations (e.g., r(obs, action)). Size [steps, ]. episode_returns (list[float]): accumulated reward for each episode. Size [number of peisodes, ]. episode_starts (list[bool]): whether the episode started at the current observation. Size [steps, ]. \"\"\" A small functional example of how to use the given functions: # command: $ python <script> --game cartpole --threads 4 --episodes 1000 --mode all from imitation_datasets.functions import baseline_enjoy, baseline_collate from imitation_datasets.controller import Controller from imitation_datasets.args import get_args args = get_args() controller = Controller(baseline_enjoy, baseline_collate, args.episodes, args.threads) controller.start(args) Converting to a HuggingFace dataset If you want to upload the dataset to HuggingFace, the imitation_datasets module also provides a function for converting the NpzFile into jsonl format. from imitation_datasets.dataset.huggingface import baseline_to_huggingface baseline_to_huggingface(\"./path/to/NpZFile\", \"./new/path\")","title":"Creating a new dataset"},{"location":"tutorials/creation/#creating-a-new-dataset","text":"Creation of new Datasets use the Controller class, which requires two different function to work: * an enjoy function, which uses the agent to interact with the environment and record an episode); and * a collate function, which puts all files created by the enjoy function into a dataset file.","title":"Creating a new dataset"},{"location":"tutorials/creation/#default-functions","text":"The imitation_datasets module provides a set of default functions, so you don't need to implement an enjoy and a collate function in every project. The resulting dataset will be a NpzFile with the following data: \"\"\" Data: obs (list[list[float]): gym environment observation. Size [steps, observations space]. actions (list[float]): agent action. Size [steps, action] (1 if single action, n if multiple actions). rewards (list[int]): reward from the action with the observations (e.g., r(obs, action)). Size [steps, ]. episode_returns (list[float]): accumulated reward for each episode. Size [number of peisodes, ]. episode_starts (list[bool]): whether the episode started at the current observation. Size [steps, ]. \"\"\" A small functional example of how to use the given functions: # command: $ python <script> --game cartpole --threads 4 --episodes 1000 --mode all from imitation_datasets.functions import baseline_enjoy, baseline_collate from imitation_datasets.controller import Controller from imitation_datasets.args import get_args args = get_args() controller = Controller(baseline_enjoy, baseline_collate, args.episodes, args.threads) controller.start(args)","title":"Default functions"},{"location":"tutorials/creation/#converting-to-a-huggingface-dataset","text":"If you want to upload the dataset to HuggingFace, the imitation_datasets module also provides a function for converting the NpzFile into jsonl format. from imitation_datasets.dataset.huggingface import baseline_to_huggingface baseline_to_huggingface(\"./path/to/NpZFile\", \"./new/path\")","title":"Converting to a HuggingFace dataset"},{"location":"tutorials/datasets/","text":"Available Datasets The IL-Datasets also come with a default PyTorch dataset, called BaselineDataset . It uses the pattern set by the baseline_collate function, and it allows the use of HuggingFace datasets created by the baseline_to_huggingface function. The dataset list for benchmarking is under development, so to check all new versions, you can visit our collection on HuggingFace . BaselineDataset To use the Baseline dataset, you can use a local file : from src.imitation_datasets.dataset import BaselineDataset BaselineDataset(f\"./dataset/cartpole/teacher.npz\") Or a HuggingFace path: from src.imitation_datasets.dataset import BaselineDataset BaselineDataset(f\"NathanGavenski/CartPole-v1\", source=\"huggingface\") Train and Evaluation splits BaselineDataset allows for fewer episodes and splitting for evaluation and train . from src.imitation_datasets.dataset import BaselineDataset dataset_train = BaselineDataset(f\"NathanGavenski/CartPole-v1\", source=\"huggingface\", n_episodes=100) dataset_eval = BaselineDataset(f\"NathanGavenski/CartPole-v1\", source=\"huggingface\", n_episodes=100, split=\"eval\")","title":"Using available datasets"},{"location":"tutorials/datasets/#available-datasets","text":"The IL-Datasets also come with a default PyTorch dataset, called BaselineDataset . It uses the pattern set by the baseline_collate function, and it allows the use of HuggingFace datasets created by the baseline_to_huggingface function. The dataset list for benchmarking is under development, so to check all new versions, you can visit our collection on HuggingFace .","title":"Available Datasets"},{"location":"tutorials/datasets/#baselinedataset","text":"To use the Baseline dataset, you can use a local file : from src.imitation_datasets.dataset import BaselineDataset BaselineDataset(f\"./dataset/cartpole/teacher.npz\") Or a HuggingFace path: from src.imitation_datasets.dataset import BaselineDataset BaselineDataset(f\"NathanGavenski/CartPole-v1\", source=\"huggingface\")","title":"BaselineDataset"},{"location":"tutorials/datasets/#train-and-evaluation-splits","text":"BaselineDataset allows for fewer episodes and splitting for evaluation and train . from src.imitation_datasets.dataset import BaselineDataset dataset_train = BaselineDataset(f\"NathanGavenski/CartPole-v1\", source=\"huggingface\", n_episodes=100) dataset_eval = BaselineDataset(f\"NathanGavenski/CartPole-v1\", source=\"huggingface\", n_episodes=100, split=\"eval\")","title":"Train and Evaluation splits"},{"location":"tutorials/install/","text":"Installing IL-Datasets IL-Datasets install everythings it depends on, with the exception of PyTorch. We do this so we would not overide the current PyTorch version you use on yout local environment. Therefore, before running IL-Datasets don't forget to install the PyTorch version you need! Requirements The project supports Python versions 3.8 ~ 3.11 , and the latest PyTorch version. PyPi IL-Datasets is available on PyPi: # Stable version pip install il-datasets if you plan to use the benchmark module, please use: # Stable version pip install \"il-datasets[benchmark]\" Local But if you prefer, you can install it from the source: git clone https://github.com/NathanGavenski/IL-Datasets.git cd IL-Datasets pip install -e . Local Requirements All requirements for the imitation_datasets module are listed in requirements.txt . These requirements are required by the module and are installed together with the IL-Datasets . For requirements to use the benchmark module, use both the imitation_datasets requirements and the ones listed in benchmark.txt . Development requirements are listed at dev.txt . We do not recommend using these dependencies outside development. They use an outdated version from gym v0.21.0 to test the GymWrapper class.","title":"Installation"},{"location":"tutorials/install/#installing-il-datasets","text":"IL-Datasets install everythings it depends on, with the exception of PyTorch. We do this so we would not overide the current PyTorch version you use on yout local environment. Therefore, before running IL-Datasets don't forget to install the PyTorch version you need!","title":"Installing IL-Datasets"},{"location":"tutorials/install/#requirements","text":"The project supports Python versions 3.8 ~ 3.11 , and the latest PyTorch version.","title":"Requirements"},{"location":"tutorials/install/#pypi","text":"IL-Datasets is available on PyPi: # Stable version pip install il-datasets if you plan to use the benchmark module, please use: # Stable version pip install \"il-datasets[benchmark]\"","title":"PyPi"},{"location":"tutorials/install/#local","text":"But if you prefer, you can install it from the source: git clone https://github.com/NathanGavenski/IL-Datasets.git cd IL-Datasets pip install -e .","title":"Local"},{"location":"tutorials/install/#local-requirements","text":"All requirements for the imitation_datasets module are listed in requirements.txt . These requirements are required by the module and are installed together with the IL-Datasets . For requirements to use the benchmark module, use both the imitation_datasets requirements and the ones listed in benchmark.txt . Development requirements are listed at dev.txt . We do not recommend using these dependencies outside development. They use an outdated version from gym v0.21.0 to test the GymWrapper class.","title":"Local Requirements"}]}